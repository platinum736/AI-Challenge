{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'migrated',\n",
       " 'Base models-entire data.ipynb',\n",
       " 'Basic models.ipynb',\n",
       " 'EDA_Challenge.ipynb',\n",
       " 'EDA.ipynb',\n",
       " 'EDA_trainData.ipynb',\n",
       " 'Ensemble models.ipynb',\n",
       " 'xgboost with null.ipynb',\n",
       " 'Untitled.ipynb',\n",
       " 'Base models-clean.ipynb',\n",
       " 'nohup.out',\n",
       " 'jupyter_notebook_config.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (6,16,17,26,40,41,49,53,54,55,100,104,107,109,112,127,128,129,130,133,153,171,220,221,222,225,226,227) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (6,16,17,26,40,41,49,53,54,55,100,104,107,109,112,127,128,129,130,153,171,220,221,222,225,226,227) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512.0059087276459\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1=time.time()\n",
    "train = pd.read_table('/aichallenge/training_set.tsv')\n",
    "test = pd.read_table('/aichallenge/scoring_set.tsv')\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['training_data.innovation_challenge_key', 'training_data.renewed_yorn',\n",
       "       'training_data.instance_id', 'training_data.contract_line_id',\n",
       "       'training_data.minor_line_yorn', 'training_data.major_line_instance_id',\n",
       "       'training_data.installation_date',\n",
       "       'training_data.product_sales_order_type',\n",
       "       'training_data.product_purchase_order_type',\n",
       "       'training_data.instance_status',\n",
       "       ...\n",
       "       'training_data.service_distributor_base_theater_name',\n",
       "       'training_data.service_distributor_base_distributor_normalized_name',\n",
       "       'training_data.contract_line_net_usd_amount',\n",
       "       'training_data.product_net_price',\n",
       "       'training_data.sales_node_renewal_rate',\n",
       "       'training_data.customer_renewal_rate',\n",
       "       'training_data.partner_renewal_rate',\n",
       "       'training_data.product_renewal_rate',\n",
       "       'training_data.service_sales_node_installed_base_sales_node_renewal_rate',\n",
       "       'training_data.service_partner_installed_base_partner_renewal_rate'],\n",
       "      dtype='object', length=237)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'training_data.renewed_yorn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'training_data.renewed_yorn'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-6a3d9d4fa9ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#remove the rows where 'challenge_data.renewed_yorn' is null in training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training_data.renewed_yorn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'training_data.renewed_yorn'"
     ]
    }
   ],
   "source": [
    "#remove the rows where 'challenge_data.renewed_yorn' is null in training data\n",
    "t1=time.time()\n",
    "train=train[train['training_data.renewed_yorn'].notnull()]\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012826919555664062\n"
     ]
    }
   ],
   "source": [
    "#remove data set name from columns names to make it uniform\n",
    "t1=time.time()\n",
    "columns=[]\n",
    "for col in train.columns:\n",
    "    columns.append(col.replace('training_data.',''))\n",
    "train.columns=columns\n",
    "columns=[]\n",
    "for col in test.columns:\n",
    "    columns.append(col.replace('scoring_set.',''))\n",
    "test.columns=columns\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.580340147018433\n"
     ]
    }
   ],
   "source": [
    "#remove the 'challenge_data.renewed_yorn' from training and test data and move it to a seperate variable\n",
    "t1=time.time()\n",
    "train_y=train['renewed_yorn']\n",
    "train = train.drop('renewed_yorn',axis=1)\n",
    "test_y=test['renewed_yorn']\n",
    "test = test.drop('renewed_yorn',axis=1)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.03571057319641\n"
     ]
    }
   ],
   "source": [
    "#remove columns which have any null values\n",
    "t1=time.time()\n",
    "percent = (train.isnull().sum()/train.isnull().count())\n",
    "train.columns[percent>0.75]\n",
    "#We would ignore all these 102 columns so we are left with 135 columns\n",
    "len(train.columns[percent>0.75])\n",
    "#all_clean_columns=train.columns[percent==0]\n",
    "#train_clean=train[all_clean_columns]\n",
    "unclean_columns=test.columns[percent>0.75]\n",
    "train = train.drop(unclean_columns,axis=1)\n",
    "test = test.drop(unclean_columns,axis=1)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a1f63af9b756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpercent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpercent\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpercent\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "percent = (train.isnull().sum()/train.isnull().count())\n",
    "train.columns[percent>0.75]\n",
    "len(train.columns[percent>0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unclean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.507105827331543\n"
     ]
    }
   ],
   "source": [
    "#Remove columns which have too many categories and is string type\n",
    "t1=time.time()\n",
    "messy_columns = []\n",
    "for col in test.columns:\n",
    "    if(test[col].dtype=='object' and len(test[col].unique())>20):\n",
    "        messy_columns.append(col)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 78 messy columns are removed\n",
    "len(messy_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.147759437561035\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "train=train.drop(messy_columns,axis=1)\n",
    "test=test.drop(messy_columns,axis=1)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6010041, 125)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.753363609313965\n"
     ]
    }
   ],
   "source": [
    "#Remove the primary key column for data fitting\n",
    "t1=time.time()\n",
    "train_ids=train['innovation_challenge_key']\n",
    "test_ids=test['innovation_challenge_key']\n",
    "train = train.drop('innovation_challenge_key',axis=1)\n",
    "test = test.drop('innovation_challenge_key',axis=1)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.503035306930542\n"
     ]
    }
   ],
   "source": [
    "#Number of unique values in each columns\n",
    "t1=time.time()\n",
    "catcols=[]\n",
    "scalcols=[]\n",
    "for col in train.columns:\n",
    "    if (train[col].dtype !='object'):\n",
    "        if(len(train[col].unique())<20):\n",
    "            catcols.append(col)\n",
    "        else:\n",
    "            scalcols.append(col)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6032524108886719\n"
     ]
    }
   ],
   "source": [
    "#handle catcols\n",
    "t1=time.time()\n",
    "for col in catcols:\n",
    "    train[col]=train[col].astype('category')\n",
    "    test[col]=test[col].astype('category')\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace integer nans with mean \n",
    "train[scalcols]=train[scalcols].fillna(train[scalcols].mean())\n",
    "test[scalcols]=test[scalcols].fillna(test[scalcols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contract_line_net_usd_amount                                 False\n",
       "product_net_price                                            False\n",
       "sales_node_renewal_rate                                      False\n",
       "partner_renewal_rate                                         False\n",
       "product_renewal_rate                                         False\n",
       "service_sales_node_installed_base_sales_node_renewal_rate    False\n",
       "service_partner_installed_base_partner_renewal_rate          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[scalcols].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contract_line_net_usd_amount                                 False\n",
       "product_net_price                                            False\n",
       "sales_node_renewal_rate                                      False\n",
       "partner_renewal_rate                                         False\n",
       "product_renewal_rate                                         False\n",
       "service_sales_node_installed_base_sales_node_renewal_rate    False\n",
       "service_partner_installed_base_partner_renewal_rate          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[scalcols].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.183216571807861\n"
     ]
    }
   ],
   "source": [
    "#Normalize integer data\n",
    "t1=time.time()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[scalcols])\n",
    "train[scalcols] = scaler.transform(train[scalcols])\n",
    "scaler = scaler.fit(test[scalcols])\n",
    "test[scalcols] = scaler.transform(test[scalcols])\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract_line_net_usd_amount</th>\n",
       "      <th>product_net_price</th>\n",
       "      <th>sales_node_renewal_rate</th>\n",
       "      <th>partner_renewal_rate</th>\n",
       "      <th>product_renewal_rate</th>\n",
       "      <th>service_sales_node_installed_base_sales_node_renewal_rate</th>\n",
       "      <th>service_partner_installed_base_partner_renewal_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.010041e+06</td>\n",
       "      <td>6.010041e+06</td>\n",
       "      <td>6.010041e+06</td>\n",
       "      <td>6.010041e+06</td>\n",
       "      <td>6.010041e+06</td>\n",
       "      <td>6.010041e+06</td>\n",
       "      <td>6.010041e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.177913e-17</td>\n",
       "      <td>5.735377e-17</td>\n",
       "      <td>-3.990025e-17</td>\n",
       "      <td>-3.949048e-16</td>\n",
       "      <td>-1.751612e-16</td>\n",
       "      <td>-1.080186e-16</td>\n",
       "      <td>-9.717581e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.164629e-01</td>\n",
       "      <td>-2.451377e-01</td>\n",
       "      <td>-9.846433e-02</td>\n",
       "      <td>-2.941421e-01</td>\n",
       "      <td>-1.849635e-01</td>\n",
       "      <td>-8.043840e-02</td>\n",
       "      <td>-6.221420e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.146668e-01</td>\n",
       "      <td>-2.451377e-01</td>\n",
       "      <td>-3.990931e-17</td>\n",
       "      <td>-3.882947e-02</td>\n",
       "      <td>-1.744921e-16</td>\n",
       "      <td>-1.025815e-02</td>\n",
       "      <td>-8.700442e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.146668e-01</td>\n",
       "      <td>-2.451377e-01</td>\n",
       "      <td>-3.990931e-17</td>\n",
       "      <td>-3.982183e-16</td>\n",
       "      <td>-1.744921e-16</td>\n",
       "      <td>-1.156315e-16</td>\n",
       "      <td>-9.274100e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-1.146668e-01</td>\n",
       "      <td>-1.634183e-01</td>\n",
       "      <td>-3.990931e-17</td>\n",
       "      <td>-3.982183e-16</td>\n",
       "      <td>-1.744921e-16</td>\n",
       "      <td>-1.156315e-16</td>\n",
       "      <td>-9.274100e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.899812e+02</td>\n",
       "      <td>7.503942e+01</td>\n",
       "      <td>9.262250e+02</td>\n",
       "      <td>4.837288e+02</td>\n",
       "      <td>2.209099e+02</td>\n",
       "      <td>1.934147e+02</td>\n",
       "      <td>7.311824e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       contract_line_net_usd_amount  product_net_price  \\\n",
       "count                  6.010041e+06       6.010041e+06   \n",
       "mean                  -3.177913e-17       5.735377e-17   \n",
       "std                    1.000000e+00       1.000000e+00   \n",
       "min                   -1.164629e-01      -2.451377e-01   \n",
       "25%                   -1.146668e-01      -2.451377e-01   \n",
       "50%                   -1.146668e-01      -2.451377e-01   \n",
       "75%                   -1.146668e-01      -1.634183e-01   \n",
       "max                    2.899812e+02       7.503942e+01   \n",
       "\n",
       "       sales_node_renewal_rate  partner_renewal_rate  product_renewal_rate  \\\n",
       "count             6.010041e+06          6.010041e+06          6.010041e+06   \n",
       "mean             -3.990025e-17         -3.949048e-16         -1.751612e-16   \n",
       "std               1.000000e+00          1.000000e+00          1.000000e+00   \n",
       "min              -9.846433e-02         -2.941421e-01         -1.849635e-01   \n",
       "25%              -3.990931e-17         -3.882947e-02         -1.744921e-16   \n",
       "50%              -3.990931e-17         -3.982183e-16         -1.744921e-16   \n",
       "75%              -3.990931e-17         -3.982183e-16         -1.744921e-16   \n",
       "max               9.262250e+02          4.837288e+02          2.209099e+02   \n",
       "\n",
       "       service_sales_node_installed_base_sales_node_renewal_rate  \\\n",
       "count                                       6.010041e+06           \n",
       "mean                                       -1.080186e-16           \n",
       "std                                         1.000000e+00           \n",
       "min                                        -8.043840e-02           \n",
       "25%                                        -1.025815e-02           \n",
       "50%                                        -1.156315e-16           \n",
       "75%                                        -1.156315e-16           \n",
       "max                                         1.934147e+02           \n",
       "\n",
       "       service_partner_installed_base_partner_renewal_rate  \n",
       "count                                       6.010041e+06    \n",
       "mean                                       -9.717581e-18    \n",
       "std                                         1.000000e+00    \n",
       "min                                        -6.221420e-02    \n",
       "25%                                        -8.700442e-03    \n",
       "50%                                        -9.274100e-18    \n",
       "75%                                        -9.274100e-18    \n",
       "max                                         7.311824e+02    "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[scalcols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_transaction_type</th>\n",
       "      <th>contract_line_reaction_time_code</th>\n",
       "      <th>sales_hierarchy_level</th>\n",
       "      <th>service_sales_node_base_sales_hierarchy_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6010041</td>\n",
       "      <td>6010041</td>\n",
       "      <td>6010041</td>\n",
       "      <td>6010041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>10002</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5316273</td>\n",
       "      <td>5985929</td>\n",
       "      <td>5248934</td>\n",
       "      <td>5945627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_transaction_type  contract_line_reaction_time_code  \\\n",
       "count                    6010041                           6010041   \n",
       "unique                         3                                 4   \n",
       "top                        10002                                -1   \n",
       "freq                     5316273                           5985929   \n",
       "\n",
       "        sales_hierarchy_level  service_sales_node_base_sales_hierarchy_level  \n",
       "count                 6010041                                        6010041  \n",
       "unique                      6                                              6  \n",
       "top                         6                                              6  \n",
       "freq                  5248934                                        5945627  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[catcols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclean_cols=train.columns[train.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all other 43 cols nan with mode values\n",
    "train[unclean_cols]=train[unclean_cols].fillna(train[unclean_cols].mode())\n",
    "test[unclean_cols]=test[unclean_cols].fillna(test[unclean_cols].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.86692428588867\n"
     ]
    }
   ],
   "source": [
    "#convert data to one hot encoding to handle categorical values\n",
    "t1=time.time()\n",
    "train_objs_num = len(train)\n",
    "dataset = pd.concat(objs=[train, test], axis=0)\n",
    "dataset = pd.get_dummies(dataset)\n",
    "train = dataset[:train_objs_num]\n",
    "test = dataset[train_objs_num:]\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['service_sales_node_base_sales_hierarchy_level',\n",
       "       'contract_line_net_usd_amount', 'product_net_price',\n",
       "       'sales_node_renewal_rate', 'partner_renewal_rate',\n",
       "       'product_renewal_rate',\n",
       "       'service_sales_node_installed_base_sales_node_renewal_rate',\n",
       "       'service_partner_installed_base_partner_renewal_rate',\n",
       "       'minor_line_yorn_N', 'minor_line_yorn_Y',\n",
       "       ...\n",
       "       'service_distributor_base_distributor_name_C321254900CAC4B6824721B6611C15CD8A9A824C66B6C1CA7FC9BDE39ABC5B18',\n",
       "       'service_distributor_base_distributor_name_C868A0ED0FB7D710C0235B586BDE12FC90AE26BA384782868BB4EE0A6A2FC9B3',\n",
       "       'service_distributor_base_distributor_name_D496D3E841D7F257C8B8314A580E5728BC034FD09FD4360908B2103860CC728F',\n",
       "       'service_distributor_base_distributor_name_E15FFA65FCD7A17CF3F1FE025C496E1B5C5D6A5126918C5FFEFFC411240000F5',\n",
       "       'service_distributor_base_distributor_name_E1B1913F5C4B81CD4DC2C80B2086BE1E90C063FCB34FBF5E3C8E51C2D89A6162',\n",
       "       'service_distributor_base_distributor_name_E645B197C64AC18DC483C6C490A6387D4AC132EACCD9D12840C2618BBEC7F3DB',\n",
       "       'service_distributor_base_distributor_name_F709970DB140E704E58FEFC168198B2A20D2E9296C9FC90443FE5123C97B02EA',\n",
       "       'service_distributor_base_distributor_name_F716737082AE46AB96E87570A13D412F6029BEAFF22815E922D46038D2424E6A',\n",
       "       'service_distributor_base_distributor_name_F833682236B2695F4EF3508100C9FFE86918E501D562BC55B221D346D30E99D9',\n",
       "       'service_distributor_base_distributor_normalized_name_A096DD969881554D12B5297CD9DF3CB0C6632E882AA28126C8FC410923A74EDF'],\n",
       "      dtype='object', length=487)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6010041, 487)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the new columns we got\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1888195, 487)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19629502296447754\n"
     ]
    }
   ],
   "source": [
    "#Replace all NaN left with 0 No nans left\n",
    "'''t1=time.time()\n",
    "train_X = train_X.fillna(0)\n",
    "test_X = test_X.fillna(0)\n",
    "t2=time.time()\n",
    "print(t2-t1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data quality now\n",
    "train.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data quality now\n",
    "test.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.365723133087158\n"
     ]
    }
   ],
   "source": [
    "#split the data between train and validation set\n",
    "t1=time.time()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train,train_y,test_size=0.33, random_state=42)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4026727, 487)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensions of training data\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1430"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.03269386291504\n"
     ]
    }
   ],
   "source": [
    "#Lets start fitting different models on the data\n",
    "#1. start with simple Bayes model\n",
    "t1=time.time()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gaussian_model = gnb.fit(X_train,y_train)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.27684736251831\n"
     ]
    }
   ],
   "source": [
    "# Lets see the prediction on our validation data\n",
    "t1=time.time()\n",
    "pred = gaussian_model.predict_proba(X_valid)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99568086, 0.00431914])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.173194297593184"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find predicted value on test set\n",
    "pred = gaussian_model.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test['innovation_challenge_key']\n",
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/users/hdpsndbx125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very important to sort the values else won't be taken at submission\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#persist to a file\n",
    "submit.to_csv('NaiveBayesGaussian_EntireTraining.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1888195, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-82a794529cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/dslab/anaconda/python3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    602\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    603\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/dslab/anaconda/python3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "#fitting a multinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "clf.fit(X_train,y_train)\n",
    "pred = clf.predict_proba(X_valid)\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find predicted value on test set\n",
    "pred = clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-fa52ad9ccbbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/dslab/anaconda/python3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    602\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    603\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/dslab/anaconda/python3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "mnb = MultinomialNB(alpha=1.0)\n",
    "mnb.fit(X_train,y_train)\n",
    "pred = mnb.predict_proba(X_valid)\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3439.8842487335205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "t1=time.time()\n",
    "lr1=lr(max_iter=100,verbose=1)\n",
    "lr1.fit(X_train,y_train)\n",
    "pred = lr1.predict_proba(X_valid)\n",
    "log_loss(y_valid,pred[:,1])\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5479825664266159"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find predicted value on test set\n",
    "pred = lr1.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (6,16,17,26,40,41,49,53,54,55,100,104,107,109,112,127,128,129,130,153,171,220,221,222,225,226,227) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_table('/aichallenge/scoring_set.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1888195, 237)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test['scoring_set.innovation_challenge_key']\n",
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=pred[:,1]\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')\n",
    "submit.to_csv('LR_100iter_impute_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1888195, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 4.85, NNZs: 477, Bias: -1.981515, T: 4026727, Avg. loss: 0.965915\n",
      "Total training time: 8.46 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.69, NNZs: 477, Bias: -1.979594, T: 8053454, Avg. loss: 0.561289\n",
      "Total training time: 16.99 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.67, NNZs: 477, Bias: -1.969140, T: 12080181, Avg. loss: 0.555884\n",
      "Total training time: 25.52 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.66, NNZs: 477, Bias: -1.968123, T: 16106908, Avg. loss: 0.553529\n",
      "Total training time: 34.13 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.63, NNZs: 477, Bias: -1.962530, T: 20133635, Avg. loss: 0.552382\n",
      "Total training time: 42.67 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.62, NNZs: 477, Bias: -1.963317, T: 24160362, Avg. loss: 0.551539\n",
      "Total training time: 51.18 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 4.62, NNZs: 477, Bias: -1.958928, T: 28187089, Avg. loss: 0.551055\n",
      "Total training time: 59.67 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.960887, T: 32213816, Avg. loss: 0.550703\n",
      "Total training time: 68.28 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 4.61, NNZs: 477, Bias: -1.959066, T: 36240543, Avg. loss: 0.550398\n",
      "Total training time: 76.86 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 4.61, NNZs: 477, Bias: -1.959904, T: 40267270, Avg. loss: 0.550162\n",
      "Total training time: 85.52 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 4.62, NNZs: 477, Bias: -1.956714, T: 44293997, Avg. loss: 0.549917\n",
      "Total training time: 94.18 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 4.61, NNZs: 477, Bias: -1.956130, T: 48320724, Avg. loss: 0.549807\n",
      "Total training time: 102.73 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 4.61, NNZs: 477, Bias: -1.954968, T: 52347451, Avg. loss: 0.549682\n",
      "Total training time: 111.21 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 4.61, NNZs: 477, Bias: -1.954452, T: 56374178, Avg. loss: 0.549542\n",
      "Total training time: 119.76 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.954276, T: 60400905, Avg. loss: 0.549475\n",
      "Total training time: 128.44 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 4.61, NNZs: 477, Bias: -1.953316, T: 64427632, Avg. loss: 0.549353\n",
      "Total training time: 136.96 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 4.61, NNZs: 477, Bias: -1.955106, T: 68454359, Avg. loss: 0.549305\n",
      "Total training time: 145.55 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.954231, T: 72481086, Avg. loss: 0.549235\n",
      "Total training time: 154.16 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.950523, T: 76507813, Avg. loss: 0.549222\n",
      "Total training time: 162.75 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.951920, T: 80534540, Avg. loss: 0.549130\n",
      "Total training time: 171.35 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.949243, T: 84561267, Avg. loss: 0.549081\n",
      "Total training time: 179.81 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.951552, T: 88587994, Avg. loss: 0.549034\n",
      "Total training time: 188.28 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.952283, T: 92614721, Avg. loss: 0.549017\n",
      "Total training time: 196.87 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.950205, T: 96641448, Avg. loss: 0.548974\n",
      "Total training time: 205.45 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.951606, T: 100668175, Avg. loss: 0.548942\n",
      "Total training time: 214.13 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.949442, T: 104694902, Avg. loss: 0.548926\n",
      "Total training time: 222.75 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.948813, T: 108721629, Avg. loss: 0.548855\n",
      "Total training time: 231.41 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.948784, T: 112748356, Avg. loss: 0.548864\n",
      "Total training time: 239.97 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.948738, T: 116775083, Avg. loss: 0.548832\n",
      "Total training time: 248.54 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.948262, T: 120801810, Avg. loss: 0.548801\n",
      "Total training time: 257.16 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.947869, T: 124828537, Avg. loss: 0.548786\n",
      "Total training time: 265.71 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.947425, T: 128855264, Avg. loss: 0.548784\n",
      "Total training time: 274.34 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.947343, T: 132881991, Avg. loss: 0.548754\n",
      "Total training time: 283.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.947659, T: 136908718, Avg. loss: 0.548729\n",
      "Total training time: 291.63 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.947222, T: 140935445, Avg. loss: 0.548734\n",
      "Total training time: 300.26 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.946653, T: 144962172, Avg. loss: 0.548699\n",
      "Total training time: 308.83 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.947623, T: 148988899, Avg. loss: 0.548689\n",
      "Total training time: 317.34 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.945944, T: 153015626, Avg. loss: 0.548674\n",
      "Total training time: 325.85 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.947460, T: 157042353, Avg. loss: 0.548656\n",
      "Total training time: 334.43 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.946519, T: 161069080, Avg. loss: 0.548662\n",
      "Total training time: 342.95 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.945704, T: 165095807, Avg. loss: 0.548642\n",
      "Total training time: 351.45 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.944392, T: 169122534, Avg. loss: 0.548626\n",
      "Total training time: 359.99 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.944814, T: 173149261, Avg. loss: 0.548616\n",
      "Total training time: 368.64 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.946056, T: 177175988, Avg. loss: 0.548605\n",
      "Total training time: 377.23 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.945618, T: 181202715, Avg. loss: 0.548602\n",
      "Total training time: 385.86 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.945417, T: 185229442, Avg. loss: 0.548590\n",
      "Total training time: 394.45 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.943740, T: 189256169, Avg. loss: 0.548587\n",
      "Total training time: 402.96 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.945192, T: 193282896, Avg. loss: 0.548584\n",
      "Total training time: 411.47 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.944981, T: 197309623, Avg. loss: 0.548559\n",
      "Total training time: 420.08 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.944958, T: 201336350, Avg. loss: 0.548565\n",
      "Total training time: 428.73 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.943875, T: 205363077, Avg. loss: 0.548553\n",
      "Total training time: 437.39 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.944045, T: 209389804, Avg. loss: 0.548547\n",
      "Total training time: 446.32 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.944242, T: 213416531, Avg. loss: 0.548553\n",
      "Total training time: 455.10 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.943821, T: 217443258, Avg. loss: 0.548519\n",
      "Total training time: 463.98 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.942831, T: 221469985, Avg. loss: 0.548531\n",
      "Total training time: 472.88 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.942974, T: 225496712, Avg. loss: 0.548527\n",
      "Total training time: 481.50 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.943088, T: 229523439, Avg. loss: 0.548511\n",
      "Total training time: 490.18 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.942237, T: 233550166, Avg. loss: 0.548505\n",
      "Total training time: 498.89 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.942787, T: 237576893, Avg. loss: 0.548514\n",
      "Total training time: 507.53 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.942527, T: 241603620, Avg. loss: 0.548497\n",
      "Total training time: 516.00 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.942528, T: 245630347, Avg. loss: 0.548490\n",
      "Total training time: 524.61 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.942268, T: 249657074, Avg. loss: 0.548487\n",
      "Total training time: 533.14 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.942804, T: 253683801, Avg. loss: 0.548486\n",
      "Total training time: 541.70 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.942166, T: 257710528, Avg. loss: 0.548477\n",
      "Total training time: 550.31 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.941736, T: 261737255, Avg. loss: 0.548472\n",
      "Total training time: 558.93 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.942581, T: 265763982, Avg. loss: 0.548459\n",
      "Total training time: 567.54 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.942358, T: 269790709, Avg. loss: 0.548466\n",
      "Total training time: 576.05 seconds.\n",
      "-- Epoch 68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.60, NNZs: 477, Bias: -1.941740, T: 273817436, Avg. loss: 0.548473\n",
      "Total training time: 584.53 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.941389, T: 277844163, Avg. loss: 0.548462\n",
      "Total training time: 593.05 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.941924, T: 281870890, Avg. loss: 0.548462\n",
      "Total training time: 601.55 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.941470, T: 285897617, Avg. loss: 0.548451\n",
      "Total training time: 610.13 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.941017, T: 289924344, Avg. loss: 0.548447\n",
      "Total training time: 618.72 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.941627, T: 293951071, Avg. loss: 0.548450\n",
      "Total training time: 627.34 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.941871, T: 297977798, Avg. loss: 0.548444\n",
      "Total training time: 635.96 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.941410, T: 302004525, Avg. loss: 0.548434\n",
      "Total training time: 644.52 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.939894, T: 306031252, Avg. loss: 0.548429\n",
      "Total training time: 653.10 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.940543, T: 310057979, Avg. loss: 0.548432\n",
      "Total training time: 661.81 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.940293, T: 314084706, Avg. loss: 0.548422\n",
      "Total training time: 670.34 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.940972, T: 318111433, Avg. loss: 0.548435\n",
      "Total training time: 678.97 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.940522, T: 322138160, Avg. loss: 0.548414\n",
      "Total training time: 687.65 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.940246, T: 326164887, Avg. loss: 0.548417\n",
      "Total training time: 696.21 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.940567, T: 330191614, Avg. loss: 0.548426\n",
      "Total training time: 704.76 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.940317, T: 334218341, Avg. loss: 0.548412\n",
      "Total training time: 713.36 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.939856, T: 338245068, Avg. loss: 0.548411\n",
      "Total training time: 721.93 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.939418, T: 342271795, Avg. loss: 0.548407\n",
      "Total training time: 730.43 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.940429, T: 346298522, Avg. loss: 0.548414\n",
      "Total training time: 738.97 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.939575, T: 350325249, Avg. loss: 0.548402\n",
      "Total training time: 747.46 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.939383, T: 354351976, Avg. loss: 0.548406\n",
      "Total training time: 756.02 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.939556, T: 358378703, Avg. loss: 0.548402\n",
      "Total training time: 764.51 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.939154, T: 362405430, Avg. loss: 0.548400\n",
      "Total training time: 773.14 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.939740, T: 366432157, Avg. loss: 0.548391\n",
      "Total training time: 781.73 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.939218, T: 370458884, Avg. loss: 0.548395\n",
      "Total training time: 790.50 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.939379, T: 374485611, Avg. loss: 0.548398\n",
      "Total training time: 799.09 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938891, T: 378512338, Avg. loss: 0.548381\n",
      "Total training time: 807.65 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938854, T: 382539065, Avg. loss: 0.548375\n",
      "Total training time: 816.19 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938495, T: 386565792, Avg. loss: 0.548385\n",
      "Total training time: 824.73 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938758, T: 390592519, Avg. loss: 0.548384\n",
      "Total training time: 833.35 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938787, T: 394619246, Avg. loss: 0.548376\n",
      "Total training time: 841.98 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938216, T: 398645973, Avg. loss: 0.548379\n",
      "Total training time: 850.61 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.938440, T: 402672700, Avg. loss: 0.548377\n",
      "Total training time: 859.22 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938387, T: 406699427, Avg. loss: 0.548374\n",
      "Total training time: 867.92 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938716, T: 410726154, Avg. loss: 0.548368\n",
      "Total training time: 876.59 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938171, T: 414752881, Avg. loss: 0.548368\n",
      "Total training time: 885.20 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938932, T: 418779608, Avg. loss: 0.548370\n",
      "Total training time: 893.82 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937953, T: 422806335, Avg. loss: 0.548367\n",
      "Total training time: 902.42 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938094, T: 426833062, Avg. loss: 0.548366\n",
      "Total training time: 910.95 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.938655, T: 430859789, Avg. loss: 0.548363\n",
      "Total training time: 919.47 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938328, T: 434886516, Avg. loss: 0.548364\n",
      "Total training time: 928.01 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937931, T: 438913243, Avg. loss: 0.548359\n",
      "Total training time: 936.55 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938341, T: 442939970, Avg. loss: 0.548352\n",
      "Total training time: 945.06 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.938128, T: 446966697, Avg. loss: 0.548358\n",
      "Total training time: 953.59 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937713, T: 450993424, Avg. loss: 0.548359\n",
      "Total training time: 962.17 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937574, T: 455020151, Avg. loss: 0.548356\n",
      "Total training time: 970.73 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937570, T: 459046878, Avg. loss: 0.548351\n",
      "Total training time: 979.27 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936761, T: 463073605, Avg. loss: 0.548348\n",
      "Total training time: 987.85 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937473, T: 467100332, Avg. loss: 0.548338\n",
      "Total training time: 996.40 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937812, T: 471127059, Avg. loss: 0.548347\n",
      "Total training time: 1004.86 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937467, T: 475153786, Avg. loss: 0.548346\n",
      "Total training time: 1013.38 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937301, T: 479180513, Avg. loss: 0.548352\n",
      "Total training time: 1021.90 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937012, T: 483207240, Avg. loss: 0.548348\n",
      "Total training time: 1030.50 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937210, T: 487233967, Avg. loss: 0.548342\n",
      "Total training time: 1039.17 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.937354, T: 491260694, Avg. loss: 0.548332\n",
      "Total training time: 1047.98 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936922, T: 495287421, Avg. loss: 0.548339\n",
      "Total training time: 1056.66 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 4.60, NNZs: 477, Bias: -1.936499, T: 499314148, Avg. loss: 0.548336\n",
      "Total training time: 1065.20 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936380, T: 503340875, Avg. loss: 0.548343\n",
      "Total training time: 1073.75 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936541, T: 507367602, Avg. loss: 0.548334\n",
      "Total training time: 1082.33 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936600, T: 511394329, Avg. loss: 0.548333\n",
      "Total training time: 1090.93 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936751, T: 515421056, Avg. loss: 0.548331\n",
      "Total training time: 1099.47 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936918, T: 519447783, Avg. loss: 0.548332\n",
      "Total training time: 1108.10 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936495, T: 523474510, Avg. loss: 0.548332\n",
      "Total training time: 1116.77 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936181, T: 527501237, Avg. loss: 0.548331\n",
      "Total training time: 1125.46 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935926, T: 531527964, Avg. loss: 0.548333\n",
      "Total training time: 1134.10 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936059, T: 535554691, Avg. loss: 0.548332\n",
      "Total training time: 1142.73 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935838, T: 539581418, Avg. loss: 0.548333\n",
      "Total training time: 1151.26 seconds.\n",
      "-- Epoch 135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.936014, T: 543608145, Avg. loss: 0.548327\n",
      "Total training time: 1159.84 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936001, T: 547634872, Avg. loss: 0.548326\n",
      "Total training time: 1168.41 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936001, T: 551661599, Avg. loss: 0.548329\n",
      "Total training time: 1176.98 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936141, T: 555688326, Avg. loss: 0.548325\n",
      "Total training time: 1185.56 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935970, T: 559715053, Avg. loss: 0.548316\n",
      "Total training time: 1194.13 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936009, T: 563741780, Avg. loss: 0.548323\n",
      "Total training time: 1202.74 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.936091, T: 567768507, Avg. loss: 0.548323\n",
      "Total training time: 1211.38 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935335, T: 571795234, Avg. loss: 0.548322\n",
      "Total training time: 1220.04 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935454, T: 575821961, Avg. loss: 0.548315\n",
      "Total training time: 1228.58 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935830, T: 579848688, Avg. loss: 0.548316\n",
      "Total training time: 1237.10 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935393, T: 583875415, Avg. loss: 0.548321\n",
      "Total training time: 1245.62 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935344, T: 587902142, Avg. loss: 0.548319\n",
      "Total training time: 1254.14 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935822, T: 591928869, Avg. loss: 0.548318\n",
      "Total training time: 1262.75 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935674, T: 595955596, Avg. loss: 0.548318\n",
      "Total training time: 1271.41 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935376, T: 599982323, Avg. loss: 0.548313\n",
      "Total training time: 1280.05 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934946, T: 604009050, Avg. loss: 0.548309\n",
      "Total training time: 1288.61 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935366, T: 608035777, Avg. loss: 0.548317\n",
      "Total training time: 1297.15 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934948, T: 612062504, Avg. loss: 0.548306\n",
      "Total training time: 1305.69 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935518, T: 616089231, Avg. loss: 0.548307\n",
      "Total training time: 1314.33 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935049, T: 620115958, Avg. loss: 0.548309\n",
      "Total training time: 1322.96 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935170, T: 624142685, Avg. loss: 0.548306\n",
      "Total training time: 1331.48 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934655, T: 628169412, Avg. loss: 0.548306\n",
      "Total training time: 1340.03 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935076, T: 632196139, Avg. loss: 0.548315\n",
      "Total training time: 1348.53 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935510, T: 636222866, Avg. loss: 0.548301\n",
      "Total training time: 1357.06 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934536, T: 640249593, Avg. loss: 0.548306\n",
      "Total training time: 1365.60 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934689, T: 644276320, Avg. loss: 0.548302\n",
      "Total training time: 1374.14 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935347, T: 648303047, Avg. loss: 0.548312\n",
      "Total training time: 1382.71 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934560, T: 652329774, Avg. loss: 0.548306\n",
      "Total training time: 1391.24 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934592, T: 656356501, Avg. loss: 0.548309\n",
      "Total training time: 1399.74 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.935184, T: 660383228, Avg. loss: 0.548300\n",
      "Total training time: 1408.29 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934266, T: 664409955, Avg. loss: 0.548303\n",
      "Total training time: 1416.79 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934600, T: 668436682, Avg. loss: 0.548300\n",
      "Total training time: 1425.31 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934620, T: 672463409, Avg. loss: 0.548301\n",
      "Total training time: 1433.81 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934263, T: 676490136, Avg. loss: 0.548298\n",
      "Total training time: 1442.31 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934660, T: 680516863, Avg. loss: 0.548304\n",
      "Total training time: 1450.83 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934315, T: 684543590, Avg. loss: 0.548297\n",
      "Total training time: 1459.44 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934360, T: 688570317, Avg. loss: 0.548300\n",
      "Total training time: 1468.23 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934113, T: 692597044, Avg. loss: 0.548295\n",
      "Total training time: 1476.86 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934235, T: 696623771, Avg. loss: 0.548299\n",
      "Total training time: 1485.33 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934402, T: 700650498, Avg. loss: 0.548294\n",
      "Total training time: 1493.86 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933877, T: 704677225, Avg. loss: 0.548301\n",
      "Total training time: 1502.31 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.934014, T: 708703952, Avg. loss: 0.548288\n",
      "Total training time: 1510.86 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933717, T: 712730679, Avg. loss: 0.548290\n",
      "Total training time: 1519.44 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933907, T: 716757406, Avg. loss: 0.548289\n",
      "Total training time: 1527.89 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933882, T: 720784133, Avg. loss: 0.548295\n",
      "Total training time: 1536.35 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933559, T: 724810860, Avg. loss: 0.548293\n",
      "Total training time: 1544.80 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933487, T: 728837587, Avg. loss: 0.548290\n",
      "Total training time: 1553.25 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933820, T: 732864314, Avg. loss: 0.548294\n",
      "Total training time: 1561.83 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933458, T: 736891041, Avg. loss: 0.548293\n",
      "Total training time: 1570.45 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933914, T: 740917768, Avg. loss: 0.548291\n",
      "Total training time: 1579.11 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933668, T: 744944495, Avg. loss: 0.548287\n",
      "Total training time: 1587.54 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933843, T: 748971222, Avg. loss: 0.548292\n",
      "Total training time: 1596.12 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933327, T: 752997949, Avg. loss: 0.548288\n",
      "Total training time: 1604.80 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933468, T: 757024676, Avg. loss: 0.548296\n",
      "Total training time: 1613.43 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933042, T: 761051403, Avg. loss: 0.548284\n",
      "Total training time: 1621.98 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933343, T: 765078130, Avg. loss: 0.548288\n",
      "Total training time: 1630.50 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933195, T: 769104857, Avg. loss: 0.548287\n",
      "Total training time: 1639.10 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933427, T: 773131584, Avg. loss: 0.548287\n",
      "Total training time: 1647.56 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933419, T: 777158311, Avg. loss: 0.548287\n",
      "Total training time: 1656.11 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933364, T: 781185038, Avg. loss: 0.548290\n",
      "Total training time: 1664.74 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933696, T: 785211765, Avg. loss: 0.548285\n",
      "Total training time: 1673.29 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933491, T: 789238492, Avg. loss: 0.548286\n",
      "Total training time: 1681.79 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933370, T: 793265219, Avg. loss: 0.548283\n",
      "Total training time: 1690.31 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933059, T: 797291946, Avg. loss: 0.548285\n",
      "Total training time: 1698.93 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933279, T: 801318673, Avg. loss: 0.548286\n",
      "Total training time: 1707.41 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932863, T: 805345400, Avg. loss: 0.548283\n",
      "Total training time: 1715.89 seconds.\n",
      "-- Epoch 201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.932930, T: 809372127, Avg. loss: 0.548280\n",
      "Total training time: 1724.48 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933370, T: 813398854, Avg. loss: 0.548282\n",
      "Total training time: 1733.04 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933140, T: 817425581, Avg. loss: 0.548278\n",
      "Total training time: 1741.53 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933173, T: 821452308, Avg. loss: 0.548284\n",
      "Total training time: 1750.00 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932781, T: 825479035, Avg. loss: 0.548279\n",
      "Total training time: 1758.51 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933111, T: 829505762, Avg. loss: 0.548276\n",
      "Total training time: 1766.98 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.933041, T: 833532489, Avg. loss: 0.548281\n",
      "Total training time: 1775.42 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932487, T: 837559216, Avg. loss: 0.548282\n",
      "Total training time: 1783.82 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932657, T: 841585943, Avg. loss: 0.548275\n",
      "Total training time: 1792.26 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932573, T: 845612670, Avg. loss: 0.548277\n",
      "Total training time: 1800.79 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932521, T: 849639397, Avg. loss: 0.548276\n",
      "Total training time: 1809.38 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932682, T: 853666124, Avg. loss: 0.548277\n",
      "Total training time: 1817.91 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932656, T: 857692851, Avg. loss: 0.548273\n",
      "Total training time: 1826.48 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932587, T: 861719578, Avg. loss: 0.548274\n",
      "Total training time: 1835.01 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932321, T: 865746305, Avg. loss: 0.548272\n",
      "Total training time: 1843.56 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932627, T: 869773032, Avg. loss: 0.548276\n",
      "Total training time: 1852.09 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932185, T: 873799759, Avg. loss: 0.548275\n",
      "Total training time: 1860.59 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932169, T: 877826486, Avg. loss: 0.548273\n",
      "Total training time: 1869.06 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932326, T: 881853213, Avg. loss: 0.548275\n",
      "Total training time: 1877.53 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932452, T: 885879940, Avg. loss: 0.548272\n",
      "Total training time: 1885.99 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932277, T: 889906667, Avg. loss: 0.548279\n",
      "Total training time: 1894.56 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932786, T: 893933394, Avg. loss: 0.548271\n",
      "Total training time: 1903.11 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932150, T: 897960121, Avg. loss: 0.548267\n",
      "Total training time: 1911.63 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932152, T: 958361026, Avg. loss: 0.548270\n",
      "Total training time: 2039.87 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931758, T: 962387753, Avg. loss: 0.548272\n",
      "Total training time: 2048.33 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.932117, T: 966414480, Avg. loss: 0.548264\n",
      "Total training time: 2056.81 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931652, T: 970441207, Avg. loss: 0.548269\n",
      "Total training time: 2065.28 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931617, T: 974467934, Avg. loss: 0.548264\n",
      "Total training time: 2073.90 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931519, T: 978494661, Avg. loss: 0.548265\n",
      "Total training time: 2082.43 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931786, T: 982521388, Avg. loss: 0.548262\n",
      "Total training time: 2090.92 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931432, T: 986548115, Avg. loss: 0.548264\n",
      "Total training time: 2099.37 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931656, T: 990574842, Avg. loss: 0.548265\n",
      "Total training time: 2107.82 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931204, T: 994601569, Avg. loss: 0.548259\n",
      "Total training time: 2116.37 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931213, T: 998628296, Avg. loss: 0.548264\n",
      "Total training time: 2124.95 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931249, T: 1002655023, Avg. loss: 0.548262\n",
      "Total training time: 2133.55 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931495, T: 1006681750, Avg. loss: 0.548262\n",
      "Total training time: 2142.08 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931080, T: 1010708477, Avg. loss: 0.548260\n",
      "Total training time: 2150.61 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931334, T: 1014735204, Avg. loss: 0.548260\n",
      "Total training time: 2159.16 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931029, T: 1018761931, Avg. loss: 0.548265\n",
      "Total training time: 2167.84 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931076, T: 1022788658, Avg. loss: 0.548261\n",
      "Total training time: 2176.43 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931119, T: 1026815385, Avg. loss: 0.548261\n",
      "Total training time: 2185.01 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930939, T: 1030842112, Avg. loss: 0.548265\n",
      "Total training time: 2193.59 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930988, T: 1034868839, Avg. loss: 0.548263\n",
      "Total training time: 2202.20 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930721, T: 1038895566, Avg. loss: 0.548264\n",
      "Total training time: 2210.72 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930998, T: 1042922293, Avg. loss: 0.548260\n",
      "Total training time: 2219.37 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930457, T: 1046949020, Avg. loss: 0.548262\n",
      "Total training time: 2227.88 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930920, T: 1050975747, Avg. loss: 0.548261\n",
      "Total training time: 2236.40 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930940, T: 1055002474, Avg. loss: 0.548261\n",
      "Total training time: 2245.20 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.931183, T: 1059029201, Avg. loss: 0.548258\n",
      "Total training time: 2253.76 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930948, T: 1063055928, Avg. loss: 0.548260\n",
      "Total training time: 2262.29 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930494, T: 1067082655, Avg. loss: 0.548256\n",
      "Total training time: 2270.82 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930864, T: 1071109382, Avg. loss: 0.548259\n",
      "Total training time: 2279.46 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930619, T: 1075136109, Avg. loss: 0.548257\n",
      "Total training time: 2288.12 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930778, T: 1079162836, Avg. loss: 0.548256\n",
      "Total training time: 2296.71 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930703, T: 1083189563, Avg. loss: 0.548258\n",
      "Total training time: 2305.20 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930455, T: 1087216290, Avg. loss: 0.548254\n",
      "Total training time: 2313.64 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930388, T: 1091243017, Avg. loss: 0.548257\n",
      "Total training time: 2322.08 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930759, T: 1095269744, Avg. loss: 0.548255\n",
      "Total training time: 2330.66 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930333, T: 1099296471, Avg. loss: 0.548260\n",
      "Total training time: 2339.27 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930417, T: 1103323198, Avg. loss: 0.548255\n",
      "Total training time: 2347.83 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930356, T: 1107349925, Avg. loss: 0.548253\n",
      "Total training time: 2356.41 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930557, T: 1111376652, Avg. loss: 0.548256\n",
      "Total training time: 2364.94 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930565, T: 1115403379, Avg. loss: 0.548257\n",
      "Total training time: 2373.49 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930266, T: 1119430106, Avg. loss: 0.548255\n",
      "Total training time: 2382.03 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930234, T: 1123456833, Avg. loss: 0.548256\n",
      "Total training time: 2390.66 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930399, T: 1127483560, Avg. loss: 0.548254\n",
      "Total training time: 2399.22 seconds.\n",
      "-- Epoch 281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.930104, T: 1131510287, Avg. loss: 0.548255\n",
      "Total training time: 2407.88 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929847, T: 1135537014, Avg. loss: 0.548251\n",
      "Total training time: 2416.40 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930266, T: 1139563741, Avg. loss: 0.548255\n",
      "Total training time: 2424.90 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930483, T: 1143590468, Avg. loss: 0.548251\n",
      "Total training time: 2433.38 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930547, T: 1147617195, Avg. loss: 0.548251\n",
      "Total training time: 2441.89 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930192, T: 1151643922, Avg. loss: 0.548256\n",
      "Total training time: 2450.37 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930136, T: 1155670649, Avg. loss: 0.548260\n",
      "Total training time: 2458.88 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930450, T: 1159697376, Avg. loss: 0.548247\n",
      "Total training time: 2467.37 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930286, T: 1163724103, Avg. loss: 0.548254\n",
      "Total training time: 2475.91 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929706, T: 1167750830, Avg. loss: 0.548248\n",
      "Total training time: 2484.48 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930342, T: 1171777557, Avg. loss: 0.548252\n",
      "Total training time: 2493.10 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929817, T: 1175804284, Avg. loss: 0.548252\n",
      "Total training time: 2501.61 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930037, T: 1179831011, Avg. loss: 0.548250\n",
      "Total training time: 2510.09 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930087, T: 1183857738, Avg. loss: 0.548253\n",
      "Total training time: 2518.60 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929762, T: 1187884465, Avg. loss: 0.548255\n",
      "Total training time: 2527.06 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929867, T: 1191911192, Avg. loss: 0.548248\n",
      "Total training time: 2535.56 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.930168, T: 1195937919, Avg. loss: 0.548250\n",
      "Total training time: 2544.00 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929962, T: 1199964646, Avg. loss: 0.548252\n",
      "Total training time: 2552.43 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929981, T: 1203991373, Avg. loss: 0.548251\n",
      "Total training time: 2560.92 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929988, T: 1208018100, Avg. loss: 0.548252\n",
      "Total training time: 2569.37 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929965, T: 1212044827, Avg. loss: 0.548244\n",
      "Total training time: 2577.84 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929985, T: 1216071554, Avg. loss: 0.548246\n",
      "Total training time: 2586.33 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929970, T: 1220098281, Avg. loss: 0.548246\n",
      "Total training time: 2594.84 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929352, T: 1224125008, Avg. loss: 0.548249\n",
      "Total training time: 2603.41 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929716, T: 1228151735, Avg. loss: 0.548250\n",
      "Total training time: 2611.94 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929903, T: 1232178462, Avg. loss: 0.548248\n",
      "Total training time: 2620.44 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929418, T: 1236205189, Avg. loss: 0.548248\n",
      "Total training time: 2629.00 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929440, T: 1240231916, Avg. loss: 0.548252\n",
      "Total training time: 2637.50 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929500, T: 1244258643, Avg. loss: 0.548241\n",
      "Total training time: 2646.00 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929737, T: 1248285370, Avg. loss: 0.548251\n",
      "Total training time: 2654.57 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929525, T: 1252312097, Avg. loss: 0.548247\n",
      "Total training time: 2663.23 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929679, T: 1256338824, Avg. loss: 0.548247\n",
      "Total training time: 2671.76 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929782, T: 1260365551, Avg. loss: 0.548246\n",
      "Total training time: 2680.27 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929387, T: 1264392278, Avg. loss: 0.548245\n",
      "Total training time: 2688.80 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929653, T: 1268419005, Avg. loss: 0.548246\n",
      "Total training time: 2697.24 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929291, T: 1272445732, Avg. loss: 0.548249\n",
      "Total training time: 2705.73 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929818, T: 1276472459, Avg. loss: 0.548250\n",
      "Total training time: 2714.21 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929343, T: 1280499186, Avg. loss: 0.548246\n",
      "Total training time: 2722.82 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929341, T: 1284525913, Avg. loss: 0.548246\n",
      "Total training time: 2731.45 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929602, T: 1288552640, Avg. loss: 0.548246\n",
      "Total training time: 2740.00 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929715, T: 1292579367, Avg. loss: 0.548250\n",
      "Total training time: 2748.47 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929238, T: 1296606094, Avg. loss: 0.548245\n",
      "Total training time: 2757.06 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929141, T: 1300632821, Avg. loss: 0.548246\n",
      "Total training time: 2765.69 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929616, T: 1304659548, Avg. loss: 0.548247\n",
      "Total training time: 2774.30 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929260, T: 1308686275, Avg. loss: 0.548243\n",
      "Total training time: 2782.85 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929021, T: 1312713002, Avg. loss: 0.548245\n",
      "Total training time: 2791.36 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928848, T: 1316739729, Avg. loss: 0.548244\n",
      "Total training time: 2799.89 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929425, T: 1320766456, Avg. loss: 0.548246\n",
      "Total training time: 2808.38 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928876, T: 1324793183, Avg. loss: 0.548247\n",
      "Total training time: 2816.95 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929317, T: 1328819910, Avg. loss: 0.548248\n",
      "Total training time: 2825.45 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929030, T: 1332846637, Avg. loss: 0.548247\n",
      "Total training time: 2833.89 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929113, T: 1336873364, Avg. loss: 0.548244\n",
      "Total training time: 2842.34 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928850, T: 1340900091, Avg. loss: 0.548245\n",
      "Total training time: 2850.75 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929035, T: 1344926818, Avg. loss: 0.548243\n",
      "Total training time: 2859.27 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929100, T: 1348953545, Avg. loss: 0.548245\n",
      "Total training time: 2867.77 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929048, T: 1352980272, Avg. loss: 0.548244\n",
      "Total training time: 2876.28 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929193, T: 1357006999, Avg. loss: 0.548243\n",
      "Total training time: 2884.71 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928858, T: 1361033726, Avg. loss: 0.548242\n",
      "Total training time: 2893.14 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928679, T: 1365060453, Avg. loss: 0.548244\n",
      "Total training time: 2901.57 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928684, T: 1369087180, Avg. loss: 0.548243\n",
      "Total training time: 2910.06 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929094, T: 1373113907, Avg. loss: 0.548244\n",
      "Total training time: 2918.58 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929267, T: 1377140634, Avg. loss: 0.548245\n",
      "Total training time: 2927.04 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929031, T: 1381167361, Avg. loss: 0.548243\n",
      "Total training time: 2935.52 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.929011, T: 1385194088, Avg. loss: 0.548239\n",
      "Total training time: 2944.04 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928738, T: 1389220815, Avg. loss: 0.548239\n",
      "Total training time: 2952.53 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928879, T: 1393247542, Avg. loss: 0.548243\n",
      "Total training time: 2961.02 seconds.\n",
      "-- Epoch 347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.928753, T: 1397274269, Avg. loss: 0.548244\n",
      "Total training time: 2969.46 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928701, T: 1401300996, Avg. loss: 0.548241\n",
      "Total training time: 2977.95 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928686, T: 1405327723, Avg. loss: 0.548244\n",
      "Total training time: 2986.41 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928484, T: 1409354450, Avg. loss: 0.548241\n",
      "Total training time: 2995.00 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928673, T: 1413381177, Avg. loss: 0.548238\n",
      "Total training time: 3003.42 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928563, T: 1417407904, Avg. loss: 0.548239\n",
      "Total training time: 3011.85 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928602, T: 1421434631, Avg. loss: 0.548244\n",
      "Total training time: 3020.39 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928485, T: 1425461358, Avg. loss: 0.548239\n",
      "Total training time: 3028.90 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928272, T: 1429488085, Avg. loss: 0.548238\n",
      "Total training time: 3037.39 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928416, T: 1433514812, Avg. loss: 0.548243\n",
      "Total training time: 3045.96 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928802, T: 1437541539, Avg. loss: 0.548244\n",
      "Total training time: 3054.64 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928446, T: 1441568266, Avg. loss: 0.548241\n",
      "Total training time: 3063.21 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928564, T: 1445594993, Avg. loss: 0.548236\n",
      "Total training time: 3071.82 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928454, T: 1449621720, Avg. loss: 0.548237\n",
      "Total training time: 3080.33 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928497, T: 1453648447, Avg. loss: 0.548243\n",
      "Total training time: 3088.72 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928541, T: 1457675174, Avg. loss: 0.548238\n",
      "Total training time: 3097.14 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928637, T: 1461701901, Avg. loss: 0.548240\n",
      "Total training time: 3105.60 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928099, T: 1465728628, Avg. loss: 0.548239\n",
      "Total training time: 3114.08 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928577, T: 1469755355, Avg. loss: 0.548236\n",
      "Total training time: 3122.54 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928362, T: 1473782082, Avg. loss: 0.548238\n",
      "Total training time: 3131.10 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928442, T: 1477808809, Avg. loss: 0.548241\n",
      "Total training time: 3139.68 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928311, T: 1481835536, Avg. loss: 0.548236\n",
      "Total training time: 3148.10 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928138, T: 1485862263, Avg. loss: 0.548236\n",
      "Total training time: 3156.51 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928244, T: 1489888990, Avg. loss: 0.548239\n",
      "Total training time: 3164.93 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927851, T: 1493915717, Avg. loss: 0.548238\n",
      "Total training time: 3173.36 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928096, T: 1497942444, Avg. loss: 0.548239\n",
      "Total training time: 3181.85 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928187, T: 1501969171, Avg. loss: 0.548234\n",
      "Total training time: 3190.25 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928498, T: 1505995898, Avg. loss: 0.548236\n",
      "Total training time: 3198.69 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928047, T: 1510022625, Avg. loss: 0.548237\n",
      "Total training time: 3207.10 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928197, T: 1514049352, Avg. loss: 0.548235\n",
      "Total training time: 3215.53 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928161, T: 1518076079, Avg. loss: 0.548241\n",
      "Total training time: 3223.98 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928147, T: 1522102806, Avg. loss: 0.548236\n",
      "Total training time: 3232.40 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928039, T: 1526129533, Avg. loss: 0.548239\n",
      "Total training time: 3240.84 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928104, T: 1530156260, Avg. loss: 0.548233\n",
      "Total training time: 3249.44 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927747, T: 1534182987, Avg. loss: 0.548236\n",
      "Total training time: 3257.93 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928132, T: 1538209714, Avg. loss: 0.548233\n",
      "Total training time: 3266.47 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927863, T: 1542236441, Avg. loss: 0.548237\n",
      "Total training time: 3274.96 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927615, T: 1546263168, Avg. loss: 0.548232\n",
      "Total training time: 3283.65 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927575, T: 1550289895, Avg. loss: 0.548236\n",
      "Total training time: 3292.27 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928136, T: 1554316622, Avg. loss: 0.548239\n",
      "Total training time: 3300.76 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927658, T: 1558343349, Avg. loss: 0.548236\n",
      "Total training time: 3309.31 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927778, T: 1562370076, Avg. loss: 0.548240\n",
      "Total training time: 3317.81 seconds.\n",
      "-- Epoch 389\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927717, T: 1566396803, Avg. loss: 0.548235\n",
      "Total training time: 3326.31 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927565, T: 1570423530, Avg. loss: 0.548236\n",
      "Total training time: 3334.76 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927572, T: 1574450257, Avg. loss: 0.548234\n",
      "Total training time: 3343.17 seconds.\n",
      "-- Epoch 392\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927708, T: 1578476984, Avg. loss: 0.548236\n",
      "Total training time: 3351.64 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927600, T: 1582503711, Avg. loss: 0.548236\n",
      "Total training time: 3360.23 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.928053, T: 1586530438, Avg. loss: 0.548236\n",
      "Total training time: 3368.81 seconds.\n",
      "-- Epoch 395\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927630, T: 1590557165, Avg. loss: 0.548237\n",
      "Total training time: 3377.34 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927266, T: 1594583892, Avg. loss: 0.548236\n",
      "Total training time: 3385.89 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927904, T: 1598610619, Avg. loss: 0.548233\n",
      "Total training time: 3394.46 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927688, T: 1602637346, Avg. loss: 0.548233\n",
      "Total training time: 3403.00 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927568, T: 1606664073, Avg. loss: 0.548234\n",
      "Total training time: 3411.55 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927350, T: 1610690800, Avg. loss: 0.548233\n",
      "Total training time: 3420.11 seconds.\n",
      "-- Epoch 401\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927724, T: 1614717527, Avg. loss: 0.548237\n",
      "Total training time: 3428.59 seconds.\n",
      "-- Epoch 402\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927735, T: 1618744254, Avg. loss: 0.548231\n",
      "Total training time: 3437.07 seconds.\n",
      "-- Epoch 403\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927840, T: 1622770981, Avg. loss: 0.548233\n",
      "Total training time: 3445.63 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927725, T: 1626797708, Avg. loss: 0.548232\n",
      "Total training time: 3454.05 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927631, T: 1630824435, Avg. loss: 0.548235\n",
      "Total training time: 3462.57 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927528, T: 1634851162, Avg. loss: 0.548233\n",
      "Total training time: 3471.19 seconds.\n",
      "-- Epoch 407\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927525, T: 1638877889, Avg. loss: 0.548235\n",
      "Total training time: 3479.69 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927523, T: 1642904616, Avg. loss: 0.548236\n",
      "Total training time: 3488.19 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927094, T: 1646931343, Avg. loss: 0.548228\n",
      "Total training time: 3496.69 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927866, T: 1650958070, Avg. loss: 0.548237\n",
      "Total training time: 3505.20 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927373, T: 1654984797, Avg. loss: 0.548231\n",
      "Total training time: 3513.72 seconds.\n",
      "-- Epoch 412\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927560, T: 1659011524, Avg. loss: 0.548235\n",
      "Total training time: 3522.22 seconds.\n",
      "-- Epoch 413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.927752, T: 1663038251, Avg. loss: 0.548237\n",
      "Total training time: 3530.65 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927276, T: 1667064978, Avg. loss: 0.548235\n",
      "Total training time: 3539.26 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927389, T: 1671091705, Avg. loss: 0.548232\n",
      "Total training time: 3547.76 seconds.\n",
      "-- Epoch 416\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927319, T: 1675118432, Avg. loss: 0.548233\n",
      "Total training time: 3556.25 seconds.\n",
      "-- Epoch 417\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927329, T: 1679145159, Avg. loss: 0.548235\n",
      "Total training time: 3564.78 seconds.\n",
      "-- Epoch 418\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927372, T: 1683171886, Avg. loss: 0.548232\n",
      "Total training time: 3573.25 seconds.\n",
      "-- Epoch 419\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927378, T: 1687198613, Avg. loss: 0.548232\n",
      "Total training time: 3581.79 seconds.\n",
      "-- Epoch 420\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927132, T: 1691225340, Avg. loss: 0.548233\n",
      "Total training time: 3590.27 seconds.\n",
      "-- Epoch 421\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927394, T: 1695252067, Avg. loss: 0.548230\n",
      "Total training time: 3598.78 seconds.\n",
      "-- Epoch 422\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927303, T: 1699278794, Avg. loss: 0.548231\n",
      "Total training time: 3607.34 seconds.\n",
      "-- Epoch 423\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927194, T: 1703305521, Avg. loss: 0.548233\n",
      "Total training time: 3615.93 seconds.\n",
      "-- Epoch 424\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927289, T: 1707332248, Avg. loss: 0.548234\n",
      "Total training time: 3624.46 seconds.\n",
      "-- Epoch 425\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927169, T: 1711358975, Avg. loss: 0.548231\n",
      "Total training time: 3632.95 seconds.\n",
      "-- Epoch 426\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927204, T: 1715385702, Avg. loss: 0.548231\n",
      "Total training time: 3641.41 seconds.\n",
      "-- Epoch 427\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927253, T: 1719412429, Avg. loss: 0.548230\n",
      "Total training time: 3649.94 seconds.\n",
      "-- Epoch 428\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927073, T: 1723439156, Avg. loss: 0.548228\n",
      "Total training time: 3658.50 seconds.\n",
      "-- Epoch 429\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927089, T: 1727465883, Avg. loss: 0.548230\n",
      "Total training time: 3667.01 seconds.\n",
      "-- Epoch 430\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926691, T: 1731492610, Avg. loss: 0.548230\n",
      "Total training time: 3675.48 seconds.\n",
      "-- Epoch 431\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926805, T: 1735519337, Avg. loss: 0.548232\n",
      "Total training time: 3683.96 seconds.\n",
      "-- Epoch 432\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927394, T: 1739546064, Avg. loss: 0.548232\n",
      "Total training time: 3692.45 seconds.\n",
      "-- Epoch 433\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927170, T: 1743572791, Avg. loss: 0.548229\n",
      "Total training time: 3700.97 seconds.\n",
      "-- Epoch 434\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926680, T: 1747599518, Avg. loss: 0.548228\n",
      "Total training time: 3709.58 seconds.\n",
      "-- Epoch 435\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927003, T: 1751626245, Avg. loss: 0.548230\n",
      "Total training time: 3718.36 seconds.\n",
      "-- Epoch 436\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926899, T: 1755652972, Avg. loss: 0.548229\n",
      "Total training time: 3726.98 seconds.\n",
      "-- Epoch 437\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926879, T: 1759679699, Avg. loss: 0.548231\n",
      "Total training time: 3735.56 seconds.\n",
      "-- Epoch 438\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926909, T: 1763706426, Avg. loss: 0.548231\n",
      "Total training time: 3744.15 seconds.\n",
      "-- Epoch 439\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926953, T: 1767733153, Avg. loss: 0.548229\n",
      "Total training time: 3752.66 seconds.\n",
      "-- Epoch 440\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927000, T: 1771759880, Avg. loss: 0.548227\n",
      "Total training time: 3761.25 seconds.\n",
      "-- Epoch 441\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926804, T: 1775786607, Avg. loss: 0.548230\n",
      "Total training time: 3769.86 seconds.\n",
      "-- Epoch 442\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926744, T: 1779813334, Avg. loss: 0.548233\n",
      "Total training time: 3778.46 seconds.\n",
      "-- Epoch 443\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.927033, T: 1783840061, Avg. loss: 0.548232\n",
      "Total training time: 3787.00 seconds.\n",
      "-- Epoch 444\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926737, T: 1787866788, Avg. loss: 0.548231\n",
      "Total training time: 3795.55 seconds.\n",
      "-- Epoch 445\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926594, T: 1791893515, Avg. loss: 0.548230\n",
      "Total training time: 3804.14 seconds.\n",
      "-- Epoch 446\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926971, T: 1795920242, Avg. loss: 0.548228\n",
      "Total training time: 3812.68 seconds.\n",
      "-- Epoch 447\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926752, T: 1799946969, Avg. loss: 0.548229\n",
      "Total training time: 3821.19 seconds.\n",
      "-- Epoch 448\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926874, T: 1803973696, Avg. loss: 0.548230\n",
      "Total training time: 3829.73 seconds.\n",
      "-- Epoch 449\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926948, T: 1808000423, Avg. loss: 0.548228\n",
      "Total training time: 3838.35 seconds.\n",
      "-- Epoch 450\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926630, T: 1812027150, Avg. loss: 0.548230\n",
      "Total training time: 3846.91 seconds.\n",
      "-- Epoch 451\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926531, T: 1816053877, Avg. loss: 0.548229\n",
      "Total training time: 3855.43 seconds.\n",
      "-- Epoch 452\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926699, T: 1820080604, Avg. loss: 0.548230\n",
      "Total training time: 3863.97 seconds.\n",
      "-- Epoch 453\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926485, T: 1824107331, Avg. loss: 0.548231\n",
      "Total training time: 3872.53 seconds.\n",
      "-- Epoch 454\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926661, T: 1828134058, Avg. loss: 0.548232\n",
      "Total training time: 3881.10 seconds.\n",
      "-- Epoch 455\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926700, T: 1832160785, Avg. loss: 0.548231\n",
      "Total training time: 3889.68 seconds.\n",
      "-- Epoch 456\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926751, T: 1836187512, Avg. loss: 0.548225\n",
      "Total training time: 3898.28 seconds.\n",
      "-- Epoch 457\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926614, T: 1840214239, Avg. loss: 0.548229\n",
      "Total training time: 3906.87 seconds.\n",
      "-- Epoch 458\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926540, T: 1844240966, Avg. loss: 0.548230\n",
      "Total training time: 3915.48 seconds.\n",
      "-- Epoch 459\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926508, T: 1848267693, Avg. loss: 0.548229\n",
      "Total training time: 3924.07 seconds.\n",
      "-- Epoch 460\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926677, T: 1852294420, Avg. loss: 0.548228\n",
      "Total training time: 3932.67 seconds.\n",
      "-- Epoch 461\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926622, T: 1856321147, Avg. loss: 0.548227\n",
      "Total training time: 3941.31 seconds.\n",
      "-- Epoch 462\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926612, T: 1860347874, Avg. loss: 0.548226\n",
      "Total training time: 3949.89 seconds.\n",
      "-- Epoch 463\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926520, T: 1864374601, Avg. loss: 0.548230\n",
      "Total training time: 3958.50 seconds.\n",
      "-- Epoch 464\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926581, T: 1868401328, Avg. loss: 0.548233\n",
      "Total training time: 3966.96 seconds.\n",
      "-- Epoch 465\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926384, T: 1872428055, Avg. loss: 0.548223\n",
      "Total training time: 3975.49 seconds.\n",
      "-- Epoch 466\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926209, T: 1876454782, Avg. loss: 0.548228\n",
      "Total training time: 3984.01 seconds.\n",
      "-- Epoch 467\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926455, T: 1880481509, Avg. loss: 0.548221\n",
      "Total training time: 3992.56 seconds.\n",
      "-- Epoch 468\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926399, T: 1884508236, Avg. loss: 0.548228\n",
      "Total training time: 4001.20 seconds.\n",
      "-- Epoch 469\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926320, T: 1888534963, Avg. loss: 0.548228\n",
      "Total training time: 4009.81 seconds.\n",
      "-- Epoch 470\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926482, T: 1892561690, Avg. loss: 0.548228\n",
      "Total training time: 4018.37 seconds.\n",
      "-- Epoch 471\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926359, T: 1896588417, Avg. loss: 0.548229\n",
      "Total training time: 4027.01 seconds.\n",
      "-- Epoch 472\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926178, T: 1900615144, Avg. loss: 0.548230\n",
      "Total training time: 4035.60 seconds.\n",
      "-- Epoch 473\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926120, T: 1904641871, Avg. loss: 0.548227\n",
      "Total training time: 4044.33 seconds.\n",
      "-- Epoch 474\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926390, T: 1908668598, Avg. loss: 0.548228\n",
      "Total training time: 4052.85 seconds.\n",
      "-- Epoch 475\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926412, T: 1912695325, Avg. loss: 0.548227\n",
      "Total training time: 4061.32 seconds.\n",
      "-- Epoch 476\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926163, T: 1916722052, Avg. loss: 0.548225\n",
      "Total training time: 4069.84 seconds.\n",
      "-- Epoch 477\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926274, T: 1920748779, Avg. loss: 0.548231\n",
      "Total training time: 4078.35 seconds.\n",
      "-- Epoch 478\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926343, T: 1924775506, Avg. loss: 0.548226\n",
      "Total training time: 4086.89 seconds.\n",
      "-- Epoch 479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.926332, T: 1928802233, Avg. loss: 0.548227\n",
      "Total training time: 4095.36 seconds.\n",
      "-- Epoch 480\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926333, T: 1932828960, Avg. loss: 0.548224\n",
      "Total training time: 4104.02 seconds.\n",
      "-- Epoch 481\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926034, T: 1936855687, Avg. loss: 0.548227\n",
      "Total training time: 4112.48 seconds.\n",
      "-- Epoch 482\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926210, T: 1940882414, Avg. loss: 0.548227\n",
      "Total training time: 4121.35 seconds.\n",
      "-- Epoch 483\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926059, T: 1944909141, Avg. loss: 0.548226\n",
      "Total training time: 4129.83 seconds.\n",
      "-- Epoch 484\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925982, T: 1948935868, Avg. loss: 0.548229\n",
      "Total training time: 4138.42 seconds.\n",
      "-- Epoch 485\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925953, T: 1952962595, Avg. loss: 0.548224\n",
      "Total training time: 4147.02 seconds.\n",
      "-- Epoch 486\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926133, T: 1956989322, Avg. loss: 0.548224\n",
      "Total training time: 4155.66 seconds.\n",
      "-- Epoch 487\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926221, T: 1961016049, Avg. loss: 0.548224\n",
      "Total training time: 4164.19 seconds.\n",
      "-- Epoch 488\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926298, T: 1965042776, Avg. loss: 0.548227\n",
      "Total training time: 4172.70 seconds.\n",
      "-- Epoch 489\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926187, T: 1969069503, Avg. loss: 0.548224\n",
      "Total training time: 4181.24 seconds.\n",
      "-- Epoch 490\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926058, T: 1973096230, Avg. loss: 0.548227\n",
      "Total training time: 4189.76 seconds.\n",
      "-- Epoch 491\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926075, T: 1977122957, Avg. loss: 0.548225\n",
      "Total training time: 4198.27 seconds.\n",
      "-- Epoch 492\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926005, T: 1981149684, Avg. loss: 0.548224\n",
      "Total training time: 4206.73 seconds.\n",
      "-- Epoch 493\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925914, T: 1985176411, Avg. loss: 0.548225\n",
      "Total training time: 4215.18 seconds.\n",
      "-- Epoch 494\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925784, T: 1989203138, Avg. loss: 0.548227\n",
      "Total training time: 4223.69 seconds.\n",
      "-- Epoch 495\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925833, T: 1993229865, Avg. loss: 0.548224\n",
      "Total training time: 4232.25 seconds.\n",
      "-- Epoch 496\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925783, T: 1997256592, Avg. loss: 0.548229\n",
      "Total training time: 4240.79 seconds.\n",
      "-- Epoch 497\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926092, T: 2001283319, Avg. loss: 0.548226\n",
      "Total training time: 4249.29 seconds.\n",
      "-- Epoch 498\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925752, T: 2005310046, Avg. loss: 0.548221\n",
      "Total training time: 4257.88 seconds.\n",
      "-- Epoch 499\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925917, T: 2009336773, Avg. loss: 0.548225\n",
      "Total training time: 4266.41 seconds.\n",
      "-- Epoch 500\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925956, T: 2013363500, Avg. loss: 0.548226\n",
      "Total training time: 4275.03 seconds.\n",
      "-- Epoch 501\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925783, T: 2017390227, Avg. loss: 0.548223\n",
      "Total training time: 4283.65 seconds.\n",
      "-- Epoch 502\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925644, T: 2021416954, Avg. loss: 0.548225\n",
      "Total training time: 4292.20 seconds.\n",
      "-- Epoch 503\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925781, T: 2025443681, Avg. loss: 0.548223\n",
      "Total training time: 4300.78 seconds.\n",
      "-- Epoch 504\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925754, T: 2029470408, Avg. loss: 0.548222\n",
      "Total training time: 4309.33 seconds.\n",
      "-- Epoch 505\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925473, T: 2033497135, Avg. loss: 0.548225\n",
      "Total training time: 4317.90 seconds.\n",
      "-- Epoch 506\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.926040, T: 2037523862, Avg. loss: 0.548223\n",
      "Total training time: 4326.48 seconds.\n",
      "-- Epoch 507\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925877, T: 2041550589, Avg. loss: 0.548225\n",
      "Total training time: 4335.04 seconds.\n",
      "-- Epoch 508\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925795, T: 2045577316, Avg. loss: 0.548222\n",
      "Total training time: 4343.61 seconds.\n",
      "-- Epoch 509\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925748, T: 2049604043, Avg. loss: 0.548220\n",
      "Total training time: 4352.12 seconds.\n",
      "-- Epoch 510\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925693, T: 2053630770, Avg. loss: 0.548222\n",
      "Total training time: 4360.74 seconds.\n",
      "-- Epoch 511\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925699, T: 2057657497, Avg. loss: 0.548227\n",
      "Total training time: 4369.28 seconds.\n",
      "-- Epoch 512\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925647, T: 2061684224, Avg. loss: 0.548224\n",
      "Total training time: 4377.87 seconds.\n",
      "-- Epoch 513\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925657, T: 2065710951, Avg. loss: 0.548225\n",
      "Total training time: 4386.42 seconds.\n",
      "-- Epoch 514\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925637, T: 2069737678, Avg. loss: 0.548224\n",
      "Total training time: 4395.06 seconds.\n",
      "-- Epoch 515\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925715, T: 2073764405, Avg. loss: 0.548222\n",
      "Total training time: 4403.81 seconds.\n",
      "-- Epoch 516\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925601, T: 2077791132, Avg. loss: 0.548222\n",
      "Total training time: 4412.45 seconds.\n",
      "-- Epoch 517\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925376, T: 2081817859, Avg. loss: 0.548225\n",
      "Total training time: 4420.98 seconds.\n",
      "-- Epoch 518\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925617, T: 2085844586, Avg. loss: 0.548224\n",
      "Total training time: 4429.64 seconds.\n",
      "-- Epoch 519\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925763, T: 2089871313, Avg. loss: 0.548222\n",
      "Total training time: 4438.27 seconds.\n",
      "-- Epoch 520\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925688, T: 2093898040, Avg. loss: 0.548224\n",
      "Total training time: 4446.79 seconds.\n",
      "-- Epoch 521\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925461, T: 2097924767, Avg. loss: 0.548223\n",
      "Total training time: 4455.33 seconds.\n",
      "-- Epoch 522\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925470, T: 2101951494, Avg. loss: 0.548225\n",
      "Total training time: 4463.87 seconds.\n",
      "-- Epoch 523\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925553, T: 2105978221, Avg. loss: 0.548225\n",
      "Total training time: 4472.47 seconds.\n",
      "-- Epoch 524\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925569, T: 2110004948, Avg. loss: 0.548221\n",
      "Total training time: 4481.07 seconds.\n",
      "-- Epoch 525\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925551, T: 2114031675, Avg. loss: 0.548223\n",
      "Total training time: 4489.76 seconds.\n",
      "-- Epoch 526\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925383, T: 2118058402, Avg. loss: 0.548223\n",
      "Total training time: 4498.36 seconds.\n",
      "-- Epoch 527\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925478, T: 2122085129, Avg. loss: 0.548223\n",
      "Total training time: 4507.64 seconds.\n",
      "-- Epoch 528\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925542, T: 2126111856, Avg. loss: 0.548222\n",
      "Total training time: 4516.25 seconds.\n",
      "-- Epoch 529\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925231, T: 2130138583, Avg. loss: 0.548221\n",
      "Total training time: 4524.89 seconds.\n",
      "-- Epoch 530\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925416, T: 2134165310, Avg. loss: 0.548224\n",
      "Total training time: 4533.39 seconds.\n",
      "-- Epoch 531\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925428, T: 2138192037, Avg. loss: 0.548222\n",
      "Total training time: 4541.92 seconds.\n",
      "-- Epoch 532\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925291, T: 2142218764, Avg. loss: 0.548220\n",
      "Total training time: 4550.47 seconds.\n",
      "-- Epoch 533\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925654, T: 2146245491, Avg. loss: 0.548225\n",
      "Total training time: 4558.96 seconds.\n",
      "-- Epoch 534\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925394, T: 2150272218, Avg. loss: 0.548221\n",
      "Total training time: 4567.41 seconds.\n",
      "-- Epoch 535\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925438, T: 2154298945, Avg. loss: 0.548222\n",
      "Total training time: 4575.92 seconds.\n",
      "-- Epoch 536\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925347, T: 2158325672, Avg. loss: 0.548220\n",
      "Total training time: 4584.48 seconds.\n",
      "-- Epoch 537\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925553, T: 2162352399, Avg. loss: 0.548222\n",
      "Total training time: 4593.07 seconds.\n",
      "-- Epoch 538\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925349, T: 2166379126, Avg. loss: 0.548222\n",
      "Total training time: 4601.76 seconds.\n",
      "-- Epoch 539\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925381, T: 2170405853, Avg. loss: 0.548219\n",
      "Total training time: 4610.40 seconds.\n",
      "-- Epoch 540\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925424, T: 2174432580, Avg. loss: 0.548217\n",
      "Total training time: 4619.00 seconds.\n",
      "-- Epoch 541\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925072, T: 2178459307, Avg. loss: 0.548220\n",
      "Total training time: 4627.67 seconds.\n",
      "-- Epoch 542\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925266, T: 2182486034, Avg. loss: 0.548219\n",
      "Total training time: 4636.20 seconds.\n",
      "-- Epoch 543\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925195, T: 2186512761, Avg. loss: 0.548220\n",
      "Total training time: 4644.83 seconds.\n",
      "-- Epoch 544\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925285, T: 2190539488, Avg. loss: 0.548221\n",
      "Total training time: 4653.40 seconds.\n",
      "-- Epoch 545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.925214, T: 2194566215, Avg. loss: 0.548223\n",
      "Total training time: 4662.00 seconds.\n",
      "-- Epoch 546\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925273, T: 2198592942, Avg. loss: 0.548218\n",
      "Total training time: 4670.59 seconds.\n",
      "-- Epoch 547\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924949, T: 2202619669, Avg. loss: 0.548219\n",
      "Total training time: 4679.13 seconds.\n",
      "-- Epoch 548\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925156, T: 2206646396, Avg. loss: 0.548222\n",
      "Total training time: 4687.70 seconds.\n",
      "-- Epoch 549\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925275, T: 2210673123, Avg. loss: 0.548220\n",
      "Total training time: 4696.30 seconds.\n",
      "-- Epoch 550\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924892, T: 2214699850, Avg. loss: 0.548218\n",
      "Total training time: 4704.87 seconds.\n",
      "-- Epoch 551\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925066, T: 2218726577, Avg. loss: 0.548219\n",
      "Total training time: 4713.46 seconds.\n",
      "-- Epoch 552\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925158, T: 2222753304, Avg. loss: 0.548222\n",
      "Total training time: 4722.10 seconds.\n",
      "-- Epoch 553\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925155, T: 2226780031, Avg. loss: 0.548220\n",
      "Total training time: 4730.60 seconds.\n",
      "-- Epoch 554\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924934, T: 2230806758, Avg. loss: 0.548223\n",
      "Total training time: 4739.13 seconds.\n",
      "-- Epoch 555\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924949, T: 2234833485, Avg. loss: 0.548218\n",
      "Total training time: 4747.71 seconds.\n",
      "-- Epoch 556\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925168, T: 2238860212, Avg. loss: 0.548224\n",
      "Total training time: 4756.24 seconds.\n",
      "-- Epoch 557\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925318, T: 2242886939, Avg. loss: 0.548221\n",
      "Total training time: 4764.81 seconds.\n",
      "-- Epoch 558\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924855, T: 2246913666, Avg. loss: 0.548222\n",
      "Total training time: 4773.34 seconds.\n",
      "-- Epoch 559\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925139, T: 2250940393, Avg. loss: 0.548219\n",
      "Total training time: 4781.79 seconds.\n",
      "-- Epoch 560\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925175, T: 2254967120, Avg. loss: 0.548220\n",
      "Total training time: 4790.35 seconds.\n",
      "-- Epoch 561\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924990, T: 2258993847, Avg. loss: 0.548218\n",
      "Total training time: 4798.83 seconds.\n",
      "-- Epoch 562\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925038, T: 2263020574, Avg. loss: 0.548222\n",
      "Total training time: 4807.37 seconds.\n",
      "-- Epoch 563\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924878, T: 2267047301, Avg. loss: 0.548222\n",
      "Total training time: 4815.92 seconds.\n",
      "-- Epoch 564\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924850, T: 2271074028, Avg. loss: 0.548221\n",
      "Total training time: 4824.41 seconds.\n",
      "-- Epoch 565\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924756, T: 2275100755, Avg. loss: 0.548219\n",
      "Total training time: 4833.03 seconds.\n",
      "-- Epoch 566\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924915, T: 2279127482, Avg. loss: 0.548222\n",
      "Total training time: 4841.56 seconds.\n",
      "-- Epoch 567\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924819, T: 2283154209, Avg. loss: 0.548219\n",
      "Total training time: 4850.13 seconds.\n",
      "-- Epoch 568\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924849, T: 2287180936, Avg. loss: 0.548220\n",
      "Total training time: 4858.82 seconds.\n",
      "-- Epoch 569\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924802, T: 2291207663, Avg. loss: 0.548219\n",
      "Total training time: 4867.41 seconds.\n",
      "-- Epoch 570\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925052, T: 2295234390, Avg. loss: 0.548220\n",
      "Total training time: 4875.95 seconds.\n",
      "-- Epoch 571\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924768, T: 2299261117, Avg. loss: 0.548218\n",
      "Total training time: 4884.52 seconds.\n",
      "-- Epoch 572\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925135, T: 2303287844, Avg. loss: 0.548221\n",
      "Total training time: 4893.02 seconds.\n",
      "-- Epoch 573\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924764, T: 2307314571, Avg. loss: 0.548220\n",
      "Total training time: 4901.52 seconds.\n",
      "-- Epoch 574\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924898, T: 2311341298, Avg. loss: 0.548221\n",
      "Total training time: 4910.00 seconds.\n",
      "-- Epoch 575\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924893, T: 2315368025, Avg. loss: 0.548219\n",
      "Total training time: 4918.47 seconds.\n",
      "-- Epoch 576\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924939, T: 2319394752, Avg. loss: 0.548217\n",
      "Total training time: 4927.10 seconds.\n",
      "-- Epoch 577\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924671, T: 2323421479, Avg. loss: 0.548220\n",
      "Total training time: 4935.74 seconds.\n",
      "-- Epoch 578\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.925008, T: 2327448206, Avg. loss: 0.548218\n",
      "Total training time: 4944.27 seconds.\n",
      "-- Epoch 579\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924886, T: 2331474933, Avg. loss: 0.548217\n",
      "Total training time: 4952.82 seconds.\n",
      "-- Epoch 580\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924659, T: 2335501660, Avg. loss: 0.548222\n",
      "Total training time: 4961.35 seconds.\n",
      "-- Epoch 581\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924794, T: 2339528387, Avg. loss: 0.548220\n",
      "Total training time: 4969.91 seconds.\n",
      "-- Epoch 582\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924794, T: 2343555114, Avg. loss: 0.548219\n",
      "Total training time: 4978.63 seconds.\n",
      "-- Epoch 583\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924713, T: 2347581841, Avg. loss: 0.548219\n",
      "Total training time: 4987.27 seconds.\n",
      "-- Epoch 584\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924713, T: 2351608568, Avg. loss: 0.548220\n",
      "Total training time: 4995.86 seconds.\n",
      "-- Epoch 585\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924516, T: 2355635295, Avg. loss: 0.548221\n",
      "Total training time: 5004.44 seconds.\n",
      "-- Epoch 586\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924787, T: 2359662022, Avg. loss: 0.548216\n",
      "Total training time: 5013.01 seconds.\n",
      "-- Epoch 587\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924807, T: 2363688749, Avg. loss: 0.548219\n",
      "Total training time: 5021.63 seconds.\n",
      "-- Epoch 588\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924653, T: 2367715476, Avg. loss: 0.548221\n",
      "Total training time: 5030.12 seconds.\n",
      "-- Epoch 589\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924453, T: 2371742203, Avg. loss: 0.548220\n",
      "Total training time: 5038.58 seconds.\n",
      "-- Epoch 590\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924353, T: 2375768930, Avg. loss: 0.548216\n",
      "Total training time: 5047.08 seconds.\n",
      "-- Epoch 591\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924593, T: 2379795657, Avg. loss: 0.548217\n",
      "Total training time: 5055.62 seconds.\n",
      "-- Epoch 592\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924448, T: 2383822384, Avg. loss: 0.548221\n",
      "Total training time: 5064.07 seconds.\n",
      "-- Epoch 593\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924634, T: 2387849111, Avg. loss: 0.548218\n",
      "Total training time: 5072.50 seconds.\n",
      "-- Epoch 594\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924692, T: 2391875838, Avg. loss: 0.548221\n",
      "Total training time: 5081.49 seconds.\n",
      "-- Epoch 595\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924730, T: 2395902565, Avg. loss: 0.548219\n",
      "Total training time: 5090.02 seconds.\n",
      "-- Epoch 596\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924415, T: 2399929292, Avg. loss: 0.548220\n",
      "Total training time: 5098.52 seconds.\n",
      "-- Epoch 597\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924546, T: 2403956019, Avg. loss: 0.548221\n",
      "Total training time: 5107.05 seconds.\n",
      "-- Epoch 598\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924524, T: 2407982746, Avg. loss: 0.548218\n",
      "Total training time: 5115.58 seconds.\n",
      "-- Epoch 599\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924441, T: 2412009473, Avg. loss: 0.548219\n",
      "Total training time: 5124.12 seconds.\n",
      "-- Epoch 600\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924153, T: 2416036200, Avg. loss: 0.548221\n",
      "Total training time: 5132.72 seconds.\n",
      "-- Epoch 601\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924543, T: 2420062927, Avg. loss: 0.548219\n",
      "Total training time: 5141.20 seconds.\n",
      "-- Epoch 602\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924399, T: 2424089654, Avg. loss: 0.548217\n",
      "Total training time: 5149.77 seconds.\n",
      "-- Epoch 603\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924537, T: 2428116381, Avg. loss: 0.548217\n",
      "Total training time: 5158.24 seconds.\n",
      "-- Epoch 604\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924367, T: 2432143108, Avg. loss: 0.548219\n",
      "Total training time: 5166.75 seconds.\n",
      "-- Epoch 605\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924546, T: 2436169835, Avg. loss: 0.548217\n",
      "Total training time: 5175.26 seconds.\n",
      "-- Epoch 606\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924531, T: 2440196562, Avg. loss: 0.548216\n",
      "Total training time: 5183.69 seconds.\n",
      "-- Epoch 607\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924323, T: 2444223289, Avg. loss: 0.548218\n",
      "Total training time: 5192.14 seconds.\n",
      "-- Epoch 608\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924487, T: 2448250016, Avg. loss: 0.548221\n",
      "Total training time: 5200.71 seconds.\n",
      "-- Epoch 609\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924375, T: 2452276743, Avg. loss: 0.548215\n",
      "Total training time: 5209.21 seconds.\n",
      "-- Epoch 610\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924309, T: 2456303470, Avg. loss: 0.548218\n",
      "Total training time: 5217.78 seconds.\n",
      "-- Epoch 611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.924157, T: 2460330197, Avg. loss: 0.548218\n",
      "Total training time: 5226.37 seconds.\n",
      "-- Epoch 612\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924283, T: 2464356924, Avg. loss: 0.548217\n",
      "Total training time: 5235.04 seconds.\n",
      "-- Epoch 613\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924296, T: 2468383651, Avg. loss: 0.548219\n",
      "Total training time: 5243.68 seconds.\n",
      "-- Epoch 614\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924382, T: 2472410378, Avg. loss: 0.548218\n",
      "Total training time: 5252.22 seconds.\n",
      "-- Epoch 615\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924233, T: 2476437105, Avg. loss: 0.548216\n",
      "Total training time: 5260.79 seconds.\n",
      "-- Epoch 616\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924332, T: 2480463832, Avg. loss: 0.548219\n",
      "Total training time: 5269.40 seconds.\n",
      "-- Epoch 617\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924197, T: 2484490559, Avg. loss: 0.548219\n",
      "Total training time: 5277.96 seconds.\n",
      "-- Epoch 618\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924041, T: 2488517286, Avg. loss: 0.548220\n",
      "Total training time: 5286.55 seconds.\n",
      "-- Epoch 619\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924108, T: 2492544013, Avg. loss: 0.548219\n",
      "Total training time: 5295.02 seconds.\n",
      "-- Epoch 620\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924037, T: 2496570740, Avg. loss: 0.548217\n",
      "Total training time: 5303.57 seconds.\n",
      "-- Epoch 621\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924302, T: 2500597467, Avg. loss: 0.548216\n",
      "Total training time: 5312.02 seconds.\n",
      "-- Epoch 622\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924325, T: 2504624194, Avg. loss: 0.548216\n",
      "Total training time: 5320.45 seconds.\n",
      "-- Epoch 623\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924138, T: 2508650921, Avg. loss: 0.548216\n",
      "Total training time: 5328.92 seconds.\n",
      "-- Epoch 624\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924113, T: 2512677648, Avg. loss: 0.548218\n",
      "Total training time: 5337.50 seconds.\n",
      "-- Epoch 625\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924084, T: 2516704375, Avg. loss: 0.548218\n",
      "Total training time: 5346.07 seconds.\n",
      "-- Epoch 626\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924045, T: 2520731102, Avg. loss: 0.548215\n",
      "Total training time: 5354.69 seconds.\n",
      "-- Epoch 627\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924067, T: 2524757829, Avg. loss: 0.548218\n",
      "Total training time: 5363.32 seconds.\n",
      "-- Epoch 628\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924182, T: 2528784556, Avg. loss: 0.548218\n",
      "Total training time: 5371.96 seconds.\n",
      "-- Epoch 629\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924207, T: 2532811283, Avg. loss: 0.548216\n",
      "Total training time: 5380.63 seconds.\n",
      "-- Epoch 630\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924310, T: 2536838010, Avg. loss: 0.548219\n",
      "Total training time: 5389.18 seconds.\n",
      "-- Epoch 631\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924113, T: 2540864737, Avg. loss: 0.548217\n",
      "Total training time: 5397.76 seconds.\n",
      "-- Epoch 632\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924272, T: 2544891464, Avg. loss: 0.548216\n",
      "Total training time: 5406.31 seconds.\n",
      "-- Epoch 633\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924188, T: 2548918191, Avg. loss: 0.548217\n",
      "Total training time: 5414.85 seconds.\n",
      "-- Epoch 634\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923934, T: 2552944918, Avg. loss: 0.548217\n",
      "Total training time: 5423.41 seconds.\n",
      "-- Epoch 635\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924029, T: 2556971645, Avg. loss: 0.548218\n",
      "Total training time: 5431.97 seconds.\n",
      "-- Epoch 636\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923844, T: 2560998372, Avg. loss: 0.548217\n",
      "Total training time: 5440.54 seconds.\n",
      "-- Epoch 637\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924238, T: 2565025099, Avg. loss: 0.548215\n",
      "Total training time: 5449.17 seconds.\n",
      "-- Epoch 638\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923869, T: 2569051826, Avg. loss: 0.548218\n",
      "Total training time: 5457.86 seconds.\n",
      "-- Epoch 639\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924173, T: 2573078553, Avg. loss: 0.548215\n",
      "Total training time: 5466.55 seconds.\n",
      "-- Epoch 640\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923961, T: 2577105280, Avg. loss: 0.548216\n",
      "Total training time: 5475.16 seconds.\n",
      "-- Epoch 641\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924209, T: 2581132007, Avg. loss: 0.548215\n",
      "Total training time: 5483.81 seconds.\n",
      "-- Epoch 642\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924071, T: 2585158734, Avg. loss: 0.548219\n",
      "Total training time: 5492.42 seconds.\n",
      "-- Epoch 643\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924012, T: 2589185461, Avg. loss: 0.548217\n",
      "Total training time: 5501.07 seconds.\n",
      "-- Epoch 644\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923735, T: 2593212188, Avg. loss: 0.548218\n",
      "Total training time: 5509.62 seconds.\n",
      "-- Epoch 645\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923805, T: 2597238915, Avg. loss: 0.548217\n",
      "Total training time: 5518.14 seconds.\n",
      "-- Epoch 646\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923966, T: 2601265642, Avg. loss: 0.548216\n",
      "Total training time: 5526.87 seconds.\n",
      "-- Epoch 647\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923930, T: 2605292369, Avg. loss: 0.548213\n",
      "Total training time: 5535.43 seconds.\n",
      "-- Epoch 648\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923990, T: 2609319096, Avg. loss: 0.548215\n",
      "Total training time: 5544.11 seconds.\n",
      "-- Epoch 649\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.924057, T: 2613345823, Avg. loss: 0.548215\n",
      "Total training time: 5552.70 seconds.\n",
      "-- Epoch 650\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923801, T: 2617372550, Avg. loss: 0.548215\n",
      "Total training time: 5561.26 seconds.\n",
      "-- Epoch 651\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923800, T: 2621399277, Avg. loss: 0.548218\n",
      "Total training time: 5569.78 seconds.\n",
      "-- Epoch 652\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923806, T: 2625426004, Avg. loss: 0.548217\n",
      "Total training time: 5578.36 seconds.\n",
      "-- Epoch 653\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923658, T: 2629452731, Avg. loss: 0.548217\n",
      "Total training time: 5586.88 seconds.\n",
      "-- Epoch 654\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923590, T: 2633479458, Avg. loss: 0.548218\n",
      "Total training time: 5595.46 seconds.\n",
      "-- Epoch 655\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923914, T: 2637506185, Avg. loss: 0.548217\n",
      "Total training time: 5604.09 seconds.\n",
      "-- Epoch 656\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923711, T: 2641532912, Avg. loss: 0.548215\n",
      "Total training time: 5612.77 seconds.\n",
      "-- Epoch 657\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923844, T: 2645559639, Avg. loss: 0.548215\n",
      "Total training time: 5621.35 seconds.\n",
      "-- Epoch 658\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923747, T: 2649586366, Avg. loss: 0.548215\n",
      "Total training time: 5629.90 seconds.\n",
      "-- Epoch 659\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923675, T: 2653613093, Avg. loss: 0.548216\n",
      "Total training time: 5638.45 seconds.\n",
      "-- Epoch 660\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923687, T: 2657639820, Avg. loss: 0.548217\n",
      "Total training time: 5646.99 seconds.\n",
      "-- Epoch 661\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923477, T: 2661666547, Avg. loss: 0.548218\n",
      "Total training time: 5655.66 seconds.\n",
      "-- Epoch 662\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923557, T: 2665693274, Avg. loss: 0.548215\n",
      "Total training time: 5664.27 seconds.\n",
      "-- Epoch 663\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923714, T: 2669720001, Avg. loss: 0.548218\n",
      "Total training time: 5672.81 seconds.\n",
      "-- Epoch 664\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923833, T: 2673746728, Avg. loss: 0.548213\n",
      "Total training time: 5681.29 seconds.\n",
      "-- Epoch 665\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923682, T: 2677773455, Avg. loss: 0.548215\n",
      "Total training time: 5689.85 seconds.\n",
      "-- Epoch 666\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923851, T: 2681800182, Avg. loss: 0.548214\n",
      "Total training time: 5698.42 seconds.\n",
      "-- Epoch 667\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923574, T: 2685826909, Avg. loss: 0.548215\n",
      "Total training time: 5707.04 seconds.\n",
      "-- Epoch 668\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923867, T: 2689853636, Avg. loss: 0.548217\n",
      "Total training time: 5715.60 seconds.\n",
      "-- Epoch 669\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923748, T: 2693880363, Avg. loss: 0.548216\n",
      "Total training time: 5724.16 seconds.\n",
      "-- Epoch 670\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923667, T: 2697907090, Avg. loss: 0.548219\n",
      "Total training time: 5732.63 seconds.\n",
      "-- Epoch 671\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923733, T: 2701933817, Avg. loss: 0.548216\n",
      "Total training time: 5741.28 seconds.\n",
      "-- Epoch 672\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923612, T: 2705960544, Avg. loss: 0.548216\n",
      "Total training time: 5749.91 seconds.\n",
      "-- Epoch 673\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923602, T: 2709987271, Avg. loss: 0.548216\n",
      "Total training time: 5758.49 seconds.\n",
      "-- Epoch 674\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923379, T: 2714013998, Avg. loss: 0.548217\n",
      "Total training time: 5767.01 seconds.\n",
      "-- Epoch 675\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923564, T: 2718040725, Avg. loss: 0.548213\n",
      "Total training time: 5775.55 seconds.\n",
      "-- Epoch 676\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923528, T: 2722067452, Avg. loss: 0.548211\n",
      "Total training time: 5784.21 seconds.\n",
      "-- Epoch 677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.923703, T: 2726094179, Avg. loss: 0.548214\n",
      "Total training time: 5793.00 seconds.\n",
      "-- Epoch 678\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923536, T: 2730120906, Avg. loss: 0.548211\n",
      "Total training time: 5801.52 seconds.\n",
      "-- Epoch 679\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923594, T: 2734147633, Avg. loss: 0.548216\n",
      "Total training time: 5810.08 seconds.\n",
      "-- Epoch 680\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923483, T: 2738174360, Avg. loss: 0.548217\n",
      "Total training time: 5818.66 seconds.\n",
      "-- Epoch 681\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923520, T: 2742201087, Avg. loss: 0.548215\n",
      "Total training time: 5827.28 seconds.\n",
      "-- Epoch 682\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923469, T: 2746227814, Avg. loss: 0.548213\n",
      "Total training time: 5835.87 seconds.\n",
      "-- Epoch 683\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923465, T: 2750254541, Avg. loss: 0.548216\n",
      "Total training time: 5844.53 seconds.\n",
      "-- Epoch 684\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923292, T: 2754281268, Avg. loss: 0.548216\n",
      "Total training time: 5853.15 seconds.\n",
      "-- Epoch 685\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923452, T: 2758307995, Avg. loss: 0.548214\n",
      "Total training time: 5861.74 seconds.\n",
      "-- Epoch 686\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923382, T: 2762334722, Avg. loss: 0.548216\n",
      "Total training time: 5870.32 seconds.\n",
      "-- Epoch 687\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923553, T: 2766361449, Avg. loss: 0.548214\n",
      "Total training time: 5878.90 seconds.\n",
      "-- Epoch 688\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923617, T: 2770388176, Avg. loss: 0.548214\n",
      "Total training time: 5887.49 seconds.\n",
      "-- Epoch 689\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923612, T: 2774414903, Avg. loss: 0.548215\n",
      "Total training time: 5896.26 seconds.\n",
      "-- Epoch 690\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923328, T: 2778441630, Avg. loss: 0.548214\n",
      "Total training time: 5904.87 seconds.\n",
      "-- Epoch 691\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923406, T: 2782468357, Avg. loss: 0.548215\n",
      "Total training time: 5913.49 seconds.\n",
      "-- Epoch 692\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923423, T: 2786495084, Avg. loss: 0.548214\n",
      "Total training time: 5922.03 seconds.\n",
      "-- Epoch 693\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923277, T: 2790521811, Avg. loss: 0.548214\n",
      "Total training time: 5930.66 seconds.\n",
      "-- Epoch 694\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923507, T: 2794548538, Avg. loss: 0.548212\n",
      "Total training time: 5939.22 seconds.\n",
      "-- Epoch 695\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923053, T: 2798575265, Avg. loss: 0.548215\n",
      "Total training time: 5947.76 seconds.\n",
      "-- Epoch 696\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923352, T: 2802601992, Avg. loss: 0.548215\n",
      "Total training time: 5956.35 seconds.\n",
      "-- Epoch 697\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923222, T: 2806628719, Avg. loss: 0.548215\n",
      "Total training time: 5964.91 seconds.\n",
      "-- Epoch 698\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923220, T: 2810655446, Avg. loss: 0.548215\n",
      "Total training time: 5973.44 seconds.\n",
      "-- Epoch 699\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923326, T: 2814682173, Avg. loss: 0.548215\n",
      "Total training time: 5982.02 seconds.\n",
      "-- Epoch 700\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923232, T: 2818708900, Avg. loss: 0.548213\n",
      "Total training time: 5990.64 seconds.\n",
      "-- Epoch 701\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923379, T: 2822735627, Avg. loss: 0.548216\n",
      "Total training time: 5999.26 seconds.\n",
      "-- Epoch 702\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923132, T: 2826762354, Avg. loss: 0.548211\n",
      "Total training time: 6007.88 seconds.\n",
      "-- Epoch 703\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923071, T: 2830789081, Avg. loss: 0.548214\n",
      "Total training time: 6016.47 seconds.\n",
      "-- Epoch 704\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923227, T: 2834815808, Avg. loss: 0.548212\n",
      "Total training time: 6025.13 seconds.\n",
      "-- Epoch 705\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923237, T: 2838842535, Avg. loss: 0.548213\n",
      "Total training time: 6033.79 seconds.\n",
      "-- Epoch 706\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923472, T: 2842869262, Avg. loss: 0.548215\n",
      "Total training time: 6042.32 seconds.\n",
      "-- Epoch 707\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923523, T: 2846895989, Avg. loss: 0.548212\n",
      "Total training time: 6050.97 seconds.\n",
      "-- Epoch 708\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923205, T: 2850922716, Avg. loss: 0.548214\n",
      "Total training time: 6059.61 seconds.\n",
      "-- Epoch 709\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923209, T: 2854949443, Avg. loss: 0.548213\n",
      "Total training time: 6068.17 seconds.\n",
      "-- Epoch 710\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923235, T: 2858976170, Avg. loss: 0.548213\n",
      "Total training time: 6076.73 seconds.\n",
      "-- Epoch 711\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923165, T: 2863002897, Avg. loss: 0.548214\n",
      "Total training time: 6085.20 seconds.\n",
      "-- Epoch 712\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923115, T: 2867029624, Avg. loss: 0.548211\n",
      "Total training time: 6093.68 seconds.\n",
      "-- Epoch 713\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923060, T: 2871056351, Avg. loss: 0.548215\n",
      "Total training time: 6102.23 seconds.\n",
      "-- Epoch 714\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923255, T: 2875083078, Avg. loss: 0.548215\n",
      "Total training time: 6110.70 seconds.\n",
      "-- Epoch 715\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923195, T: 2879109805, Avg. loss: 0.548212\n",
      "Total training time: 6119.21 seconds.\n",
      "-- Epoch 716\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922903, T: 2883136532, Avg. loss: 0.548215\n",
      "Total training time: 6127.72 seconds.\n",
      "-- Epoch 717\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923034, T: 2887163259, Avg. loss: 0.548212\n",
      "Total training time: 6136.31 seconds.\n",
      "-- Epoch 718\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923080, T: 2891189986, Avg. loss: 0.548211\n",
      "Total training time: 6144.91 seconds.\n",
      "-- Epoch 719\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923172, T: 2895216713, Avg. loss: 0.548213\n",
      "Total training time: 6153.47 seconds.\n",
      "-- Epoch 720\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923165, T: 2899243440, Avg. loss: 0.548212\n",
      "Total training time: 6162.08 seconds.\n",
      "-- Epoch 721\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922932, T: 2903270167, Avg. loss: 0.548214\n",
      "Total training time: 6170.74 seconds.\n",
      "-- Epoch 722\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923225, T: 2907296894, Avg. loss: 0.548214\n",
      "Total training time: 6179.40 seconds.\n",
      "-- Epoch 723\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922989, T: 2911323621, Avg. loss: 0.548214\n",
      "Total training time: 6188.09 seconds.\n",
      "-- Epoch 724\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922960, T: 2915350348, Avg. loss: 0.548215\n",
      "Total training time: 6196.73 seconds.\n",
      "-- Epoch 725\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923344, T: 2919377075, Avg. loss: 0.548211\n",
      "Total training time: 6205.39 seconds.\n",
      "-- Epoch 726\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923142, T: 2923403802, Avg. loss: 0.548212\n",
      "Total training time: 6213.95 seconds.\n",
      "-- Epoch 727\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922852, T: 2927430529, Avg. loss: 0.548213\n",
      "Total training time: 6222.55 seconds.\n",
      "-- Epoch 728\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923207, T: 2931457256, Avg. loss: 0.548214\n",
      "Total training time: 6231.15 seconds.\n",
      "-- Epoch 729\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922953, T: 2935483983, Avg. loss: 0.548215\n",
      "Total training time: 6239.71 seconds.\n",
      "-- Epoch 730\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922970, T: 2939510710, Avg. loss: 0.548210\n",
      "Total training time: 6248.28 seconds.\n",
      "-- Epoch 731\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923278, T: 2943537437, Avg. loss: 0.548212\n",
      "Total training time: 6256.85 seconds.\n",
      "-- Epoch 732\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923098, T: 2947564164, Avg. loss: 0.548213\n",
      "Total training time: 6265.77 seconds.\n",
      "-- Epoch 733\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922913, T: 2951590891, Avg. loss: 0.548214\n",
      "Total training time: 6274.40 seconds.\n",
      "-- Epoch 734\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922912, T: 2955617618, Avg. loss: 0.548214\n",
      "Total training time: 6282.94 seconds.\n",
      "-- Epoch 735\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922949, T: 2959644345, Avg. loss: 0.548211\n",
      "Total training time: 6291.50 seconds.\n",
      "-- Epoch 736\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923076, T: 2963671072, Avg. loss: 0.548212\n",
      "Total training time: 6300.08 seconds.\n",
      "-- Epoch 737\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922876, T: 2967697799, Avg. loss: 0.548212\n",
      "Total training time: 6308.73 seconds.\n",
      "-- Epoch 738\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922835, T: 2971724526, Avg. loss: 0.548211\n",
      "Total training time: 6317.28 seconds.\n",
      "-- Epoch 739\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922852, T: 2975751253, Avg. loss: 0.548213\n",
      "Total training time: 6325.78 seconds.\n",
      "-- Epoch 740\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922645, T: 2979777980, Avg. loss: 0.548211\n",
      "Total training time: 6334.35 seconds.\n",
      "-- Epoch 741\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.923046, T: 2983804707, Avg. loss: 0.548213\n",
      "Total training time: 6342.85 seconds.\n",
      "-- Epoch 742\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922658, T: 2987831434, Avg. loss: 0.548213\n",
      "Total training time: 6351.38 seconds.\n",
      "-- Epoch 743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.922747, T: 2991858161, Avg. loss: 0.548212\n",
      "Total training time: 6359.93 seconds.\n",
      "-- Epoch 744\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922915, T: 2995884888, Avg. loss: 0.548212\n",
      "Total training time: 6368.42 seconds.\n",
      "-- Epoch 745\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922824, T: 2999911615, Avg. loss: 0.548210\n",
      "Total training time: 6377.00 seconds.\n",
      "-- Epoch 746\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922664, T: 3003938342, Avg. loss: 0.548212\n",
      "Total training time: 6385.53 seconds.\n",
      "-- Epoch 747\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922607, T: 3007965069, Avg. loss: 0.548214\n",
      "Total training time: 6394.34 seconds.\n",
      "-- Epoch 748\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922791, T: 3011991796, Avg. loss: 0.548209\n",
      "Total training time: 6402.95 seconds.\n",
      "-- Epoch 749\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922552, T: 3016018523, Avg. loss: 0.548215\n",
      "Total training time: 6412.07 seconds.\n",
      "-- Epoch 750\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922727, T: 3020045250, Avg. loss: 0.548214\n",
      "Total training time: 6420.63 seconds.\n",
      "-- Epoch 751\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922653, T: 3024071977, Avg. loss: 0.548209\n",
      "Total training time: 6429.25 seconds.\n",
      "-- Epoch 752\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922798, T: 3028098704, Avg. loss: 0.548211\n",
      "Total training time: 6437.91 seconds.\n",
      "-- Epoch 753\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922862, T: 3032125431, Avg. loss: 0.548214\n",
      "Total training time: 6446.47 seconds.\n",
      "-- Epoch 754\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922650, T: 3036152158, Avg. loss: 0.548209\n",
      "Total training time: 6454.97 seconds.\n",
      "-- Epoch 755\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922921, T: 3040178885, Avg. loss: 0.548212\n",
      "Total training time: 6463.57 seconds.\n",
      "-- Epoch 756\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922727, T: 3044205612, Avg. loss: 0.548212\n",
      "Total training time: 6472.14 seconds.\n",
      "-- Epoch 757\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922634, T: 3048232339, Avg. loss: 0.548211\n",
      "Total training time: 6480.75 seconds.\n",
      "-- Epoch 758\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922734, T: 3052259066, Avg. loss: 0.548212\n",
      "Total training time: 6489.37 seconds.\n",
      "-- Epoch 759\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922537, T: 3056285793, Avg. loss: 0.548212\n",
      "Total training time: 6498.00 seconds.\n",
      "-- Epoch 760\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922455, T: 3060312520, Avg. loss: 0.548211\n",
      "Total training time: 6506.56 seconds.\n",
      "-- Epoch 761\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922616, T: 3064339247, Avg. loss: 0.548209\n",
      "Total training time: 6515.11 seconds.\n",
      "-- Epoch 762\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922781, T: 3068365974, Avg. loss: 0.548210\n",
      "Total training time: 6523.70 seconds.\n",
      "-- Epoch 763\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922615, T: 3072392701, Avg. loss: 0.548212\n",
      "Total training time: 6532.30 seconds.\n",
      "-- Epoch 764\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922790, T: 3076419428, Avg. loss: 0.548212\n",
      "Total training time: 6540.87 seconds.\n",
      "-- Epoch 765\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922540, T: 3080446155, Avg. loss: 0.548213\n",
      "Total training time: 6549.50 seconds.\n",
      "-- Epoch 766\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922529, T: 3084472882, Avg. loss: 0.548210\n",
      "Total training time: 6558.16 seconds.\n",
      "-- Epoch 767\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922463, T: 3088499609, Avg. loss: 0.548213\n",
      "Total training time: 6566.64 seconds.\n",
      "-- Epoch 768\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922452, T: 3092526336, Avg. loss: 0.548213\n",
      "Total training time: 6575.14 seconds.\n",
      "-- Epoch 769\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922515, T: 3096553063, Avg. loss: 0.548211\n",
      "Total training time: 6583.69 seconds.\n",
      "-- Epoch 770\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922571, T: 3100579790, Avg. loss: 0.548212\n",
      "Total training time: 6592.23 seconds.\n",
      "-- Epoch 771\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922584, T: 3104606517, Avg. loss: 0.548210\n",
      "Total training time: 6600.74 seconds.\n",
      "-- Epoch 772\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922293, T: 3108633244, Avg. loss: 0.548211\n",
      "Total training time: 6609.27 seconds.\n",
      "-- Epoch 773\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922453, T: 3112659971, Avg. loss: 0.548210\n",
      "Total training time: 6617.83 seconds.\n",
      "-- Epoch 774\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922511, T: 3116686698, Avg. loss: 0.548210\n",
      "Total training time: 6626.40 seconds.\n",
      "-- Epoch 775\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922604, T: 3120713425, Avg. loss: 0.548212\n",
      "Total training time: 6635.00 seconds.\n",
      "-- Epoch 776\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922581, T: 3124740152, Avg. loss: 0.548211\n",
      "Total training time: 6643.56 seconds.\n",
      "-- Epoch 777\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922407, T: 3128766879, Avg. loss: 0.548211\n",
      "Total training time: 6652.12 seconds.\n",
      "-- Epoch 778\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922478, T: 3132793606, Avg. loss: 0.548213\n",
      "Total training time: 6660.73 seconds.\n",
      "-- Epoch 779\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922398, T: 3136820333, Avg. loss: 0.548210\n",
      "Total training time: 6669.39 seconds.\n",
      "-- Epoch 780\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922340, T: 3140847060, Avg. loss: 0.548209\n",
      "Total training time: 6678.09 seconds.\n",
      "-- Epoch 781\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922308, T: 3144873787, Avg. loss: 0.548212\n",
      "Total training time: 6686.83 seconds.\n",
      "-- Epoch 782\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922344, T: 3148900514, Avg. loss: 0.548211\n",
      "Total training time: 6695.49 seconds.\n",
      "-- Epoch 783\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922332, T: 3152927241, Avg. loss: 0.548213\n",
      "Total training time: 6704.07 seconds.\n",
      "-- Epoch 784\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922329, T: 3156953968, Avg. loss: 0.548212\n",
      "Total training time: 6712.67 seconds.\n",
      "-- Epoch 785\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922381, T: 3160980695, Avg. loss: 0.548212\n",
      "Total training time: 6721.26 seconds.\n",
      "-- Epoch 786\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922468, T: 3165007422, Avg. loss: 0.548211\n",
      "Total training time: 6729.91 seconds.\n",
      "-- Epoch 787\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922302, T: 3169034149, Avg. loss: 0.548213\n",
      "Total training time: 6738.54 seconds.\n",
      "-- Epoch 788\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922465, T: 3173060876, Avg. loss: 0.548209\n",
      "Total training time: 6747.11 seconds.\n",
      "-- Epoch 789\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922345, T: 3177087603, Avg. loss: 0.548211\n",
      "Total training time: 6756.04 seconds.\n",
      "-- Epoch 790\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922434, T: 3181114330, Avg. loss: 0.548211\n",
      "Total training time: 6764.66 seconds.\n",
      "-- Epoch 791\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922248, T: 3185141057, Avg. loss: 0.548214\n",
      "Total training time: 6773.18 seconds.\n",
      "-- Epoch 792\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922191, T: 3189167784, Avg. loss: 0.548209\n",
      "Total training time: 6781.71 seconds.\n",
      "-- Epoch 793\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922225, T: 3193194511, Avg. loss: 0.548209\n",
      "Total training time: 6790.23 seconds.\n",
      "-- Epoch 794\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922236, T: 3197221238, Avg. loss: 0.548212\n",
      "Total training time: 6798.77 seconds.\n",
      "-- Epoch 795\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922230, T: 3201247965, Avg. loss: 0.548212\n",
      "Total training time: 6807.39 seconds.\n",
      "-- Epoch 796\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922490, T: 3205274692, Avg. loss: 0.548210\n",
      "Total training time: 6816.03 seconds.\n",
      "-- Epoch 797\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921933, T: 3209301419, Avg. loss: 0.548209\n",
      "Total training time: 6824.60 seconds.\n",
      "-- Epoch 798\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922413, T: 3213328146, Avg. loss: 0.548209\n",
      "Total training time: 6833.36 seconds.\n",
      "-- Epoch 799\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922157, T: 3217354873, Avg. loss: 0.548210\n",
      "Total training time: 6841.97 seconds.\n",
      "-- Epoch 800\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922167, T: 3221381600, Avg. loss: 0.548210\n",
      "Total training time: 6850.58 seconds.\n",
      "-- Epoch 801\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922195, T: 3225408327, Avg. loss: 0.548209\n",
      "Total training time: 6859.24 seconds.\n",
      "-- Epoch 802\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922251, T: 3229435054, Avg. loss: 0.548210\n",
      "Total training time: 6867.85 seconds.\n",
      "-- Epoch 803\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922147, T: 3233461781, Avg. loss: 0.548209\n",
      "Total training time: 6876.44 seconds.\n",
      "-- Epoch 804\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922289, T: 3237488508, Avg. loss: 0.548210\n",
      "Total training time: 6884.99 seconds.\n",
      "-- Epoch 805\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922176, T: 3241515235, Avg. loss: 0.548210\n",
      "Total training time: 6893.58 seconds.\n",
      "-- Epoch 806\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922398, T: 3245541962, Avg. loss: 0.548211\n",
      "Total training time: 6902.17 seconds.\n",
      "-- Epoch 807\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922327, T: 3249568689, Avg. loss: 0.548211\n",
      "Total training time: 6910.87 seconds.\n",
      "-- Epoch 808\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922236, T: 3253595416, Avg. loss: 0.548207\n",
      "Total training time: 6919.52 seconds.\n",
      "-- Epoch 809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.922137, T: 3257622143, Avg. loss: 0.548211\n",
      "Total training time: 6927.98 seconds.\n",
      "-- Epoch 810\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922062, T: 3261648870, Avg. loss: 0.548210\n",
      "Total training time: 6936.49 seconds.\n",
      "-- Epoch 811\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922174, T: 3265675597, Avg. loss: 0.548211\n",
      "Total training time: 6945.09 seconds.\n",
      "-- Epoch 812\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922273, T: 3269702324, Avg. loss: 0.548212\n",
      "Total training time: 6953.66 seconds.\n",
      "-- Epoch 813\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922114, T: 3273729051, Avg. loss: 0.548210\n",
      "Total training time: 6962.40 seconds.\n",
      "-- Epoch 814\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922108, T: 3277755778, Avg. loss: 0.548211\n",
      "Total training time: 6971.04 seconds.\n",
      "-- Epoch 815\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922170, T: 3281782505, Avg. loss: 0.548212\n",
      "Total training time: 6979.73 seconds.\n",
      "-- Epoch 816\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922187, T: 3285809232, Avg. loss: 0.548206\n",
      "Total training time: 6988.37 seconds.\n",
      "-- Epoch 817\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922260, T: 3289835959, Avg. loss: 0.548207\n",
      "Total training time: 6996.96 seconds.\n",
      "-- Epoch 818\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921914, T: 3293862686, Avg. loss: 0.548209\n",
      "Total training time: 7005.53 seconds.\n",
      "-- Epoch 819\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921872, T: 3297889413, Avg. loss: 0.548210\n",
      "Total training time: 7014.09 seconds.\n",
      "-- Epoch 820\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922036, T: 3301916140, Avg. loss: 0.548209\n",
      "Total training time: 7022.65 seconds.\n",
      "-- Epoch 821\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922010, T: 3305942867, Avg. loss: 0.548211\n",
      "Total training time: 7031.20 seconds.\n",
      "-- Epoch 822\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922231, T: 3309969594, Avg. loss: 0.548209\n",
      "Total training time: 7039.83 seconds.\n",
      "-- Epoch 823\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921945, T: 3313996321, Avg. loss: 0.548211\n",
      "Total training time: 7048.49 seconds.\n",
      "-- Epoch 824\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922210, T: 3318023048, Avg. loss: 0.548214\n",
      "Total training time: 7056.99 seconds.\n",
      "-- Epoch 825\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922152, T: 3322049775, Avg. loss: 0.548210\n",
      "Total training time: 7065.63 seconds.\n",
      "-- Epoch 826\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921946, T: 3326076502, Avg. loss: 0.548212\n",
      "Total training time: 7074.23 seconds.\n",
      "-- Epoch 827\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921843, T: 3330103229, Avg. loss: 0.548206\n",
      "Total training time: 7082.85 seconds.\n",
      "-- Epoch 828\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921952, T: 3334129956, Avg. loss: 0.548211\n",
      "Total training time: 7091.46 seconds.\n",
      "-- Epoch 829\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921890, T: 3338156683, Avg. loss: 0.548213\n",
      "Total training time: 7100.05 seconds.\n",
      "-- Epoch 830\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921871, T: 3342183410, Avg. loss: 0.548209\n",
      "Total training time: 7108.63 seconds.\n",
      "-- Epoch 831\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922110, T: 3346210137, Avg. loss: 0.548210\n",
      "Total training time: 7117.18 seconds.\n",
      "-- Epoch 832\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921946, T: 3350236864, Avg. loss: 0.548210\n",
      "Total training time: 7125.67 seconds.\n",
      "-- Epoch 833\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921887, T: 3354263591, Avg. loss: 0.548212\n",
      "Total training time: 7134.29 seconds.\n",
      "-- Epoch 834\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.922186, T: 3358290318, Avg. loss: 0.548210\n",
      "Total training time: 7142.80 seconds.\n",
      "-- Epoch 835\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921931, T: 3362317045, Avg. loss: 0.548212\n",
      "Total training time: 7151.28 seconds.\n",
      "-- Epoch 836\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921915, T: 3366343772, Avg. loss: 0.548208\n",
      "Total training time: 7159.77 seconds.\n",
      "-- Epoch 837\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921863, T: 3370370499, Avg. loss: 0.548209\n",
      "Total training time: 7168.46 seconds.\n",
      "-- Epoch 838\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921862, T: 3374397226, Avg. loss: 0.548208\n",
      "Total training time: 7177.09 seconds.\n",
      "-- Epoch 839\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921795, T: 3378423953, Avg. loss: 0.548207\n",
      "Total training time: 7185.70 seconds.\n",
      "-- Epoch 840\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921868, T: 3382450680, Avg. loss: 0.548210\n",
      "Total training time: 7194.26 seconds.\n",
      "-- Epoch 841\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921868, T: 3386477407, Avg. loss: 0.548209\n",
      "Total training time: 7202.87 seconds.\n",
      "-- Epoch 842\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921802, T: 3390504134, Avg. loss: 0.548207\n",
      "Total training time: 7211.44 seconds.\n",
      "-- Epoch 843\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921779, T: 3394530861, Avg. loss: 0.548211\n",
      "Total training time: 7220.02 seconds.\n",
      "-- Epoch 844\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921799, T: 3398557588, Avg. loss: 0.548209\n",
      "Total training time: 7228.59 seconds.\n",
      "-- Epoch 845\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921809, T: 3402584315, Avg. loss: 0.548211\n",
      "Total training time: 7237.16 seconds.\n",
      "-- Epoch 846\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921914, T: 3406611042, Avg. loss: 0.548206\n",
      "Total training time: 7245.76 seconds.\n",
      "-- Epoch 847\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921808, T: 3410637769, Avg. loss: 0.548207\n",
      "Total training time: 7254.40 seconds.\n",
      "-- Epoch 848\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921717, T: 3414664496, Avg. loss: 0.548207\n",
      "Total training time: 7263.01 seconds.\n",
      "-- Epoch 849\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921863, T: 3418691223, Avg. loss: 0.548208\n",
      "Total training time: 7271.56 seconds.\n",
      "-- Epoch 850\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921881, T: 3422717950, Avg. loss: 0.548209\n",
      "Total training time: 7280.17 seconds.\n",
      "-- Epoch 851\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921616, T: 3426744677, Avg. loss: 0.548206\n",
      "Total training time: 7288.87 seconds.\n",
      "-- Epoch 852\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921866, T: 3430771404, Avg. loss: 0.548208\n",
      "Total training time: 7297.46 seconds.\n",
      "-- Epoch 853\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921746, T: 3434798131, Avg. loss: 0.548211\n",
      "Total training time: 7306.06 seconds.\n",
      "-- Epoch 854\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921821, T: 3438824858, Avg. loss: 0.548210\n",
      "Total training time: 7314.64 seconds.\n",
      "-- Epoch 855\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921680, T: 3442851585, Avg. loss: 0.548208\n",
      "Total training time: 7323.18 seconds.\n",
      "-- Epoch 856\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921814, T: 3446878312, Avg. loss: 0.548211\n",
      "Total training time: 7331.79 seconds.\n",
      "-- Epoch 857\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921751, T: 3450905039, Avg. loss: 0.548209\n",
      "Total training time: 7340.36 seconds.\n",
      "-- Epoch 858\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921813, T: 3454931766, Avg. loss: 0.548207\n",
      "Total training time: 7348.95 seconds.\n",
      "-- Epoch 859\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921654, T: 3458958493, Avg. loss: 0.548212\n",
      "Total training time: 7357.56 seconds.\n",
      "-- Epoch 860\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921546, T: 3462985220, Avg. loss: 0.548207\n",
      "Total training time: 7366.17 seconds.\n",
      "-- Epoch 861\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921643, T: 3467011947, Avg. loss: 0.548210\n",
      "Total training time: 7374.66 seconds.\n",
      "-- Epoch 862\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921790, T: 3471038674, Avg. loss: 0.548206\n",
      "Total training time: 7383.16 seconds.\n",
      "-- Epoch 863\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921667, T: 3475065401, Avg. loss: 0.548208\n",
      "Total training time: 7391.71 seconds.\n",
      "-- Epoch 864\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921558, T: 3479092128, Avg. loss: 0.548206\n",
      "Total training time: 7400.28 seconds.\n",
      "-- Epoch 865\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921855, T: 3483118855, Avg. loss: 0.548209\n",
      "Total training time: 7408.93 seconds.\n",
      "-- Epoch 866\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921649, T: 3487145582, Avg. loss: 0.548207\n",
      "Total training time: 7417.62 seconds.\n",
      "-- Epoch 867\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921465, T: 3491172309, Avg. loss: 0.548210\n",
      "Total training time: 7426.28 seconds.\n",
      "-- Epoch 868\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921586, T: 3495199036, Avg. loss: 0.548207\n",
      "Total training time: 7434.88 seconds.\n",
      "-- Epoch 869\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921736, T: 3499225763, Avg. loss: 0.548208\n",
      "Total training time: 7443.46 seconds.\n",
      "-- Epoch 870\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921664, T: 3503252490, Avg. loss: 0.548208\n",
      "Total training time: 7452.01 seconds.\n",
      "-- Epoch 871\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921618, T: 3507279217, Avg. loss: 0.548207\n",
      "Total training time: 7460.62 seconds.\n",
      "-- Epoch 872\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921644, T: 3511305944, Avg. loss: 0.548208\n",
      "Total training time: 7469.18 seconds.\n",
      "-- Epoch 873\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921564, T: 3515332671, Avg. loss: 0.548211\n",
      "Total training time: 7477.74 seconds.\n",
      "-- Epoch 874\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921634, T: 3519359398, Avg. loss: 0.548211\n",
      "Total training time: 7486.25 seconds.\n",
      "-- Epoch 875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.921616, T: 3523386125, Avg. loss: 0.548206\n",
      "Total training time: 7494.74 seconds.\n",
      "-- Epoch 876\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921465, T: 3527412852, Avg. loss: 0.548210\n",
      "Total training time: 7503.32 seconds.\n",
      "-- Epoch 877\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921517, T: 3531439579, Avg. loss: 0.548210\n",
      "Total training time: 7511.97 seconds.\n",
      "-- Epoch 878\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921328, T: 3535466306, Avg. loss: 0.548208\n",
      "Total training time: 7520.67 seconds.\n",
      "-- Epoch 879\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921542, T: 3539493033, Avg. loss: 0.548210\n",
      "Total training time: 7529.30 seconds.\n",
      "-- Epoch 880\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921414, T: 3543519760, Avg. loss: 0.548209\n",
      "Total training time: 7537.83 seconds.\n",
      "-- Epoch 881\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921552, T: 3547546487, Avg. loss: 0.548208\n",
      "Total training time: 7546.31 seconds.\n",
      "-- Epoch 882\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921314, T: 3551573214, Avg. loss: 0.548208\n",
      "Total training time: 7554.81 seconds.\n",
      "-- Epoch 883\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921563, T: 3555599941, Avg. loss: 0.548209\n",
      "Total training time: 7563.43 seconds.\n",
      "-- Epoch 884\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921524, T: 3559626668, Avg. loss: 0.548208\n",
      "Total training time: 7572.02 seconds.\n",
      "-- Epoch 885\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921402, T: 3563653395, Avg. loss: 0.548206\n",
      "Total training time: 7580.61 seconds.\n",
      "-- Epoch 886\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921392, T: 3567680122, Avg. loss: 0.548208\n",
      "Total training time: 7589.15 seconds.\n",
      "-- Epoch 887\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921320, T: 3571706849, Avg. loss: 0.548208\n",
      "Total training time: 7597.69 seconds.\n",
      "-- Epoch 888\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921298, T: 3575733576, Avg. loss: 0.548210\n",
      "Total training time: 7606.28 seconds.\n",
      "-- Epoch 889\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921451, T: 3579760303, Avg. loss: 0.548209\n",
      "Total training time: 7614.87 seconds.\n",
      "-- Epoch 890\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921333, T: 3583787030, Avg. loss: 0.548207\n",
      "Total training time: 7623.45 seconds.\n",
      "-- Epoch 891\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921503, T: 3587813757, Avg. loss: 0.548210\n",
      "Total training time: 7632.03 seconds.\n",
      "-- Epoch 892\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921228, T: 3591840484, Avg. loss: 0.548209\n",
      "Total training time: 7640.57 seconds.\n",
      "-- Epoch 893\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921472, T: 3595867211, Avg. loss: 0.548208\n",
      "Total training time: 7649.13 seconds.\n",
      "-- Epoch 894\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921640, T: 3599893938, Avg. loss: 0.548208\n",
      "Total training time: 7657.77 seconds.\n",
      "-- Epoch 895\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921418, T: 3603920665, Avg. loss: 0.548207\n",
      "Total training time: 7666.38 seconds.\n",
      "-- Epoch 896\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921425, T: 3607947392, Avg. loss: 0.548204\n",
      "Total training time: 7674.91 seconds.\n",
      "-- Epoch 897\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921242, T: 3611974119, Avg. loss: 0.548208\n",
      "Total training time: 7683.44 seconds.\n",
      "-- Epoch 898\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921557, T: 3616000846, Avg. loss: 0.548205\n",
      "Total training time: 7692.16 seconds.\n",
      "-- Epoch 899\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921369, T: 3620027573, Avg. loss: 0.548211\n",
      "Total training time: 7700.78 seconds.\n",
      "-- Epoch 900\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921252, T: 3624054300, Avg. loss: 0.548206\n",
      "Total training time: 7709.34 seconds.\n",
      "-- Epoch 901\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921273, T: 3628081027, Avg. loss: 0.548207\n",
      "Total training time: 7717.95 seconds.\n",
      "-- Epoch 902\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921444, T: 3632107754, Avg. loss: 0.548209\n",
      "Total training time: 7726.57 seconds.\n",
      "-- Epoch 903\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921365, T: 3636134481, Avg. loss: 0.548208\n",
      "Total training time: 7735.10 seconds.\n",
      "-- Epoch 904\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921625, T: 3640161208, Avg. loss: 0.548207\n",
      "Total training time: 7743.73 seconds.\n",
      "-- Epoch 905\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921280, T: 3644187935, Avg. loss: 0.548209\n",
      "Total training time: 7752.32 seconds.\n",
      "-- Epoch 906\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921217, T: 3648214662, Avg. loss: 0.548207\n",
      "Total training time: 7760.83 seconds.\n",
      "-- Epoch 907\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921200, T: 3652241389, Avg. loss: 0.548208\n",
      "Total training time: 7769.41 seconds.\n",
      "-- Epoch 908\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921205, T: 3656268116, Avg. loss: 0.548208\n",
      "Total training time: 7777.99 seconds.\n",
      "-- Epoch 909\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921200, T: 3660294843, Avg. loss: 0.548208\n",
      "Total training time: 7786.60 seconds.\n",
      "-- Epoch 910\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921414, T: 3664321570, Avg. loss: 0.548207\n",
      "Total training time: 7795.15 seconds.\n",
      "-- Epoch 911\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921194, T: 3668348297, Avg. loss: 0.548208\n",
      "Total training time: 7803.68 seconds.\n",
      "-- Epoch 912\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921181, T: 3672375024, Avg. loss: 0.548207\n",
      "Total training time: 7812.26 seconds.\n",
      "-- Epoch 913\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921239, T: 3676401751, Avg. loss: 0.548209\n",
      "Total training time: 7820.82 seconds.\n",
      "-- Epoch 914\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921437, T: 3680428478, Avg. loss: 0.548207\n",
      "Total training time: 7829.38 seconds.\n",
      "-- Epoch 915\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921337, T: 3684455205, Avg. loss: 0.548209\n",
      "Total training time: 7837.90 seconds.\n",
      "-- Epoch 916\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921156, T: 3688481932, Avg. loss: 0.548203\n",
      "Total training time: 7846.47 seconds.\n",
      "-- Epoch 917\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921342, T: 3692508659, Avg. loss: 0.548207\n",
      "Total training time: 7854.95 seconds.\n",
      "-- Epoch 918\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921152, T: 3696535386, Avg. loss: 0.548209\n",
      "Total training time: 7863.52 seconds.\n",
      "-- Epoch 919\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921143, T: 3700562113, Avg. loss: 0.548208\n",
      "Total training time: 7872.07 seconds.\n",
      "-- Epoch 920\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921254, T: 3704588840, Avg. loss: 0.548210\n",
      "Total training time: 7880.84 seconds.\n",
      "-- Epoch 921\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921169, T: 3708615567, Avg. loss: 0.548207\n",
      "Total training time: 7889.42 seconds.\n",
      "-- Epoch 922\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921222, T: 3712642294, Avg. loss: 0.548208\n",
      "Total training time: 7897.91 seconds.\n",
      "-- Epoch 923\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921122, T: 3716669021, Avg. loss: 0.548207\n",
      "Total training time: 7906.43 seconds.\n",
      "-- Epoch 924\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920840, T: 3720695748, Avg. loss: 0.548207\n",
      "Total training time: 7915.02 seconds.\n",
      "-- Epoch 925\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921143, T: 3724722475, Avg. loss: 0.548206\n",
      "Total training time: 7923.77 seconds.\n",
      "-- Epoch 926\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920930, T: 3728749202, Avg. loss: 0.548205\n",
      "Total training time: 7932.28 seconds.\n",
      "-- Epoch 927\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920912, T: 3732775929, Avg. loss: 0.548208\n",
      "Total training time: 7940.79 seconds.\n",
      "-- Epoch 928\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921077, T: 3736802656, Avg. loss: 0.548209\n",
      "Total training time: 7949.42 seconds.\n",
      "-- Epoch 929\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921025, T: 3740829383, Avg. loss: 0.548210\n",
      "Total training time: 7958.01 seconds.\n",
      "-- Epoch 930\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921082, T: 3744856110, Avg. loss: 0.548208\n",
      "Total training time: 7966.70 seconds.\n",
      "-- Epoch 931\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920981, T: 3748882837, Avg. loss: 0.548206\n",
      "Total training time: 7975.33 seconds.\n",
      "-- Epoch 932\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921097, T: 3752909564, Avg. loss: 0.548208\n",
      "Total training time: 7983.92 seconds.\n",
      "-- Epoch 933\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921013, T: 3756936291, Avg. loss: 0.548208\n",
      "Total training time: 7992.56 seconds.\n",
      "-- Epoch 934\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921143, T: 3760963018, Avg. loss: 0.548208\n",
      "Total training time: 8001.49 seconds.\n",
      "-- Epoch 935\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921200, T: 3764989745, Avg. loss: 0.548210\n",
      "Total training time: 8010.23 seconds.\n",
      "-- Epoch 936\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920978, T: 3769016472, Avg. loss: 0.548208\n",
      "Total training time: 8018.88 seconds.\n",
      "-- Epoch 937\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921214, T: 3773043199, Avg. loss: 0.548208\n",
      "Total training time: 8027.48 seconds.\n",
      "-- Epoch 938\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920992, T: 3777069926, Avg. loss: 0.548209\n",
      "Total training time: 8036.05 seconds.\n",
      "-- Epoch 939\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921262, T: 3781096653, Avg. loss: 0.548206\n",
      "Total training time: 8044.62 seconds.\n",
      "-- Epoch 940\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921163, T: 3785123380, Avg. loss: 0.548207\n",
      "Total training time: 8053.23 seconds.\n",
      "-- Epoch 941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.59, NNZs: 477, Bias: -1.921293, T: 3789150107, Avg. loss: 0.548204\n",
      "Total training time: 8061.79 seconds.\n",
      "-- Epoch 942\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920929, T: 3793176834, Avg. loss: 0.548208\n",
      "Total training time: 8070.33 seconds.\n",
      "-- Epoch 943\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921249, T: 3797203561, Avg. loss: 0.548209\n",
      "Total training time: 8078.81 seconds.\n",
      "-- Epoch 944\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920967, T: 3801230288, Avg. loss: 0.548206\n",
      "Total training time: 8087.26 seconds.\n",
      "-- Epoch 945\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920900, T: 3805257015, Avg. loss: 0.548205\n",
      "Total training time: 8095.73 seconds.\n",
      "-- Epoch 946\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920938, T: 3809283742, Avg. loss: 0.548209\n",
      "Total training time: 8104.20 seconds.\n",
      "-- Epoch 947\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920983, T: 3813310469, Avg. loss: 0.548208\n",
      "Total training time: 8112.89 seconds.\n",
      "-- Epoch 948\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920875, T: 3817337196, Avg. loss: 0.548208\n",
      "Total training time: 8121.50 seconds.\n",
      "-- Epoch 949\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921102, T: 3821363923, Avg. loss: 0.548207\n",
      "Total training time: 8130.01 seconds.\n",
      "-- Epoch 950\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920955, T: 3825390650, Avg. loss: 0.548206\n",
      "Total training time: 8138.51 seconds.\n",
      "-- Epoch 951\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920969, T: 3829417377, Avg. loss: 0.548207\n",
      "Total training time: 8147.02 seconds.\n",
      "-- Epoch 952\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920858, T: 3833444104, Avg. loss: 0.548207\n",
      "Total training time: 8155.51 seconds.\n",
      "-- Epoch 953\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920863, T: 3837470831, Avg. loss: 0.548205\n",
      "Total training time: 8164.00 seconds.\n",
      "-- Epoch 954\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920932, T: 3841497558, Avg. loss: 0.548206\n",
      "Total training time: 8172.47 seconds.\n",
      "-- Epoch 955\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920973, T: 3845524285, Avg. loss: 0.548207\n",
      "Total training time: 8180.94 seconds.\n",
      "-- Epoch 956\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920960, T: 3849551012, Avg. loss: 0.548206\n",
      "Total training time: 8189.40 seconds.\n",
      "-- Epoch 957\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921014, T: 3853577739, Avg. loss: 0.548208\n",
      "Total training time: 8197.95 seconds.\n",
      "-- Epoch 958\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.921132, T: 3857604466, Avg. loss: 0.548208\n",
      "Total training time: 8206.42 seconds.\n",
      "-- Epoch 959\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920799, T: 3861631193, Avg. loss: 0.548204\n",
      "Total training time: 8214.91 seconds.\n",
      "-- Epoch 960\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920923, T: 3865657920, Avg. loss: 0.548205\n",
      "Total training time: 8223.44 seconds.\n",
      "-- Epoch 961\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920710, T: 3869684647, Avg. loss: 0.548204\n",
      "Total training time: 8232.03 seconds.\n",
      "-- Epoch 962\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920853, T: 3873711374, Avg. loss: 0.548206\n",
      "Total training time: 8240.63 seconds.\n",
      "-- Epoch 963\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920920, T: 3877738101, Avg. loss: 0.548204\n",
      "Total training time: 8249.35 seconds.\n",
      "-- Epoch 964\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920803, T: 3881764828, Avg. loss: 0.548207\n",
      "Total training time: 8257.88 seconds.\n",
      "-- Epoch 965\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920835, T: 3885791555, Avg. loss: 0.548207\n",
      "Total training time: 8266.41 seconds.\n",
      "-- Epoch 966\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920646, T: 3889818282, Avg. loss: 0.548207\n",
      "Total training time: 8274.89 seconds.\n",
      "-- Epoch 967\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920869, T: 3893845009, Avg. loss: 0.548207\n",
      "Total training time: 8283.43 seconds.\n",
      "-- Epoch 968\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920668, T: 3897871736, Avg. loss: 0.548208\n",
      "Total training time: 8291.97 seconds.\n",
      "-- Epoch 969\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920738, T: 3901898463, Avg. loss: 0.548205\n",
      "Total training time: 8300.47 seconds.\n",
      "-- Epoch 970\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920835, T: 3905925190, Avg. loss: 0.548206\n",
      "Total training time: 8309.03 seconds.\n",
      "-- Epoch 971\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920693, T: 3909951917, Avg. loss: 0.548206\n",
      "Total training time: 8317.55 seconds.\n",
      "-- Epoch 972\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920886, T: 3913978644, Avg. loss: 0.548204\n",
      "Total training time: 8326.08 seconds.\n",
      "-- Epoch 973\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920773, T: 3918005371, Avg. loss: 0.548207\n",
      "Total training time: 8334.70 seconds.\n",
      "-- Epoch 974\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920736, T: 3922032098, Avg. loss: 0.548206\n",
      "Total training time: 8343.38 seconds.\n",
      "-- Epoch 975\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920655, T: 3926058825, Avg. loss: 0.548205\n",
      "Total training time: 8352.05 seconds.\n",
      "-- Epoch 976\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920688, T: 3930085552, Avg. loss: 0.548206\n",
      "Total training time: 8360.60 seconds.\n",
      "-- Epoch 977\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920605, T: 3934112279, Avg. loss: 0.548204\n",
      "Total training time: 8369.09 seconds.\n",
      "-- Epoch 978\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920715, T: 3938139006, Avg. loss: 0.548207\n",
      "Total training time: 8377.68 seconds.\n",
      "-- Epoch 979\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920697, T: 3942165733, Avg. loss: 0.548206\n",
      "Total training time: 8386.21 seconds.\n",
      "-- Epoch 980\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920681, T: 3946192460, Avg. loss: 0.548205\n",
      "Total training time: 8394.70 seconds.\n",
      "-- Epoch 981\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920677, T: 3950219187, Avg. loss: 0.548206\n",
      "Total training time: 8403.19 seconds.\n",
      "-- Epoch 982\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920684, T: 3954245914, Avg. loss: 0.548206\n",
      "Total training time: 8411.72 seconds.\n",
      "-- Epoch 983\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920557, T: 3958272641, Avg. loss: 0.548205\n",
      "Total training time: 8420.20 seconds.\n",
      "-- Epoch 984\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920635, T: 3962299368, Avg. loss: 0.548207\n",
      "Total training time: 8428.68 seconds.\n",
      "-- Epoch 985\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920555, T: 3966326095, Avg. loss: 0.548205\n",
      "Total training time: 8437.17 seconds.\n",
      "-- Epoch 986\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920576, T: 3970352822, Avg. loss: 0.548204\n",
      "Total training time: 8445.66 seconds.\n",
      "-- Epoch 987\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920706, T: 3974379549, Avg. loss: 0.548207\n",
      "Total training time: 8454.16 seconds.\n",
      "-- Epoch 988\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920782, T: 3978406276, Avg. loss: 0.548206\n",
      "Total training time: 8462.60 seconds.\n",
      "-- Epoch 989\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920665, T: 3982433003, Avg. loss: 0.548208\n",
      "Total training time: 8471.10 seconds.\n",
      "-- Epoch 990\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920606, T: 3986459730, Avg. loss: 0.548208\n",
      "Total training time: 8479.60 seconds.\n",
      "-- Epoch 991\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920472, T: 3990486457, Avg. loss: 0.548207\n",
      "Total training time: 8488.10 seconds.\n",
      "-- Epoch 992\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920606, T: 3994513184, Avg. loss: 0.548206\n",
      "Total training time: 8496.63 seconds.\n",
      "-- Epoch 993\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920654, T: 3998539911, Avg. loss: 0.548205\n",
      "Total training time: 8505.09 seconds.\n",
      "-- Epoch 994\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920579, T: 4002566638, Avg. loss: 0.548204\n",
      "Total training time: 8513.56 seconds.\n",
      "-- Epoch 995\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920615, T: 4006593365, Avg. loss: 0.548204\n",
      "Total training time: 8522.05 seconds.\n",
      "-- Epoch 996\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920593, T: 4010620092, Avg. loss: 0.548206\n",
      "Total training time: 8530.53 seconds.\n",
      "-- Epoch 997\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920586, T: 4014646819, Avg. loss: 0.548209\n",
      "Total training time: 8539.21 seconds.\n",
      "-- Epoch 998\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920619, T: 4018673546, Avg. loss: 0.548205\n",
      "Total training time: 8547.68 seconds.\n",
      "-- Epoch 999\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920457, T: 4022700273, Avg. loss: 0.548204\n",
      "Total training time: 8556.12 seconds.\n",
      "-- Epoch 1000\n",
      "Norm: 4.59, NNZs: 477, Bias: -1.920542, T: 4026727000, Avg. loss: 0.548206\n",
      "Total training time: 8564.64 seconds.\n",
      "0.5479825664266159\n",
      "8628.801725149155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "t1=time.time()\n",
    "sgd = SGDClassifier(loss=\"log\", penalty=\"l2\",max_iter=1000,verbose=True)\n",
    "sgd.fit(X_train,y_train)\n",
    "pred = sgd.predict_proba(X_valid)\n",
    "print(log_loss(y_valid,pred[:,1]))\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5479825664266159"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = sgd.predict_proba(X_valid)\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find predicted value on test set\n",
    "pred = sgd.predict_proba(test)\n",
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test_ids\n",
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=pred[:,1]\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')\n",
    "submit.to_csv('SGD_1000iter_impute_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   34.8s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5660623015116591\n",
      "62.94131875038147\n"
     ]
    }
   ],
   "source": [
    "#trying Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "t1=time.time()\n",
    "rfc = RandomForestClassifier(max_depth=4, random_state=0,verbose=True)\n",
    "rfc.fit(X_train, y_train)\n",
    "pred = rfc.predict_proba(X_valid)\n",
    "print(log_loss(y_valid,pred[:,1]))\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "pred = rfc.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test_ids\n",
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=pred[:,1]\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')\n",
    "submit.to_csv('RFC_depth4_impute-full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_col_name=[]\n",
    "for col in X_train.columns:\n",
    "    col = col.replace('<','X')\n",
    "    col = col.replace('>','Y')\n",
    "    col = col.replace('[','A')\n",
    "    col = col.replace(']','B')\n",
    "    clean_col_name.append(col)\n",
    "X_train.columns = clean_col_name\n",
    "clean_col_name=[]\n",
    "for col in X_valid.columns:\n",
    "    col = col.replace('<','X')\n",
    "    col = col.replace('>','Y')\n",
    "    col = col.replace('[','A')\n",
    "    col = col.replace(']','B')\n",
    "    clean_col_name.append(col)\n",
    "X_valid.columns = clean_col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x7f7011857d30>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/site-packages/xgboost-0.7-py3.6.egg/xgboost/core.py\", line 366, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n",
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x7f70243239e8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/site-packages/xgboost-0.7-py3.6.egg/xgboost/core.py\", line 366, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "619"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time for xgboost\n",
    "import xgboost as xgb\n",
    "import time\n",
    "t1 = time.time()\n",
    "model1 = xgb.XGBClassifier()\n",
    "#model2 = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5)\n",
    "xgb1 = model1.fit(X_train, y_train)\n",
    "#xgb2 = model2.fit(X_train.iloc[0:10,], y_train[0:10])\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4776.111815214157"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=xgb1.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5286575244360493"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=test.columns\n",
    "clean_col_name=[]\n",
    "for col in test.columns:\n",
    "    col = col.replace('<','X')\n",
    "    col = col.replace('>','Y')\n",
    "    col = col.replace('[','A')\n",
    "    col = col.replace(']','B')\n",
    "    clean_col_name.append(col)\n",
    "test.columns = clean_col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test_ids\n",
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=xgb1.predict_proba(test)[:,1]\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')\n",
    "submit.to_csv('XGB-entiretrainscl-impute-full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x7f7011857cc0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/site-packages/xgboost-0.7-py3.6.egg/xgboost/core.py\", line 366, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "788"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying MLPerceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20),verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-952cf16db5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \"\"\"\n\u001b[1;32m    972\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0;32m--> 973\u001b[0;31m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    364\u001b[0m         activations.extend(np.empty((batch_size, n_fan_out))\n\u001b[1;32m    365\u001b[0m                            for n_fan_out in layer_units[1:])\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_layer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    364\u001b[0m         activations.extend(np.empty((batch_size, n_fan_out))\n\u001b[1;32m    365\u001b[0m                            for n_fan_out in layer_units[1:])\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_layer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "mlp.fit(X_train,y_train)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32762739704396604"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "pred = mlp.predict_proba(X_valid)\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test['innovation_challenge_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=mlp.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1888195"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=pred[:,1]\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')\n",
    "submit.to_csv('MLP_20_20_20_scl-impute.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
