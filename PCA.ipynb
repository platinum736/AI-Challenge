{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'migrated',\n",
       " 'Base models-entire data.ipynb',\n",
       " 'Basic models.ipynb',\n",
       " 'EDA_Challenge.ipynb',\n",
       " 'EDA.ipynb',\n",
       " 'EDA_trainData.ipynb',\n",
       " 'Ensemble models.ipynb',\n",
       " 'xgboost with null.ipynb',\n",
       " 'Untitled.ipynb',\n",
       " 'Base models-clean.ipynb',\n",
       " 'nohup.out',\n",
       " 'jupyter_notebook_config.py',\n",
       " 'LR_100iter_impute_full.csv',\n",
       " 'SGD_1000iter_impute_full.csv',\n",
       " 'RFC_depth4_impute-full.csv',\n",
       " 'XGB-entiretrainscl-impute-full.csv',\n",
       " 'data_clean.ipynb',\n",
       " 'PCA.ipynb',\n",
       " 'RFC_depth4_impute-full_pca.csv',\n",
       " 'XGB-entiretrainscl-impute-pca.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (6,16,17,26,40,41,49,53,97,98,99,100,104,107,109,112,127,128,129,130,220,221,222) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (6,16,17,26,40,41,49,53,54,55,100,104,107,109,112,127,128,129,130,153,171,220,221,222,225,226,227) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.88337469100952\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1=time.time()\n",
    "train = pd.read_table('/aichallenge/challengeData.tsv')\n",
    "test = pd.read_table('/aichallenge/scoring_set.tsv')\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['challenge_data.innovation_challenge_key',\n",
       "       'challenge_data.renewed_yorn', 'challenge_data.instance_id',\n",
       "       'challenge_data.contract_line_id', 'challenge_data.minor_line_yorn',\n",
       "       'challenge_data.major_line_instance_id',\n",
       "       'challenge_data.installation_date',\n",
       "       'challenge_data.product_sales_order_type',\n",
       "       'challenge_data.product_purchase_order_type',\n",
       "       'challenge_data.instance_status',\n",
       "       ...\n",
       "       'challenge_data.service_distributor_base_theater_name',\n",
       "       'challenge_data.service_distributor_base_distributor_normalized_name',\n",
       "       'challenge_data.contract_line_net_usd_amount',\n",
       "       'challenge_data.product_net_price',\n",
       "       'challenge_data.sales_node_renewal_rate',\n",
       "       'challenge_data.customer_renewal_rate',\n",
       "       'challenge_data.partner_renewal_rate',\n",
       "       'challenge_data.product_renewal_rate',\n",
       "       'challenge_data.service_sales_node_installed_base_sales_node_renewal_rate',\n",
       "       'challenge_data.service_partner_installed_base_partner_renewal_rate'],\n",
       "      dtype='object', length=237)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014498233795166016\n"
     ]
    }
   ],
   "source": [
    "#remove data set name from columns names to make it uniform\n",
    "t1=time.time()\n",
    "columns=[]\n",
    "for col in train.columns:\n",
    "    columns.append(col.replace('challenge_data.',''))\n",
    "train.columns=columns\n",
    "columns=[]\n",
    "for col in test.columns:\n",
    "    columns.append(col.replace('scoring_set.',''))\n",
    "test.columns=columns\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'renewed_yorn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'renewed_yorn'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0e4cdceae944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#remove the rows where 'challenge_data.renewed_yorn' is null in training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'renewed_yorn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'renewed_yorn'"
     ]
    }
   ],
   "source": [
    "#remove the rows where 'challenge_data.renewed_yorn' is null in training data\n",
    "t1=time.time()\n",
    "train=train[train['renewed_yorn'].notnull()]\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.139721155166626\n"
     ]
    }
   ],
   "source": [
    "#remove the 'challenge_data.renewed_yorn' from training and test data and move it to a seperate variable\n",
    "t1=time.time()\n",
    "train_y=train['renewed_yorn']\n",
    "train = train.drop('renewed_yorn',axis=1)\n",
    "test_y=test['renewed_yorn']\n",
    "test = test.drop('renewed_yorn',axis=1)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.64650297164917\n"
     ]
    }
   ],
   "source": [
    "#remove columns which have more than 0.5 null values\n",
    "t1=time.time()\n",
    "percent = (train.isnull().sum()/train.isnull().count())\n",
    "train.columns[percent>0.5]\n",
    "#We would ignore all these 102 columns so we are left with 135 columns\n",
    "len(train.columns[percent>0.5])\n",
    "#all_clean_columns=train.columns[percent==0]\n",
    "#train_clean=train[all_clean_columns]\n",
    "unclean_columns=test.columns[percent>0.5]\n",
    "train = train.drop(unclean_columns,axis=1)\n",
    "test = test.drop(unclean_columns,axis=1)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0A1639173E7ABD9BDD35A5EC980B932E01C293939210F0737535E10CC29E0284',\n",
       "       '0486A25069AB01F95CF4B85CCC64408767C6A53DF7B2510355DC82C3A106B7ED',\n",
       "       '07B384DCEB1B7692CAFD38806AA4542A6973ACD893755C7BF4A6067810B1C0D6',\n",
       "       ...,\n",
       "       '019F3BE24B57ED3484ACE4528C5D7AC184563CF65C4E9B9EFD869CE2CA52C752',\n",
       "       '0363A480FE0DD214C76D6E3479571E5B6D2FF1861C85466F336A47B629AD4F11',\n",
       "       '020167519BA6B73171807389FF543BD70E5C311691DC8101DF147A20DA401725'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['major_line_instance_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.49243140220642\n"
     ]
    }
   ],
   "source": [
    "#Remove columns which have too many categories and is string type\n",
    "t1=time.time()\n",
    "messy_columns = []\n",
    "for col in test.columns:\n",
    "    if(test[col].dtype=='object' and len(test[col].unique())>100):\n",
    "        messy_columns.append(col)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 78 messy columns are removed\n",
    "len(messy_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.891085147857666\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "train=train.drop(messy_columns,axis=1)\n",
    "test=test.drop(messy_columns,axis=1)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226140, 120)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.114413261413574\n"
     ]
    }
   ],
   "source": [
    "#Remove the primary key column for data fitting\n",
    "t1=time.time()\n",
    "train_ids=train['innovation_challenge_key']\n",
    "test_ids=test['innovation_challenge_key']\n",
    "train = train.drop('innovation_challenge_key',axis=1)\n",
    "test = test.drop('innovation_challenge_key',axis=1)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023067474365234375\n"
     ]
    }
   ],
   "source": [
    "#Number of unique values in each columns\n",
    "t1=time.time()\n",
    "catcols=[]\n",
    "scalcols=[]\n",
    "for col in train.columns:\n",
    "    if (train[col].dtype !='object'):\n",
    "        if(len(train[col].unique())<20):\n",
    "            catcols.append(col)\n",
    "        else:\n",
    "            scalcols.append(col)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1856088638305664\n"
     ]
    }
   ],
   "source": [
    "#handle catcols\n",
    "t1=time.time()\n",
    "for col in catcols:\n",
    "    train[col]=train[col].astype('category')\n",
    "    test[col]=test[col].astype('category')\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace integer nans with mean \n",
    "train[scalcols]=train[scalcols].fillna(train[scalcols].mean())\n",
    "test[scalcols]=test[scalcols].fillna(test[scalcols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contract_line_net_usd_amount    False\n",
       "product_net_price               False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[scalcols].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contract_line_net_usd_amount    False\n",
       "product_net_price               False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[scalcols].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6246292591094971\n"
     ]
    }
   ],
   "source": [
    "#Normalize integer data\n",
    "t1=time.time()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[scalcols])\n",
    "train[scalcols] = scaler.transform(train[scalcols])\n",
    "scaler = scaler.fit(test[scalcols])\n",
    "test[scalcols] = scaler.transform(test[scalcols])\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract_line_net_usd_amount</th>\n",
       "      <th>product_net_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.261400e+05</td>\n",
       "      <td>2.261400e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.005455e-17</td>\n",
       "      <td>1.206546e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000002e+00</td>\n",
       "      <td>1.000002e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.252316e-01</td>\n",
       "      <td>-2.321716e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.252316e-01</td>\n",
       "      <td>-2.321716e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.252316e-01</td>\n",
       "      <td>-2.321716e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-1.252316e-01</td>\n",
       "      <td>-1.584198e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.774224e+01</td>\n",
       "      <td>6.648806e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       contract_line_net_usd_amount  product_net_price\n",
       "count                  2.261400e+05       2.261400e+05\n",
       "mean                   1.005455e-17       1.206546e-17\n",
       "std                    1.000002e+00       1.000002e+00\n",
       "min                   -1.252316e-01      -2.321716e-01\n",
       "25%                   -1.252316e-01      -2.321716e-01\n",
       "50%                   -1.252316e-01      -2.321716e-01\n",
       "75%                   -1.252316e-01      -1.584198e-01\n",
       "max                    7.774224e+01       6.648806e+01"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[scalcols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_transaction_type</th>\n",
       "      <th>contract_line_reaction_time_code</th>\n",
       "      <th>sales_hierarchy_level</th>\n",
       "      <th>service_sales_node_base_sales_hierarchy_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>226140</td>\n",
       "      <td>226140</td>\n",
       "      <td>226140</td>\n",
       "      <td>226140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>10002</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>202522</td>\n",
       "      <td>225296</td>\n",
       "      <td>199960</td>\n",
       "      <td>224170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_transaction_type  contract_line_reaction_time_code  \\\n",
       "count                     226140                            226140   \n",
       "unique                         3                                 3   \n",
       "top                        10002                                -1   \n",
       "freq                      202522                            225296   \n",
       "\n",
       "        sales_hierarchy_level  service_sales_node_base_sales_hierarchy_level  \n",
       "count                  226140                                         226140  \n",
       "unique                      6                                              6  \n",
       "top                         6                                              6  \n",
       "freq                   199960                                         224170  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[catcols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclean_cols=train.columns[train.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all other 43 cols nan with mode values\n",
    "train[unclean_cols]=train[unclean_cols].fillna(train[unclean_cols].median())\n",
    "test[unclean_cols]=test[unclean_cols].fillna(test[unclean_cols].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.214226484298706\n"
     ]
    }
   ],
   "source": [
    "#convert data to one hot encoding to handle categorical values\n",
    "t1=time.time()\n",
    "train_objs_num = len(train)\n",
    "dataset = pd.concat(objs=[train, test], axis=0)\n",
    "dataset = pd.get_dummies(dataset)\n",
    "train = dataset[:train_objs_num]\n",
    "test = dataset[train_objs_num:]\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['contract_line_reaction_time_code',\n",
       "       'service_sales_node_base_sales_hierarchy_level',\n",
       "       'contract_line_net_usd_amount', 'product_net_price',\n",
       "       'minor_line_yorn_N', 'minor_line_yorn_Y',\n",
       "       'instance_status_Latest-INSTALLED', 'product_transaction_type_-9999',\n",
       "       'product_transaction_type_10000', 'product_transaction_type_10002',\n",
       "       ...\n",
       "       'service_distributor_base_distributor_name_63E656F07CCA14C7E92A9CF75C45C2F0E5BDFD3A07A1EF9EC3E92F5AF051FA22',\n",
       "       'service_distributor_base_distributor_name_7F3282462F1B81C79FE6937D9B49AF13384D961DDF42DF6CF1BB59D7630308A2',\n",
       "       'service_distributor_base_distributor_name_815A7B66FE919A14C089996194F98F696B39D83332E31755073B836FEA52C364',\n",
       "       'service_distributor_base_distributor_name_84DCB9C6A3A305DBE0088EDCF4C4BC00C13208407ADA44DFBECEA93D98E8D4B3',\n",
       "       'service_distributor_base_distributor_name_861E3825094B361623EE986A723F9B544B21EC4718F3667780CB8528C4E6767C',\n",
       "       'service_distributor_base_distributor_name_98F1585B970DBC564E4063B0DFEFF2BB7A1A6F48B54D457B46379F06A8D2F740',\n",
       "       'service_distributor_base_distributor_name_D496D3E841D7F257C8B8314A580E5728BC034FD09FD4360908B2103860CC728F',\n",
       "       'service_distributor_base_distributor_name_E15FFA65FCD7A17CF3F1FE025C496E1B5C5D6A5126918C5FFEFFC411240000F5',\n",
       "       'service_distributor_base_distributor_name_F833682236B2695F4EF3508100C9FFE86918E501D562BC55B221D346D30E99D9',\n",
       "       'service_distributor_base_distributor_normalized_name_A096DD969881554D12B5297CD9DF3CB0C6632E882AA28126C8FC410923A74EDF'],\n",
       "      dtype='object', length=444)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226140, 444)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the new columns we got\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1888195, 444)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t1=time.time()\\ntrain_X = train_X.fillna(0)\\ntest_X = test_X.fillna(0)\\nt2=time.time()\\nprint(t2-t1)'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace all NaN left with 0 No nans left\n",
    "'''t1=time.time()\n",
    "train_X = train_X.fillna(0)\n",
    "test_X = test_X.fillna(0)\n",
    "t2=time.time()\n",
    "print(t2-t1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data quality now\n",
    "train.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data quality now\n",
    "test.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1341"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5088908672332764\n"
     ]
    }
   ],
   "source": [
    "#split the data between train and validation set\n",
    "t1=time.time()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train,train_y,test_size=0.33, random_state=42)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151513, 444)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensions of training data\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time to introduce PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract_line_reaction_time_code</th>\n",
       "      <th>service_sales_node_base_sales_hierarchy_level</th>\n",
       "      <th>contract_line_net_usd_amount</th>\n",
       "      <th>product_net_price</th>\n",
       "      <th>minor_line_yorn_N</th>\n",
       "      <th>minor_line_yorn_Y</th>\n",
       "      <th>instance_status_Latest-INSTALLED</th>\n",
       "      <th>product_transaction_type_-9999</th>\n",
       "      <th>product_transaction_type_10000</th>\n",
       "      <th>product_transaction_type_10002</th>\n",
       "      <th>...</th>\n",
       "      <th>service_distributor_base_distributor_name_63E656F07CCA14C7E92A9CF75C45C2F0E5BDFD3A07A1EF9EC3E92F5AF051FA22</th>\n",
       "      <th>service_distributor_base_distributor_name_7F3282462F1B81C79FE6937D9B49AF13384D961DDF42DF6CF1BB59D7630308A2</th>\n",
       "      <th>service_distributor_base_distributor_name_815A7B66FE919A14C089996194F98F696B39D83332E31755073B836FEA52C364</th>\n",
       "      <th>service_distributor_base_distributor_name_84DCB9C6A3A305DBE0088EDCF4C4BC00C13208407ADA44DFBECEA93D98E8D4B3</th>\n",
       "      <th>service_distributor_base_distributor_name_861E3825094B361623EE986A723F9B544B21EC4718F3667780CB8528C4E6767C</th>\n",
       "      <th>service_distributor_base_distributor_name_98F1585B970DBC564E4063B0DFEFF2BB7A1A6F48B54D457B46379F06A8D2F740</th>\n",
       "      <th>service_distributor_base_distributor_name_D496D3E841D7F257C8B8314A580E5728BC034FD09FD4360908B2103860CC728F</th>\n",
       "      <th>service_distributor_base_distributor_name_E15FFA65FCD7A17CF3F1FE025C496E1B5C5D6A5126918C5FFEFFC411240000F5</th>\n",
       "      <th>service_distributor_base_distributor_name_F833682236B2695F4EF3508100C9FFE86918E501D562BC55B221D346D30E99D9</th>\n",
       "      <th>service_distributor_base_distributor_normalized_name_A096DD969881554D12B5297CD9DF3CB0C6632E882AA28126C8FC410923A74EDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227682</th>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.125232</td>\n",
       "      <td>-0.232172</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120974</th>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.125232</td>\n",
       "      <td>-0.232172</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48142</th>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.125232</td>\n",
       "      <td>-0.232172</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36978</th>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.125232</td>\n",
       "      <td>-0.232172</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124715</th>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.125232</td>\n",
       "      <td>-0.232172</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 444 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        contract_line_reaction_time_code  \\\n",
       "227682                                -1   \n",
       "120974                                -1   \n",
       "48142                                 -1   \n",
       "36978                                 -1   \n",
       "124715                                -1   \n",
       "\n",
       "        service_sales_node_base_sales_hierarchy_level  \\\n",
       "227682                                              6   \n",
       "120974                                              6   \n",
       "48142                                               6   \n",
       "36978                                               6   \n",
       "124715                                              6   \n",
       "\n",
       "        contract_line_net_usd_amount  product_net_price  minor_line_yorn_N  \\\n",
       "227682                     -0.125232          -0.232172                  0   \n",
       "120974                     -0.125232          -0.232172                  0   \n",
       "48142                      -0.125232          -0.232172                  0   \n",
       "36978                      -0.125232          -0.232172                  0   \n",
       "124715                     -0.125232          -0.232172                  0   \n",
       "\n",
       "        minor_line_yorn_Y  instance_status_Latest-INSTALLED  \\\n",
       "227682                  1                                 1   \n",
       "120974                  1                                 1   \n",
       "48142                   1                                 1   \n",
       "36978                   1                                 1   \n",
       "124715                  1                                 1   \n",
       "\n",
       "        product_transaction_type_-9999  product_transaction_type_10000  \\\n",
       "227682                               0                               0   \n",
       "120974                               0                               0   \n",
       "48142                                0                               0   \n",
       "36978                                0                               0   \n",
       "124715                               0                               0   \n",
       "\n",
       "        product_transaction_type_10002  \\\n",
       "227682                               1   \n",
       "120974                               1   \n",
       "48142                                1   \n",
       "36978                                1   \n",
       "124715                               1   \n",
       "\n",
       "                                                                ...                                                            \\\n",
       "227682                                                          ...                                                             \n",
       "120974                                                          ...                                                             \n",
       "48142                                                           ...                                                             \n",
       "36978                                                           ...                                                             \n",
       "124715                                                          ...                                                             \n",
       "\n",
       "        service_distributor_base_distributor_name_63E656F07CCA14C7E92A9CF75C45C2F0E5BDFD3A07A1EF9EC3E92F5AF051FA22  \\\n",
       "227682                                                  0                                                            \n",
       "120974                                                  0                                                            \n",
       "48142                                                   0                                                            \n",
       "36978                                                   0                                                            \n",
       "124715                                                  0                                                            \n",
       "\n",
       "        service_distributor_base_distributor_name_7F3282462F1B81C79FE6937D9B49AF13384D961DDF42DF6CF1BB59D7630308A2  \\\n",
       "227682                                                  0                                                            \n",
       "120974                                                  0                                                            \n",
       "48142                                                   0                                                            \n",
       "36978                                                   0                                                            \n",
       "124715                                                  0                                                            \n",
       "\n",
       "        service_distributor_base_distributor_name_815A7B66FE919A14C089996194F98F696B39D83332E31755073B836FEA52C364  \\\n",
       "227682                                                  0                                                            \n",
       "120974                                                  0                                                            \n",
       "48142                                                   0                                                            \n",
       "36978                                                   0                                                            \n",
       "124715                                                  0                                                            \n",
       "\n",
       "        service_distributor_base_distributor_name_84DCB9C6A3A305DBE0088EDCF4C4BC00C13208407ADA44DFBECEA93D98E8D4B3  \\\n",
       "227682                                                  0                                                            \n",
       "120974                                                  0                                                            \n",
       "48142                                                   0                                                            \n",
       "36978                                                   0                                                            \n",
       "124715                                                  0                                                            \n",
       "\n",
       "        service_distributor_base_distributor_name_861E3825094B361623EE986A723F9B544B21EC4718F3667780CB8528C4E6767C  \\\n",
       "227682                                                  0                                                            \n",
       "120974                                                  0                                                            \n",
       "48142                                                   0                                                            \n",
       "36978                                                   0                                                            \n",
       "124715                                                  0                                                            \n",
       "\n",
       "        service_distributor_base_distributor_name_98F1585B970DBC564E4063B0DFEFF2BB7A1A6F48B54D457B46379F06A8D2F740  \\\n",
       "227682                                                  0                                                            \n",
       "120974                                                  0                                                            \n",
       "48142                                                   0                                                            \n",
       "36978                                                   0                                                            \n",
       "124715                                                  0                                                            \n",
       "\n",
       "        service_distributor_base_distributor_name_D496D3E841D7F257C8B8314A580E5728BC034FD09FD4360908B2103860CC728F  \\\n",
       "227682                                                  0                                                            \n",
       "120974                                                  0                                                            \n",
       "48142                                                   0                                                            \n",
       "36978                                                   0                                                            \n",
       "124715                                                  0                                                            \n",
       "\n",
       "        service_distributor_base_distributor_name_E15FFA65FCD7A17CF3F1FE025C496E1B5C5D6A5126918C5FFEFFC411240000F5  \\\n",
       "227682                                                  0                                                            \n",
       "120974                                                  1                                                            \n",
       "48142                                                   1                                                            \n",
       "36978                                                   1                                                            \n",
       "124715                                                  1                                                            \n",
       "\n",
       "        service_distributor_base_distributor_name_F833682236B2695F4EF3508100C9FFE86918E501D562BC55B221D346D30E99D9  \\\n",
       "227682                                                  0                                                            \n",
       "120974                                                  0                                                            \n",
       "48142                                                   0                                                            \n",
       "36978                                                   0                                                            \n",
       "124715                                                  0                                                            \n",
       "\n",
       "        service_distributor_base_distributor_normalized_name_A096DD969881554D12B5297CD9DF3CB0C6632E882AA28126C8FC410923A74EDF  \n",
       "227682                                                  0                                                                      \n",
       "120974                                                  1                                                                      \n",
       "48142                                                   1                                                                      \n",
       "36978                                                   1                                                                      \n",
       "124715                                                  1                                                                      \n",
       "\n",
       "[5 rows x 444 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scale(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1434407234191895\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "pca.fit(X)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "var= pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.09  10.64  13.69  16.37  18.61  20.75  22.78  24.65  26.3   27.85\n",
      "  29.26  30.64  31.96  33.17  34.31  35.4   36.47  37.53  38.56  39.54\n",
      "  40.5   41.44  42.37  43.29  44.18  45.07  45.94  46.8   47.65  48.48\n",
      "  49.29  50.09  50.88  51.63  52.37  53.07  53.75  54.41  55.05  55.67\n",
      "  56.29  56.89  57.47  58.04  58.59  59.13  59.66  60.18  60.7   61.21\n",
      "  61.71  62.21  62.7   63.19  63.68  64.17  64.66  65.14  65.62  66.09\n",
      "  66.55  67.01  67.46  67.91  68.35  68.79  69.21  69.63  70.05  70.45\n",
      "  70.84  71.23  71.61  71.98  72.35  72.7   73.05  73.39  73.73  74.06\n",
      "  74.39  74.72  75.04  75.35  75.66  75.96  76.26  76.56  76.86  77.15\n",
      "  77.44  77.73  78.01  78.29  78.57  78.85  79.12  79.39  79.66  79.93\n",
      "  80.19  80.45  80.71  80.97  81.23  81.49  81.74  81.99  82.24  82.49\n",
      "  82.74  82.99  83.24  83.48  83.72  83.96  84.2   84.44  84.68  84.92\n",
      "  85.16  85.4   85.64  85.88  86.12  86.36  86.6   86.84  87.08  87.31\n",
      "  87.54  87.77  88.    88.23  88.46  88.69  88.92  89.14  89.36  89.58\n",
      "  89.8   90.02  90.24  90.45  90.66  90.87  91.08  91.28  91.48  91.68\n",
      "  91.88  92.07  92.26  92.45  92.64  92.82  93.    93.18  93.36  93.53\n",
      "  93.7   93.87  94.03  94.19  94.35  94.51  94.66  94.81  94.96  95.1\n",
      "  95.24  95.37  95.5   95.63  95.75  95.87  95.99  96.1   96.21  96.32\n",
      "  96.43  96.54  96.64  96.74  96.84  96.94  97.04  97.13  97.22  97.31\n",
      "  97.4   97.49  97.58  97.66  97.74  97.82  97.9   97.98  98.06  98.14\n",
      "  98.21  98.28  98.35  98.42  98.49  98.56  98.62  98.68  98.74  98.8\n",
      "  98.86  98.92  98.98  99.04  99.09  99.14  99.19  99.24  99.29  99.34\n",
      "  99.39  99.43  99.47  99.51  99.54  99.57  99.6   99.63  99.66  99.69\n",
      "  99.72  99.74  99.76  99.78  99.8   99.82  99.84  99.86  99.88  99.9\n",
      "  99.91  99.92  99.93  99.94  99.95  99.96  99.97  99.98  99.99 100.\n",
      " 100.01 100.02 100.03 100.04 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05 100.05\n",
      " 100.05 100.05 100.05 100.05]\n"
     ]
    }
   ],
   "source": [
    "print(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f00d6f9a5c0>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so we take 200 components\n",
    "pca = PCA(n_components=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=200, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid=scale(X_valid)\n",
    "test=scale(test)\n",
    "X_valid=pca.fit_transform(X_valid)\n",
    "test=pca.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7353649139404297\n"
     ]
    }
   ],
   "source": [
    "#Lets start fitting different models on the data\n",
    "#1. start with simple Bayes model\n",
    "t1=time.time()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gaussian_model = gnb.fit(X_train,y_train)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26989316940307617\n"
     ]
    }
   ],
   "source": [
    "# Lets see the prediction on our validation data\n",
    "t1=time.time()\n",
    "pred = gaussian_model.predict_proba(X_valid)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.99309707e-05, 9.99980069e-01])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.102895357758928"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find predicted value on test set\n",
    "pred = gaussian_model.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test['innovation_challenge_key']\n",
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/users/hdpsndbx125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very important to sort the values else won't be taken at submission\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#persist to a file\n",
    "submit.to_csv('NaiveBayesGaussian_EntireTraining.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1888195, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-82a794529cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/dslab/anaconda/python3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    602\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    603\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/dslab/anaconda/python3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "#fitting a multinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "clf.fit(X_train,y_train)\n",
    "pred = clf.predict_proba(X_valid)\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find predicted value on test set\n",
    "pred = clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-fa52ad9ccbbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/dslab/anaconda/python3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    602\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    603\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/dslab/anaconda/python3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "mnb = MultinomialNB(alpha=1.0)\n",
    "mnb.fit(X_train,y_train)\n",
    "pred = mnb.predict_proba(X_valid)\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/svm/base.py:898: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530.868843793869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "t1=time.time()\n",
    "lr1=lr(max_iter=100,verbose=1)\n",
    "lr1.fit(X_train,y_train)\n",
    "pred = lr1.predict_proba(X_valid)\n",
    "log_loss(y_valid,pred[:,1])\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9556585972832622"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find predicted value on test set\n",
    "pred = lr1.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (6,16,17,26,40,41,49,53,54,55,100,104,107,109,112,127,128,129,130,153,171,220,221,222,225,226,227) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_table('/aichallenge/scoring_set.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1888195, 237)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test['scoring_set.innovation_challenge_key']\n",
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=pred[:,1]\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')\n",
    "submit.to_csv('LR_100iter_impute_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1888195, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 58.23, NNZs: 200, Bias: 5.288217, T: 151513, Avg. loss: 22.643164\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 30.42, NNZs: 200, Bias: 3.625874, T: 303026, Avg. loss: 3.325676\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 19.04, NNZs: 200, Bias: 2.020440, T: 454539, Avg. loss: 2.017875\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 15.34, NNZs: 200, Bias: 1.894565, T: 606052, Avg. loss: 1.478932\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.73, NNZs: 200, Bias: 1.454906, T: 757565, Avg. loss: 1.209632\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 9.52, NNZs: 200, Bias: 1.374292, T: 909078, Avg. loss: 1.075235\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 8.25, NNZs: 200, Bias: 1.414824, T: 1060591, Avg. loss: 0.978810\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 7.17, NNZs: 200, Bias: 1.391815, T: 1212104, Avg. loss: 0.907324\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 6.25, NNZs: 200, Bias: 1.101544, T: 1363617, Avg. loss: 0.851004\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 5.73, NNZs: 200, Bias: 1.107140, T: 1515130, Avg. loss: 0.816767\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 4.91, NNZs: 200, Bias: 1.242024, T: 1666643, Avg. loss: 0.784790\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 4.62, NNZs: 200, Bias: 1.154305, T: 1818156, Avg. loss: 0.762305\n",
      "Total training time: 1.93 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 4.37, NNZs: 200, Bias: 1.131790, T: 1969669, Avg. loss: 0.738583\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 4.09, NNZs: 200, Bias: 1.195001, T: 2121182, Avg. loss: 0.719846\n",
      "Total training time: 2.25 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 3.76, NNZs: 200, Bias: 1.057712, T: 2272695, Avg. loss: 0.709327\n",
      "Total training time: 2.42 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 3.67, NNZs: 200, Bias: 1.060273, T: 2424208, Avg. loss: 0.699250\n",
      "Total training time: 2.58 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 3.19, NNZs: 200, Bias: 1.069170, T: 2575721, Avg. loss: 0.690208\n",
      "Total training time: 2.73 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 3.10, NNZs: 200, Bias: 1.034107, T: 2727234, Avg. loss: 0.676870\n",
      "Total training time: 2.89 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 3.05, NNZs: 200, Bias: 1.087545, T: 2878747, Avg. loss: 0.672529\n",
      "Total training time: 3.06 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.94, NNZs: 200, Bias: 1.015693, T: 3030260, Avg. loss: 0.660922\n",
      "Total training time: 3.22 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.83, NNZs: 200, Bias: 1.020369, T: 3181773, Avg. loss: 0.653502\n",
      "Total training time: 3.39 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.80, NNZs: 200, Bias: 1.079178, T: 3333286, Avg. loss: 0.652878\n",
      "Total training time: 3.55 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.51, NNZs: 200, Bias: 1.002479, T: 3484799, Avg. loss: 0.646294\n",
      "Total training time: 3.71 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.50, NNZs: 200, Bias: 1.087107, T: 3636312, Avg. loss: 0.641035\n",
      "Total training time: 3.87 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.49, NNZs: 200, Bias: 1.020694, T: 3787825, Avg. loss: 0.638981\n",
      "Total training time: 4.02 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.51, NNZs: 200, Bias: 0.970492, T: 3939338, Avg. loss: 0.633705\n",
      "Total training time: 4.19 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.34, NNZs: 200, Bias: 1.034038, T: 4090851, Avg. loss: 0.633583\n",
      "Total training time: 4.35 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.14, NNZs: 200, Bias: 0.998738, T: 4242364, Avg. loss: 0.630171\n",
      "Total training time: 4.52 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.17, NNZs: 200, Bias: 1.078740, T: 4393877, Avg. loss: 0.625310\n",
      "Total training time: 4.68 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.14, NNZs: 200, Bias: 1.063871, T: 4545390, Avg. loss: 0.620934\n",
      "Total training time: 4.85 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.14, NNZs: 200, Bias: 1.007138, T: 4696903, Avg. loss: 0.621199\n",
      "Total training time: 5.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.12, NNZs: 200, Bias: 0.976647, T: 4848416, Avg. loss: 0.618657\n",
      "Total training time: 5.17 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 2.04, NNZs: 200, Bias: 0.942896, T: 4999929, Avg. loss: 0.616826\n",
      "Total training time: 5.33 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.93, NNZs: 200, Bias: 0.995937, T: 5151442, Avg. loss: 0.614807\n",
      "Total training time: 5.48 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.93, NNZs: 200, Bias: 0.996619, T: 5302955, Avg. loss: 0.611460\n",
      "Total training time: 5.64 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.86, NNZs: 200, Bias: 1.047310, T: 5454468, Avg. loss: 0.610280\n",
      "Total training time: 5.80 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.81, NNZs: 200, Bias: 0.992083, T: 5605981, Avg. loss: 0.607173\n",
      "Total training time: 5.96 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.74, NNZs: 200, Bias: 1.021163, T: 5757494, Avg. loss: 0.607304\n",
      "Total training time: 6.11 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1.77, NNZs: 200, Bias: 0.965025, T: 5909007, Avg. loss: 0.604173\n",
      "Total training time: 6.28 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.72, NNZs: 200, Bias: 1.049346, T: 6060520, Avg. loss: 0.604480\n",
      "Total training time: 6.44 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1.75, NNZs: 200, Bias: 0.934492, T: 6212033, Avg. loss: 0.602526\n",
      "Total training time: 6.60 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1.71, NNZs: 200, Bias: 1.010791, T: 6363546, Avg. loss: 0.600588\n",
      "Total training time: 6.77 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.61, NNZs: 200, Bias: 0.997178, T: 6515059, Avg. loss: 0.601393\n",
      "Total training time: 6.93 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1.66, NNZs: 200, Bias: 0.911688, T: 6666572, Avg. loss: 0.598841\n",
      "Total training time: 7.10 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1.65, NNZs: 200, Bias: 1.021378, T: 6818085, Avg. loss: 0.598831\n",
      "Total training time: 7.26 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1.67, NNZs: 200, Bias: 0.976900, T: 6969598, Avg. loss: 0.595738\n",
      "Total training time: 7.42 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1.56, NNZs: 200, Bias: 1.020647, T: 7121111, Avg. loss: 0.598649\n",
      "Total training time: 7.59 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1.59, NNZs: 200, Bias: 0.956018, T: 7272624, Avg. loss: 0.595156\n",
      "Total training time: 7.75 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.51, NNZs: 200, Bias: 0.933277, T: 7424137, Avg. loss: 0.595072\n",
      "Total training time: 7.91 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.47, NNZs: 200, Bias: 1.028222, T: 7575650, Avg. loss: 0.593609\n",
      "Total training time: 8.08 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 1.53, NNZs: 200, Bias: 0.957648, T: 7727163, Avg. loss: 0.592776\n",
      "Total training time: 8.24 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 1.47, NNZs: 200, Bias: 0.921776, T: 7878676, Avg. loss: 0.592773\n",
      "Total training time: 8.40 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 1.49, NNZs: 200, Bias: 0.959790, T: 8030189, Avg. loss: 0.591624\n",
      "Total training time: 8.56 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 1.48, NNZs: 200, Bias: 0.962597, T: 8181702, Avg. loss: 0.592384\n",
      "Total training time: 8.72 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 1.46, NNZs: 200, Bias: 0.958386, T: 8333215, Avg. loss: 0.589759\n",
      "Total training time: 8.93 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 1.46, NNZs: 200, Bias: 0.968220, T: 8484728, Avg. loss: 0.590551\n",
      "Total training time: 9.19 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 1.40, NNZs: 200, Bias: 0.993854, T: 8636241, Avg. loss: 0.589070\n",
      "Total training time: 9.46 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 1.39, NNZs: 200, Bias: 0.967089, T: 8787754, Avg. loss: 0.588583\n",
      "Total training time: 9.75 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 1.41, NNZs: 200, Bias: 0.898730, T: 8939267, Avg. loss: 0.587165\n",
      "Total training time: 10.03 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 1.44, NNZs: 200, Bias: 0.947755, T: 9090780, Avg. loss: 0.587600\n",
      "Total training time: 10.29 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 1.37, NNZs: 200, Bias: 0.980714, T: 9242293, Avg. loss: 0.587086\n",
      "Total training time: 10.44 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 1.39, NNZs: 200, Bias: 0.941808, T: 9393806, Avg. loss: 0.586560\n",
      "Total training time: 10.60 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 1.37, NNZs: 200, Bias: 0.961349, T: 9545319, Avg. loss: 0.585517\n",
      "Total training time: 10.75 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 1.38, NNZs: 200, Bias: 0.956523, T: 9696832, Avg. loss: 0.585424\n",
      "Total training time: 10.91 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 1.36, NNZs: 200, Bias: 0.962562, T: 9848345, Avg. loss: 0.584853\n",
      "Total training time: 11.07 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 1.33, NNZs: 200, Bias: 0.961777, T: 9999858, Avg. loss: 0.584930\n",
      "Total training time: 11.22 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 1.34, NNZs: 200, Bias: 0.962612, T: 10151371, Avg. loss: 0.583762\n",
      "Total training time: 11.38 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 1.31, NNZs: 200, Bias: 0.989195, T: 10302884, Avg. loss: 0.584095\n",
      "Total training time: 11.53 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 1.33, NNZs: 200, Bias: 0.991406, T: 10454397, Avg. loss: 0.582556\n",
      "Total training time: 11.69 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 1.34, NNZs: 200, Bias: 0.974402, T: 10605910, Avg. loss: 0.583240\n",
      "Total training time: 11.85 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 1.27, NNZs: 200, Bias: 0.982118, T: 10757423, Avg. loss: 0.583538\n",
      "Total training time: 12.01 seconds.\n",
      "-- Epoch 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.26, NNZs: 200, Bias: 0.948760, T: 10908936, Avg. loss: 0.582513\n",
      "Total training time: 12.17 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 1.30, NNZs: 200, Bias: 0.952611, T: 11060449, Avg. loss: 0.581887\n",
      "Total training time: 12.33 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 1.27, NNZs: 200, Bias: 0.961099, T: 11211962, Avg. loss: 0.581880\n",
      "Total training time: 12.50 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 1.28, NNZs: 200, Bias: 0.928547, T: 11363475, Avg. loss: 0.580717\n",
      "Total training time: 12.66 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 1.28, NNZs: 200, Bias: 0.941437, T: 11514988, Avg. loss: 0.580464\n",
      "Total training time: 12.82 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 1.24, NNZs: 200, Bias: 0.980996, T: 11666501, Avg. loss: 0.581567\n",
      "Total training time: 12.98 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 1.26, NNZs: 200, Bias: 0.964353, T: 11818014, Avg. loss: 0.581008\n",
      "Total training time: 13.15 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 1.27, NNZs: 200, Bias: 0.949761, T: 11969527, Avg. loss: 0.580023\n",
      "Total training time: 13.31 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 1.24, NNZs: 200, Bias: 0.920749, T: 12121040, Avg. loss: 0.579870\n",
      "Total training time: 13.48 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 1.24, NNZs: 200, Bias: 0.988286, T: 12272553, Avg. loss: 0.579641\n",
      "Total training time: 13.64 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 1.23, NNZs: 200, Bias: 0.961066, T: 12424066, Avg. loss: 0.579328\n",
      "Total training time: 13.80 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 1.22, NNZs: 200, Bias: 0.943754, T: 12575579, Avg. loss: 0.579690\n",
      "Total training time: 13.96 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 1.23, NNZs: 200, Bias: 0.931262, T: 12727092, Avg. loss: 0.578441\n",
      "Total training time: 14.12 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 1.22, NNZs: 200, Bias: 0.976045, T: 12878605, Avg. loss: 0.579330\n",
      "Total training time: 14.28 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 1.21, NNZs: 200, Bias: 0.938238, T: 13030118, Avg. loss: 0.578140\n",
      "Total training time: 14.44 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 1.20, NNZs: 200, Bias: 0.936440, T: 13181631, Avg. loss: 0.578693\n",
      "Total training time: 14.61 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 1.20, NNZs: 200, Bias: 0.976412, T: 13333144, Avg. loss: 0.578291\n",
      "Total training time: 14.77 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 1.20, NNZs: 200, Bias: 0.961199, T: 13484657, Avg. loss: 0.577712\n",
      "Total training time: 14.95 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 1.22, NNZs: 200, Bias: 0.955398, T: 13636170, Avg. loss: 0.576737\n",
      "Total training time: 15.11 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 1.20, NNZs: 200, Bias: 0.955625, T: 13787683, Avg. loss: 0.577961\n",
      "Total training time: 15.26 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 1.21, NNZs: 200, Bias: 0.935698, T: 13939196, Avg. loss: 0.576726\n",
      "Total training time: 15.43 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 1.21, NNZs: 200, Bias: 0.949245, T: 14090709, Avg. loss: 0.576748\n",
      "Total training time: 15.60 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 1.20, NNZs: 200, Bias: 0.956523, T: 14242222, Avg. loss: 0.577238\n",
      "Total training time: 15.76 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 1.20, NNZs: 200, Bias: 0.952290, T: 14393735, Avg. loss: 0.576971\n",
      "Total training time: 15.91 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 1.18, NNZs: 200, Bias: 0.944532, T: 14545248, Avg. loss: 0.576490\n",
      "Total training time: 16.07 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 1.18, NNZs: 200, Bias: 0.966194, T: 14696761, Avg. loss: 0.575952\n",
      "Total training time: 16.23 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 1.19, NNZs: 200, Bias: 0.967863, T: 14848274, Avg. loss: 0.576060\n",
      "Total training time: 16.38 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 1.18, NNZs: 200, Bias: 0.945028, T: 14999787, Avg. loss: 0.575651\n",
      "Total training time: 16.55 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 1.18, NNZs: 200, Bias: 0.938274, T: 15151300, Avg. loss: 0.575371\n",
      "Total training time: 16.71 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 1.18, NNZs: 200, Bias: 0.947332, T: 15302813, Avg. loss: 0.575403\n",
      "Total training time: 16.89 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 1.18, NNZs: 200, Bias: 0.941111, T: 15454326, Avg. loss: 0.575042\n",
      "Total training time: 17.06 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 1.19, NNZs: 200, Bias: 0.931436, T: 15605839, Avg. loss: 0.575300\n",
      "Total training time: 17.22 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 1.17, NNZs: 200, Bias: 0.937678, T: 15757352, Avg. loss: 0.575222\n",
      "Total training time: 17.38 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 1.15, NNZs: 200, Bias: 0.927036, T: 15908865, Avg. loss: 0.574704\n",
      "Total training time: 17.55 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 1.17, NNZs: 200, Bias: 0.928038, T: 16060378, Avg. loss: 0.574668\n",
      "Total training time: 17.71 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 1.16, NNZs: 200, Bias: 0.934751, T: 16211891, Avg. loss: 0.574305\n",
      "Total training time: 17.88 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 1.14, NNZs: 200, Bias: 0.988439, T: 16363404, Avg. loss: 0.574450\n",
      "Total training time: 18.06 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 1.17, NNZs: 200, Bias: 0.957532, T: 16514917, Avg. loss: 0.573698\n",
      "Total training time: 18.23 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 1.16, NNZs: 200, Bias: 0.941690, T: 16666430, Avg. loss: 0.573777\n",
      "Total training time: 18.39 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 1.14, NNZs: 200, Bias: 0.941996, T: 16817943, Avg. loss: 0.574565\n",
      "Total training time: 18.56 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 1.14, NNZs: 200, Bias: 0.943672, T: 16969456, Avg. loss: 0.573707\n",
      "Total training time: 18.73 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 1.15, NNZs: 200, Bias: 0.960885, T: 17120969, Avg. loss: 0.573672\n",
      "Total training time: 18.90 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 1.13, NNZs: 200, Bias: 0.937104, T: 17272482, Avg. loss: 0.574157\n",
      "Total training time: 19.07 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 1.15, NNZs: 200, Bias: 0.951479, T: 17423995, Avg. loss: 0.573356\n",
      "Total training time: 19.24 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 1.13, NNZs: 200, Bias: 0.952915, T: 17575508, Avg. loss: 0.572887\n",
      "Total training time: 19.41 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 1.13, NNZs: 200, Bias: 0.912825, T: 17727021, Avg. loss: 0.572710\n",
      "Total training time: 19.57 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 1.14, NNZs: 200, Bias: 0.950478, T: 17878534, Avg. loss: 0.573177\n",
      "Total training time: 19.75 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 1.13, NNZs: 200, Bias: 0.947579, T: 18030047, Avg. loss: 0.573521\n",
      "Total training time: 19.92 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 1.12, NNZs: 200, Bias: 0.930284, T: 18181560, Avg. loss: 0.572439\n",
      "Total training time: 20.09 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 1.14, NNZs: 200, Bias: 0.920496, T: 18333073, Avg. loss: 0.572197\n",
      "Total training time: 20.26 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 1.13, NNZs: 200, Bias: 0.952857, T: 18484586, Avg. loss: 0.573416\n",
      "Total training time: 20.43 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 1.11, NNZs: 200, Bias: 0.949869, T: 18636099, Avg. loss: 0.572725\n",
      "Total training time: 20.59 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 1.13, NNZs: 200, Bias: 0.944496, T: 18787612, Avg. loss: 0.572130\n",
      "Total training time: 20.76 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 1.14, NNZs: 200, Bias: 0.933350, T: 18939125, Avg. loss: 0.572438\n",
      "Total training time: 20.92 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 1.11, NNZs: 200, Bias: 0.962881, T: 19090638, Avg. loss: 0.572261\n",
      "Total training time: 21.10 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 1.11, NNZs: 200, Bias: 0.951733, T: 19242151, Avg. loss: 0.571924\n",
      "Total training time: 21.28 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 1.11, NNZs: 200, Bias: 0.916050, T: 19393664, Avg. loss: 0.571997\n",
      "Total training time: 21.45 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 1.10, NNZs: 200, Bias: 0.925247, T: 19545177, Avg. loss: 0.572447\n",
      "Total training time: 21.62 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 1.10, NNZs: 200, Bias: 0.956165, T: 19696690, Avg. loss: 0.571635\n",
      "Total training time: 21.79 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 1.11, NNZs: 200, Bias: 0.923008, T: 19848203, Avg. loss: 0.571781\n",
      "Total training time: 21.96 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 1.11, NNZs: 200, Bias: 0.940508, T: 19999716, Avg. loss: 0.571753\n",
      "Total training time: 22.12 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 1.13, NNZs: 200, Bias: 0.945421, T: 20151229, Avg. loss: 0.571374\n",
      "Total training time: 22.29 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 1.13, NNZs: 200, Bias: 0.922392, T: 20302742, Avg. loss: 0.571321\n",
      "Total training time: 22.46 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 1.10, NNZs: 200, Bias: 0.949519, T: 20454255, Avg. loss: 0.571442\n",
      "Total training time: 22.63 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 1.11, NNZs: 200, Bias: 0.935562, T: 20605768, Avg. loss: 0.571240\n",
      "Total training time: 22.80 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 1.12, NNZs: 200, Bias: 0.950580, T: 20757281, Avg. loss: 0.570976\n",
      "Total training time: 22.97 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 1.11, NNZs: 200, Bias: 0.948123, T: 20908794, Avg. loss: 0.571330\n",
      "Total training time: 23.14 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 1.10, NNZs: 200, Bias: 0.928510, T: 21060307, Avg. loss: 0.571045\n",
      "Total training time: 23.31 seconds.\n",
      "-- Epoch 140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.10, NNZs: 200, Bias: 0.935449, T: 21211820, Avg. loss: 0.570968\n",
      "Total training time: 23.47 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 1.11, NNZs: 200, Bias: 0.922916, T: 21363333, Avg. loss: 0.570743\n",
      "Total training time: 23.64 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.935223, T: 21514846, Avg. loss: 0.571001\n",
      "Total training time: 23.81 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.942102, T: 21666359, Avg. loss: 0.570760\n",
      "Total training time: 23.98 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.958232, T: 21817872, Avg. loss: 0.570441\n",
      "Total training time: 24.14 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.910428, T: 21969385, Avg. loss: 0.570553\n",
      "Total training time: 24.30 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 1.10, NNZs: 200, Bias: 0.946638, T: 22120898, Avg. loss: 0.570610\n",
      "Total training time: 24.46 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 1.10, NNZs: 200, Bias: 0.924611, T: 22272411, Avg. loss: 0.569972\n",
      "Total training time: 24.63 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 1.11, NNZs: 200, Bias: 0.923270, T: 22423924, Avg. loss: 0.569884\n",
      "Total training time: 24.79 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 1.10, NNZs: 200, Bias: 0.940369, T: 22575437, Avg. loss: 0.570091\n",
      "Total training time: 24.96 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.928213, T: 22726950, Avg. loss: 0.570134\n",
      "Total training time: 25.13 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 1.10, NNZs: 200, Bias: 0.958666, T: 22878463, Avg. loss: 0.569393\n",
      "Total training time: 25.28 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.942364, T: 23029976, Avg. loss: 0.570148\n",
      "Total training time: 25.45 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.951058, T: 23181489, Avg. loss: 0.570233\n",
      "Total training time: 25.60 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.921324, T: 23333002, Avg. loss: 0.569829\n",
      "Total training time: 25.76 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.941350, T: 23484515, Avg. loss: 0.569803\n",
      "Total training time: 25.92 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.932382, T: 23636028, Avg. loss: 0.569160\n",
      "Total training time: 26.07 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.939241, T: 23787541, Avg. loss: 0.569795\n",
      "Total training time: 26.22 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.948841, T: 23939054, Avg. loss: 0.569842\n",
      "Total training time: 26.38 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.945072, T: 24090567, Avg. loss: 0.569721\n",
      "Total training time: 26.53 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.920652, T: 24242080, Avg. loss: 0.569488\n",
      "Total training time: 26.69 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.924667, T: 24393593, Avg. loss: 0.568868\n",
      "Total training time: 26.85 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.952926, T: 24545106, Avg. loss: 0.569238\n",
      "Total training time: 27.01 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.940451, T: 24696619, Avg. loss: 0.569179\n",
      "Total training time: 27.17 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.932896, T: 24848132, Avg. loss: 0.568926\n",
      "Total training time: 27.33 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 1.10, NNZs: 200, Bias: 0.927424, T: 24999645, Avg. loss: 0.568844\n",
      "Total training time: 27.49 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.913634, T: 25151158, Avg. loss: 0.568874\n",
      "Total training time: 27.65 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.913294, T: 25302671, Avg. loss: 0.569158\n",
      "Total training time: 27.81 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.936227, T: 25454184, Avg. loss: 0.569085\n",
      "Total training time: 27.98 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.941605, T: 25605697, Avg. loss: 0.568817\n",
      "Total training time: 28.15 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.937639, T: 25757210, Avg. loss: 0.568928\n",
      "Total training time: 28.31 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.955686, T: 25908723, Avg. loss: 0.568650\n",
      "Total training time: 28.47 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.955319, T: 26060236, Avg. loss: 0.568678\n",
      "Total training time: 28.63 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.950354, T: 26211749, Avg. loss: 0.568246\n",
      "Total training time: 28.78 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.930130, T: 26363262, Avg. loss: 0.568505\n",
      "Total training time: 28.94 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 1.09, NNZs: 200, Bias: 0.946660, T: 26514775, Avg. loss: 0.567916\n",
      "Total training time: 29.11 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.959033, T: 26666288, Avg. loss: 0.568635\n",
      "Total training time: 29.27 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.936597, T: 26817801, Avg. loss: 0.568411\n",
      "Total training time: 29.44 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.948008, T: 26969314, Avg. loss: 0.568205\n",
      "Total training time: 29.60 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.959603, T: 27120827, Avg. loss: 0.568521\n",
      "Total training time: 29.75 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.934118, T: 27272340, Avg. loss: 0.568247\n",
      "Total training time: 29.91 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.916546, T: 27423853, Avg. loss: 0.568055\n",
      "Total training time: 30.06 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.948130, T: 27575366, Avg. loss: 0.568069\n",
      "Total training time: 30.22 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.938799, T: 27726879, Avg. loss: 0.567988\n",
      "Total training time: 30.37 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.949471, T: 27878392, Avg. loss: 0.567739\n",
      "Total training time: 30.52 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.958895, T: 28029905, Avg. loss: 0.568409\n",
      "Total training time: 30.68 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.937300, T: 28181418, Avg. loss: 0.568139\n",
      "Total training time: 30.84 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.938998, T: 28332931, Avg. loss: 0.567956\n",
      "Total training time: 31.00 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.937057, T: 28484444, Avg. loss: 0.567769\n",
      "Total training time: 31.16 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.949161, T: 28635957, Avg. loss: 0.568251\n",
      "Total training time: 31.32 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.940172, T: 28787470, Avg. loss: 0.568042\n",
      "Total training time: 31.48 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.955666, T: 28938983, Avg. loss: 0.567958\n",
      "Total training time: 31.64 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.927823, T: 29090496, Avg. loss: 0.567788\n",
      "Total training time: 31.80 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.944131, T: 29242009, Avg. loss: 0.568007\n",
      "Total training time: 31.96 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.962071, T: 29393522, Avg. loss: 0.567327\n",
      "Total training time: 32.12 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.933652, T: 29545035, Avg. loss: 0.567556\n",
      "Total training time: 32.28 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.949208, T: 29696548, Avg. loss: 0.567434\n",
      "Total training time: 32.44 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.933983, T: 29848061, Avg. loss: 0.567238\n",
      "Total training time: 32.60 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.942287, T: 29999574, Avg. loss: 0.567454\n",
      "Total training time: 32.76 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.957454, T: 30151087, Avg. loss: 0.567527\n",
      "Total training time: 32.92 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 1.08, NNZs: 200, Bias: 0.944921, T: 30302600, Avg. loss: 0.567344\n",
      "Total training time: 33.08 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.941379, T: 30454113, Avg. loss: 0.567501\n",
      "Total training time: 33.24 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.955986, T: 30605626, Avg. loss: 0.567396\n",
      "Total training time: 33.40 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.933619, T: 30757139, Avg. loss: 0.567466\n",
      "Total training time: 33.56 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.932021, T: 30908652, Avg. loss: 0.567421\n",
      "Total training time: 33.72 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.931296, T: 31060165, Avg. loss: 0.567303\n",
      "Total training time: 33.89 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.941612, T: 31211678, Avg. loss: 0.567172\n",
      "Total training time: 34.05 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.934450, T: 31363191, Avg. loss: 0.566976\n",
      "Total training time: 34.22 seconds.\n",
      "-- Epoch 208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.06, NNZs: 200, Bias: 0.941273, T: 31514704, Avg. loss: 0.567006\n",
      "Total training time: 34.38 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.952595, T: 31666217, Avg. loss: 0.567217\n",
      "Total training time: 34.53 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.946478, T: 31817730, Avg. loss: 0.567012\n",
      "Total training time: 34.69 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.961055, T: 31969243, Avg. loss: 0.567387\n",
      "Total training time: 34.85 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.954152, T: 32120756, Avg. loss: 0.567340\n",
      "Total training time: 35.01 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.938966, T: 32272269, Avg. loss: 0.567131\n",
      "Total training time: 35.17 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.934146, T: 32423782, Avg. loss: 0.566743\n",
      "Total training time: 35.34 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.926132, T: 32575295, Avg. loss: 0.566908\n",
      "Total training time: 35.50 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.933091, T: 32726808, Avg. loss: 0.566676\n",
      "Total training time: 35.65 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.923238, T: 32878321, Avg. loss: 0.566532\n",
      "Total training time: 35.81 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.939292, T: 33029834, Avg. loss: 0.566610\n",
      "Total training time: 35.97 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.923319, T: 33181347, Avg. loss: 0.566989\n",
      "Total training time: 36.12 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.932668, T: 33332860, Avg. loss: 0.566846\n",
      "Total training time: 36.28 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.946709, T: 33484373, Avg. loss: 0.566814\n",
      "Total training time: 36.44 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.946470, T: 33635886, Avg. loss: 0.566493\n",
      "Total training time: 36.59 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.952499, T: 33787399, Avg. loss: 0.566578\n",
      "Total training time: 36.75 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.928751, T: 33938912, Avg. loss: 0.566503\n",
      "Total training time: 36.91 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.943208, T: 34090425, Avg. loss: 0.566767\n",
      "Total training time: 37.08 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.938940, T: 34241938, Avg. loss: 0.566439\n",
      "Total training time: 37.24 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.940906, T: 34393451, Avg. loss: 0.566435\n",
      "Total training time: 37.40 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.927656, T: 34544964, Avg. loss: 0.566605\n",
      "Total training time: 37.56 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.942505, T: 34696477, Avg. loss: 0.566136\n",
      "Total training time: 37.72 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.939011, T: 34847990, Avg. loss: 0.566329\n",
      "Total training time: 37.88 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.928962, T: 34999503, Avg. loss: 0.566324\n",
      "Total training time: 38.03 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.947139, T: 35151016, Avg. loss: 0.566472\n",
      "Total training time: 38.19 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.936015, T: 35302529, Avg. loss: 0.566267\n",
      "Total training time: 38.35 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.940642, T: 35454042, Avg. loss: 0.566357\n",
      "Total training time: 38.52 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.942528, T: 35605555, Avg. loss: 0.566195\n",
      "Total training time: 38.68 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.929472, T: 35757068, Avg. loss: 0.566277\n",
      "Total training time: 38.84 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.922695, T: 35908581, Avg. loss: 0.566067\n",
      "Total training time: 39.00 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.938574, T: 36060094, Avg. loss: 0.566329\n",
      "Total training time: 39.15 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.940385, T: 36211607, Avg. loss: 0.566250\n",
      "Total training time: 39.31 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 1.07, NNZs: 200, Bias: 0.931764, T: 36363120, Avg. loss: 0.566154\n",
      "Total training time: 39.48 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.941333, T: 36514633, Avg. loss: 0.566049\n",
      "Total training time: 39.64 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.930791, T: 36666146, Avg. loss: 0.566167\n",
      "Total training time: 39.80 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.932580, T: 36817659, Avg. loss: 0.566093\n",
      "Total training time: 39.96 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.935630, T: 36969172, Avg. loss: 0.566000\n",
      "Total training time: 40.11 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.931263, T: 37120685, Avg. loss: 0.566383\n",
      "Total training time: 40.26 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.955260, T: 37272198, Avg. loss: 0.565894\n",
      "Total training time: 40.42 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.929907, T: 37423711, Avg. loss: 0.565943\n",
      "Total training time: 40.57 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.931952, T: 37575224, Avg. loss: 0.565889\n",
      "Total training time: 40.73 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.938079, T: 37726737, Avg. loss: 0.565653\n",
      "Total training time: 40.88 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.937468, T: 37878250, Avg. loss: 0.565960\n",
      "Total training time: 41.04 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.927121, T: 38029763, Avg. loss: 0.566092\n",
      "Total training time: 41.20 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.939206, T: 38181276, Avg. loss: 0.565811\n",
      "Total training time: 41.36 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.957404, T: 38332789, Avg. loss: 0.565976\n",
      "Total training time: 41.52 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.929129, T: 38484302, Avg. loss: 0.565799\n",
      "Total training time: 41.68 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.912702, T: 38635815, Avg. loss: 0.565823\n",
      "Total training time: 41.84 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.921700, T: 38787328, Avg. loss: 0.565687\n",
      "Total training time: 42.00 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.935130, T: 38938841, Avg. loss: 0.565672\n",
      "Total training time: 42.16 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.957095, T: 39090354, Avg. loss: 0.565570\n",
      "Total training time: 42.32 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.940251, T: 39241867, Avg. loss: 0.565392\n",
      "Total training time: 42.47 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.947517, T: 39393380, Avg. loss: 0.565808\n",
      "Total training time: 42.64 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.932260, T: 39544893, Avg. loss: 0.565791\n",
      "Total training time: 42.80 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.944009, T: 39696406, Avg. loss: 0.565379\n",
      "Total training time: 42.96 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.944055, T: 39847919, Avg. loss: 0.565663\n",
      "Total training time: 43.12 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.920714, T: 39999432, Avg. loss: 0.565455\n",
      "Total training time: 43.28 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.927017, T: 40150945, Avg. loss: 0.565458\n",
      "Total training time: 43.44 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.939951, T: 40302458, Avg. loss: 0.565676\n",
      "Total training time: 43.59 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.935175, T: 40453971, Avg. loss: 0.565457\n",
      "Total training time: 43.75 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.942419, T: 40605484, Avg. loss: 0.565452\n",
      "Total training time: 43.91 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.944247, T: 40756997, Avg. loss: 0.565404\n",
      "Total training time: 44.07 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.923648, T: 40908510, Avg. loss: 0.565711\n",
      "Total training time: 44.23 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.940288, T: 41060023, Avg. loss: 0.565378\n",
      "Total training time: 44.39 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.928975, T: 41211536, Avg. loss: 0.565427\n",
      "Total training time: 44.54 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.929827, T: 41363049, Avg. loss: 0.565533\n",
      "Total training time: 44.70 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.963364, T: 41514562, Avg. loss: 0.565006\n",
      "Total training time: 44.86 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.958181, T: 41666075, Avg. loss: 0.565370\n",
      "Total training time: 45.02 seconds.\n",
      "-- Epoch 276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.04, NNZs: 200, Bias: 0.941531, T: 41817588, Avg. loss: 0.565267\n",
      "Total training time: 45.18 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.934995, T: 41969101, Avg. loss: 0.565316\n",
      "Total training time: 45.34 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.931707, T: 42120614, Avg. loss: 0.564862\n",
      "Total training time: 45.50 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.932974, T: 42272127, Avg. loss: 0.565612\n",
      "Total training time: 45.65 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.944264, T: 42423640, Avg. loss: 0.565003\n",
      "Total training time: 45.81 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.947537, T: 42575153, Avg. loss: 0.564852\n",
      "Total training time: 45.97 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.944354, T: 42726666, Avg. loss: 0.565399\n",
      "Total training time: 46.12 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.942739, T: 42878179, Avg. loss: 0.565229\n",
      "Total training time: 46.27 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.928815, T: 43029692, Avg. loss: 0.565258\n",
      "Total training time: 46.45 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.930810, T: 43181205, Avg. loss: 0.565312\n",
      "Total training time: 46.61 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.954982, T: 43332718, Avg. loss: 0.565033\n",
      "Total training time: 46.77 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.932871, T: 43484231, Avg. loss: 0.565200\n",
      "Total training time: 46.92 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.920008, T: 43635744, Avg. loss: 0.565061\n",
      "Total training time: 47.09 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.928328, T: 43787257, Avg. loss: 0.565081\n",
      "Total training time: 47.25 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.937937, T: 43938770, Avg. loss: 0.565240\n",
      "Total training time: 47.42 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.953580, T: 44090283, Avg. loss: 0.565000\n",
      "Total training time: 47.59 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.944330, T: 44241796, Avg. loss: 0.565153\n",
      "Total training time: 47.75 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.931199, T: 44393309, Avg. loss: 0.564897\n",
      "Total training time: 47.92 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.923225, T: 44544822, Avg. loss: 0.564694\n",
      "Total training time: 48.10 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.937150, T: 44696335, Avg. loss: 0.564835\n",
      "Total training time: 48.27 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.923398, T: 44847848, Avg. loss: 0.564869\n",
      "Total training time: 48.44 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.942275, T: 44999361, Avg. loss: 0.565174\n",
      "Total training time: 48.61 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.918447, T: 45150874, Avg. loss: 0.565071\n",
      "Total training time: 48.77 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.935866, T: 45302387, Avg. loss: 0.564996\n",
      "Total training time: 48.94 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 1.06, NNZs: 200, Bias: 0.944228, T: 45453900, Avg. loss: 0.564783\n",
      "Total training time: 49.11 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.924291, T: 45605413, Avg. loss: 0.564817\n",
      "Total training time: 49.27 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.917908, T: 45756926, Avg. loss: 0.565006\n",
      "Total training time: 49.44 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.924293, T: 45908439, Avg. loss: 0.564862\n",
      "Total training time: 49.61 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.939548, T: 46059952, Avg. loss: 0.564894\n",
      "Total training time: 49.79 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.944362, T: 46211465, Avg. loss: 0.564669\n",
      "Total training time: 49.96 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.935172, T: 46362978, Avg. loss: 0.564736\n",
      "Total training time: 50.12 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.950284, T: 46514491, Avg. loss: 0.564601\n",
      "Total training time: 50.29 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.935544, T: 46666004, Avg. loss: 0.564752\n",
      "Total training time: 50.46 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.920669, T: 46817517, Avg. loss: 0.564807\n",
      "Total training time: 50.62 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.953662, T: 46969030, Avg. loss: 0.564628\n",
      "Total training time: 50.79 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.928739, T: 47120543, Avg. loss: 0.564754\n",
      "Total training time: 50.95 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.939412, T: 47272056, Avg. loss: 0.564679\n",
      "Total training time: 51.12 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.919217, T: 47423569, Avg. loss: 0.564382\n",
      "Total training time: 51.28 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.940115, T: 47575082, Avg. loss: 0.564552\n",
      "Total training time: 51.45 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.941603, T: 47726595, Avg. loss: 0.564784\n",
      "Total training time: 51.61 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.933912, T: 47878108, Avg. loss: 0.564647\n",
      "Total training time: 51.78 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.926313, T: 48029621, Avg. loss: 0.564572\n",
      "Total training time: 51.95 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.937355, T: 48181134, Avg. loss: 0.564574\n",
      "Total training time: 52.12 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.931289, T: 48332647, Avg. loss: 0.564596\n",
      "Total training time: 52.28 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.939296, T: 48484160, Avg. loss: 0.564377\n",
      "Total training time: 52.45 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.923666, T: 48635673, Avg. loss: 0.564425\n",
      "Total training time: 52.61 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.925104, T: 48787186, Avg. loss: 0.564583\n",
      "Total training time: 52.78 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.927393, T: 48938699, Avg. loss: 0.564523\n",
      "Total training time: 52.95 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.937741, T: 49090212, Avg. loss: 0.564333\n",
      "Total training time: 53.12 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.953515, T: 49241725, Avg. loss: 0.564386\n",
      "Total training time: 53.28 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.942914, T: 49393238, Avg. loss: 0.564541\n",
      "Total training time: 53.44 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.946471, T: 49544751, Avg. loss: 0.564332\n",
      "Total training time: 53.59 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.941300, T: 49696264, Avg. loss: 0.564232\n",
      "Total training time: 53.75 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.933253, T: 49847777, Avg. loss: 0.564395\n",
      "Total training time: 53.90 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.943566, T: 49999290, Avg. loss: 0.564270\n",
      "Total training time: 54.07 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.935093, T: 50150803, Avg. loss: 0.564436\n",
      "Total training time: 54.27 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.937772, T: 50302316, Avg. loss: 0.564357\n",
      "Total training time: 54.45 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 1.05, NNZs: 200, Bias: 0.942636, T: 50453829, Avg. loss: 0.564412\n",
      "Total training time: 54.61 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.940033, T: 50605342, Avg. loss: 0.564492\n",
      "Total training time: 54.77 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.932502, T: 50756855, Avg. loss: 0.564375\n",
      "Total training time: 54.93 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.942952, T: 50908368, Avg. loss: 0.564158\n",
      "Total training time: 55.08 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.929527, T: 51059881, Avg. loss: 0.563929\n",
      "Total training time: 55.24 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.941087, T: 51211394, Avg. loss: 0.564325\n",
      "Total training time: 55.39 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.943079, T: 51362907, Avg. loss: 0.564355\n",
      "Total training time: 55.55 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.937194, T: 51514420, Avg. loss: 0.564346\n",
      "Total training time: 55.70 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.943347, T: 51665933, Avg. loss: 0.564202\n",
      "Total training time: 55.86 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.945929, T: 51817446, Avg. loss: 0.564296\n",
      "Total training time: 56.01 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.944013, T: 51968959, Avg. loss: 0.564066\n",
      "Total training time: 56.17 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.929288, T: 52120472, Avg. loss: 0.564210\n",
      "Total training time: 56.33 seconds.\n",
      "-- Epoch 345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.04, NNZs: 200, Bias: 0.930996, T: 52271985, Avg. loss: 0.564154\n",
      "Total training time: 56.49 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.919976, T: 52423498, Avg. loss: 0.563941\n",
      "Total training time: 56.65 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.933883, T: 52575011, Avg. loss: 0.564223\n",
      "Total training time: 56.81 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.941778, T: 52726524, Avg. loss: 0.564027\n",
      "Total training time: 56.98 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.929067, T: 52878037, Avg. loss: 0.564270\n",
      "Total training time: 57.14 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.943178, T: 53029550, Avg. loss: 0.564014\n",
      "Total training time: 57.29 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.924897, T: 53181063, Avg. loss: 0.564138\n",
      "Total training time: 57.45 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.945140, T: 53332576, Avg. loss: 0.563939\n",
      "Total training time: 57.61 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.928263, T: 53484089, Avg. loss: 0.564284\n",
      "Total training time: 57.77 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.926184, T: 53635602, Avg. loss: 0.564092\n",
      "Total training time: 57.93 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.953515, T: 53787115, Avg. loss: 0.564067\n",
      "Total training time: 58.10 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.941810, T: 53938628, Avg. loss: 0.564057\n",
      "Total training time: 58.26 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.929562, T: 54090141, Avg. loss: 0.564084\n",
      "Total training time: 58.41 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.920635, T: 54241654, Avg. loss: 0.564244\n",
      "Total training time: 58.57 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.922792, T: 54393167, Avg. loss: 0.563922\n",
      "Total training time: 58.73 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.936394, T: 54544680, Avg. loss: 0.563968\n",
      "Total training time: 58.89 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.931123, T: 54696193, Avg. loss: 0.564038\n",
      "Total training time: 59.06 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.929299, T: 54847706, Avg. loss: 0.564124\n",
      "Total training time: 59.22 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.934917, T: 54999219, Avg. loss: 0.564019\n",
      "Total training time: 59.38 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.941383, T: 55150732, Avg. loss: 0.563807\n",
      "Total training time: 59.54 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.933528, T: 55302245, Avg. loss: 0.564004\n",
      "Total training time: 59.70 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.937644, T: 55453758, Avg. loss: 0.563936\n",
      "Total training time: 59.86 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.930634, T: 55605271, Avg. loss: 0.563917\n",
      "Total training time: 60.02 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.938418, T: 55756784, Avg. loss: 0.563966\n",
      "Total training time: 60.18 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.935201, T: 55908297, Avg. loss: 0.564127\n",
      "Total training time: 60.33 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.919850, T: 56059810, Avg. loss: 0.563943\n",
      "Total training time: 60.48 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.928208, T: 56211323, Avg. loss: 0.563667\n",
      "Total training time: 60.64 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.931493, T: 56362836, Avg. loss: 0.563791\n",
      "Total training time: 60.79 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.927253, T: 56514349, Avg. loss: 0.563803\n",
      "Total training time: 60.95 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.925250, T: 56665862, Avg. loss: 0.563868\n",
      "Total training time: 61.11 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.920265, T: 56817375, Avg. loss: 0.563794\n",
      "Total training time: 61.27 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.937452, T: 56968888, Avg. loss: 0.563801\n",
      "Total training time: 61.44 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.921173, T: 57120401, Avg. loss: 0.563627\n",
      "Total training time: 61.59 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.932396, T: 57271914, Avg. loss: 0.563668\n",
      "Total training time: 61.75 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.938830, T: 57423427, Avg. loss: 0.563737\n",
      "Total training time: 61.90 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.918252, T: 57574940, Avg. loss: 0.563812\n",
      "Total training time: 62.06 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.926356, T: 57726453, Avg. loss: 0.563814\n",
      "Total training time: 62.22 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.936610, T: 57877966, Avg. loss: 0.563665\n",
      "Total training time: 62.38 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.945864, T: 58029479, Avg. loss: 0.563760\n",
      "Total training time: 62.54 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.937856, T: 58180992, Avg. loss: 0.563769\n",
      "Total training time: 62.71 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.935273, T: 58332505, Avg. loss: 0.563569\n",
      "Total training time: 62.87 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.931873, T: 58484018, Avg. loss: 0.563837\n",
      "Total training time: 63.03 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.939090, T: 58635531, Avg. loss: 0.563761\n",
      "Total training time: 63.19 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.937464, T: 58787044, Avg. loss: 0.563714\n",
      "Total training time: 63.36 seconds.\n",
      "-- Epoch 389\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.928029, T: 58938557, Avg. loss: 0.563674\n",
      "Total training time: 63.52 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.917750, T: 59090070, Avg. loss: 0.563928\n",
      "Total training time: 63.68 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.926932, T: 59241583, Avg. loss: 0.563573\n",
      "Total training time: 63.84 seconds.\n",
      "-- Epoch 392\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.920847, T: 59393096, Avg. loss: 0.563728\n",
      "Total training time: 63.99 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.923771, T: 59544609, Avg. loss: 0.563607\n",
      "Total training time: 64.15 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.915933, T: 59696122, Avg. loss: 0.563861\n",
      "Total training time: 64.30 seconds.\n",
      "-- Epoch 395\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.940192, T: 59847635, Avg. loss: 0.563684\n",
      "Total training time: 64.47 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.946150, T: 59999148, Avg. loss: 0.563522\n",
      "Total training time: 64.63 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.933829, T: 60150661, Avg. loss: 0.563796\n",
      "Total training time: 64.79 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.930176, T: 60302174, Avg. loss: 0.563563\n",
      "Total training time: 64.95 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.925551, T: 60453687, Avg. loss: 0.563653\n",
      "Total training time: 65.11 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.929603, T: 60605200, Avg. loss: 0.563577\n",
      "Total training time: 65.26 seconds.\n",
      "-- Epoch 401\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.922217, T: 60756713, Avg. loss: 0.563467\n",
      "Total training time: 65.42 seconds.\n",
      "-- Epoch 402\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.932150, T: 60908226, Avg. loss: 0.563505\n",
      "Total training time: 65.58 seconds.\n",
      "-- Epoch 403\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.925751, T: 61059739, Avg. loss: 0.563444\n",
      "Total training time: 65.74 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.939189, T: 61211252, Avg. loss: 0.563450\n",
      "Total training time: 65.90 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.926035, T: 61362765, Avg. loss: 0.563757\n",
      "Total training time: 66.06 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.926304, T: 61514278, Avg. loss: 0.563577\n",
      "Total training time: 66.21 seconds.\n",
      "-- Epoch 407\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.936566, T: 61665791, Avg. loss: 0.563429\n",
      "Total training time: 66.37 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.943460, T: 61817304, Avg. loss: 0.563625\n",
      "Total training time: 66.52 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.933445, T: 61968817, Avg. loss: 0.563405\n",
      "Total training time: 66.68 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.929978, T: 62120330, Avg. loss: 0.563459\n",
      "Total training time: 66.84 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.930717, T: 62271843, Avg. loss: 0.563404\n",
      "Total training time: 67.00 seconds.\n",
      "-- Epoch 412\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.926697, T: 62423356, Avg. loss: 0.563516\n",
      "Total training time: 67.17 seconds.\n",
      "-- Epoch 413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.03, NNZs: 200, Bias: 0.921793, T: 62574869, Avg. loss: 0.563589\n",
      "Total training time: 67.32 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.941990, T: 62726382, Avg. loss: 0.563380\n",
      "Total training time: 67.48 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.938164, T: 62877895, Avg. loss: 0.563490\n",
      "Total training time: 67.63 seconds.\n",
      "-- Epoch 416\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.941398, T: 63029408, Avg. loss: 0.563311\n",
      "Total training time: 67.79 seconds.\n",
      "-- Epoch 417\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.923067, T: 63180921, Avg. loss: 0.563458\n",
      "Total training time: 67.96 seconds.\n",
      "-- Epoch 418\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.929332, T: 63332434, Avg. loss: 0.563399\n",
      "Total training time: 68.12 seconds.\n",
      "-- Epoch 419\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.924827, T: 63483947, Avg. loss: 0.563494\n",
      "Total training time: 68.28 seconds.\n",
      "-- Epoch 420\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.936353, T: 63635460, Avg. loss: 0.563528\n",
      "Total training time: 68.43 seconds.\n",
      "-- Epoch 421\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.917344, T: 63786973, Avg. loss: 0.563442\n",
      "Total training time: 68.59 seconds.\n",
      "-- Epoch 422\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.931026, T: 63938486, Avg. loss: 0.563209\n",
      "Total training time: 68.75 seconds.\n",
      "-- Epoch 423\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.932563, T: 64089999, Avg. loss: 0.563224\n",
      "Total training time: 68.91 seconds.\n",
      "-- Epoch 424\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.927617, T: 64241512, Avg. loss: 0.563414\n",
      "Total training time: 69.07 seconds.\n",
      "-- Epoch 425\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.934515, T: 64393025, Avg. loss: 0.563241\n",
      "Total training time: 69.24 seconds.\n",
      "-- Epoch 426\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.929505, T: 64544538, Avg. loss: 0.563477\n",
      "Total training time: 69.40 seconds.\n",
      "-- Epoch 427\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.926778, T: 64696051, Avg. loss: 0.563466\n",
      "Total training time: 69.56 seconds.\n",
      "-- Epoch 428\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.933059, T: 64847564, Avg. loss: 0.563371\n",
      "Total training time: 69.72 seconds.\n",
      "-- Epoch 429\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.931511, T: 64999077, Avg. loss: 0.563258\n",
      "Total training time: 69.87 seconds.\n",
      "-- Epoch 430\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.932339, T: 65150590, Avg. loss: 0.563452\n",
      "Total training time: 70.03 seconds.\n",
      "-- Epoch 431\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.933741, T: 65302103, Avg. loss: 0.563301\n",
      "Total training time: 70.18 seconds.\n",
      "-- Epoch 432\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.924298, T: 65453616, Avg. loss: 0.563272\n",
      "Total training time: 70.34 seconds.\n",
      "-- Epoch 433\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.931483, T: 65605129, Avg. loss: 0.563402\n",
      "Total training time: 70.49 seconds.\n",
      "-- Epoch 434\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.941218, T: 65756642, Avg. loss: 0.563170\n",
      "Total training time: 70.64 seconds.\n",
      "-- Epoch 435\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.930372, T: 65908155, Avg. loss: 0.563306\n",
      "Total training time: 70.80 seconds.\n",
      "-- Epoch 436\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.935082, T: 66059668, Avg. loss: 0.563334\n",
      "Total training time: 70.96 seconds.\n",
      "-- Epoch 437\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.931053, T: 66211181, Avg. loss: 0.563284\n",
      "Total training time: 71.12 seconds.\n",
      "-- Epoch 438\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.940259, T: 66362694, Avg. loss: 0.563402\n",
      "Total training time: 71.28 seconds.\n",
      "-- Epoch 439\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.928664, T: 66514207, Avg. loss: 0.563303\n",
      "Total training time: 71.44 seconds.\n",
      "-- Epoch 440\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.945796, T: 66665720, Avg. loss: 0.563210\n",
      "Total training time: 71.60 seconds.\n",
      "-- Epoch 441\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.942057, T: 66817233, Avg. loss: 0.563162\n",
      "Total training time: 71.75 seconds.\n",
      "-- Epoch 442\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.932163, T: 66968746, Avg. loss: 0.563334\n",
      "Total training time: 71.91 seconds.\n",
      "-- Epoch 443\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.929368, T: 67120259, Avg. loss: 0.563190\n",
      "Total training time: 72.07 seconds.\n",
      "-- Epoch 444\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.939121, T: 67271772, Avg. loss: 0.563101\n",
      "Total training time: 72.23 seconds.\n",
      "-- Epoch 445\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.927995, T: 67423285, Avg. loss: 0.563240\n",
      "Total training time: 72.39 seconds.\n",
      "-- Epoch 446\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.935025, T: 67574798, Avg. loss: 0.563224\n",
      "Total training time: 72.55 seconds.\n",
      "-- Epoch 447\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.929428, T: 67726311, Avg. loss: 0.563269\n",
      "Total training time: 72.71 seconds.\n",
      "-- Epoch 448\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.938532, T: 67877824, Avg. loss: 0.563213\n",
      "Total training time: 72.86 seconds.\n",
      "-- Epoch 449\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.937980, T: 68029337, Avg. loss: 0.563239\n",
      "Total training time: 73.02 seconds.\n",
      "-- Epoch 450\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.937271, T: 68180850, Avg. loss: 0.563199\n",
      "Total training time: 73.18 seconds.\n",
      "-- Epoch 451\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.930485, T: 68332363, Avg. loss: 0.563226\n",
      "Total training time: 73.34 seconds.\n",
      "-- Epoch 452\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.925818, T: 68483876, Avg. loss: 0.563276\n",
      "Total training time: 73.51 seconds.\n",
      "-- Epoch 453\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.936530, T: 68635389, Avg. loss: 0.563202\n",
      "Total training time: 73.67 seconds.\n",
      "-- Epoch 454\n",
      "Norm: 1.04, NNZs: 200, Bias: 0.921233, T: 68786902, Avg. loss: 0.562916\n",
      "Total training time: 73.83 seconds.\n",
      "-- Epoch 455\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.927008, T: 68938415, Avg. loss: 0.563227\n",
      "Total training time: 74.00 seconds.\n",
      "-- Epoch 456\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.929559, T: 69089928, Avg. loss: 0.563084\n",
      "Total training time: 74.15 seconds.\n",
      "-- Epoch 457\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.936926, T: 69241441, Avg. loss: 0.563127\n",
      "Total training time: 74.31 seconds.\n",
      "-- Epoch 458\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.929985, T: 69392954, Avg. loss: 0.563217\n",
      "Total training time: 74.47 seconds.\n",
      "-- Epoch 459\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.935318, T: 69544467, Avg. loss: 0.563022\n",
      "Total training time: 74.63 seconds.\n",
      "-- Epoch 460\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.940587, T: 69695980, Avg. loss: 0.563385\n",
      "Total training time: 74.79 seconds.\n",
      "-- Epoch 461\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.928168, T: 69847493, Avg. loss: 0.563028\n",
      "Total training time: 74.95 seconds.\n",
      "-- Epoch 462\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.941519, T: 69999006, Avg. loss: 0.562998\n",
      "Total training time: 75.11 seconds.\n",
      "-- Epoch 463\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.938199, T: 70150519, Avg. loss: 0.563087\n",
      "Total training time: 75.26 seconds.\n",
      "-- Epoch 464\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.941817, T: 70302032, Avg. loss: 0.563226\n",
      "Total training time: 75.42 seconds.\n",
      "-- Epoch 465\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.931773, T: 70453545, Avg. loss: 0.563017\n",
      "Total training time: 75.58 seconds.\n",
      "-- Epoch 466\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.931002, T: 70605058, Avg. loss: 0.563023\n",
      "Total training time: 75.74 seconds.\n",
      "-- Epoch 467\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.937611, T: 70756571, Avg. loss: 0.563020\n",
      "Total training time: 75.90 seconds.\n",
      "-- Epoch 468\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.931757, T: 70908084, Avg. loss: 0.562869\n",
      "Total training time: 76.06 seconds.\n",
      "-- Epoch 469\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.931167, T: 71059597, Avg. loss: 0.563099\n",
      "Total training time: 76.21 seconds.\n",
      "-- Epoch 470\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.929143, T: 71211110, Avg. loss: 0.563050\n",
      "Total training time: 76.37 seconds.\n",
      "-- Epoch 471\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.932806, T: 71362623, Avg. loss: 0.563034\n",
      "Total training time: 76.53 seconds.\n",
      "-- Epoch 472\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.938732, T: 71514136, Avg. loss: 0.563120\n",
      "Total training time: 76.68 seconds.\n",
      "-- Epoch 473\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.936427, T: 71665649, Avg. loss: 0.562930\n",
      "Total training time: 76.85 seconds.\n",
      "-- Epoch 474\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.932387, T: 71817162, Avg. loss: 0.563025\n",
      "Total training time: 77.01 seconds.\n",
      "-- Epoch 475\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.936456, T: 71968675, Avg. loss: 0.563111\n",
      "Total training time: 77.17 seconds.\n",
      "-- Epoch 476\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.922642, T: 72120188, Avg. loss: 0.563112\n",
      "Total training time: 77.34 seconds.\n",
      "-- Epoch 477\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.935411, T: 72271701, Avg. loss: 0.563000\n",
      "Total training time: 77.50 seconds.\n",
      "-- Epoch 478\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.933436, T: 72423214, Avg. loss: 0.563017\n",
      "Total training time: 77.67 seconds.\n",
      "-- Epoch 479\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.929174, T: 72574727, Avg. loss: 0.562877\n",
      "Total training time: 77.84 seconds.\n",
      "-- Epoch 480\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.924649, T: 72726240, Avg. loss: 0.562886\n",
      "Total training time: 78.01 seconds.\n",
      "-- Epoch 481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.03, NNZs: 200, Bias: 0.930448, T: 72877753, Avg. loss: 0.562914\n",
      "Total training time: 78.21 seconds.\n",
      "-- Epoch 482\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.935185, T: 73029266, Avg. loss: 0.562899\n",
      "Total training time: 78.37 seconds.\n",
      "-- Epoch 483\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.935036, T: 73180779, Avg. loss: 0.562984\n",
      "Total training time: 78.54 seconds.\n",
      "-- Epoch 484\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.926441, T: 73332292, Avg. loss: 0.563073\n",
      "Total training time: 78.70 seconds.\n",
      "-- Epoch 485\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.927457, T: 73483805, Avg. loss: 0.562965\n",
      "Total training time: 78.87 seconds.\n",
      "-- Epoch 486\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.931970, T: 73635318, Avg. loss: 0.562970\n",
      "Total training time: 79.03 seconds.\n",
      "-- Epoch 487\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.934576, T: 73786831, Avg. loss: 0.562902\n",
      "Total training time: 79.20 seconds.\n",
      "-- Epoch 488\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.937629, T: 73938344, Avg. loss: 0.562886\n",
      "Total training time: 79.38 seconds.\n",
      "-- Epoch 489\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.936188, T: 74089857, Avg. loss: 0.563019\n",
      "Total training time: 79.55 seconds.\n",
      "-- Epoch 490\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.927892, T: 74241370, Avg. loss: 0.563044\n",
      "Total training time: 79.73 seconds.\n",
      "-- Epoch 491\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.931869, T: 74392883, Avg. loss: 0.562767\n",
      "Total training time: 79.90 seconds.\n",
      "-- Epoch 492\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.932387, T: 74544396, Avg. loss: 0.562930\n",
      "Total training time: 80.07 seconds.\n",
      "-- Epoch 493\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.926874, T: 74695909, Avg. loss: 0.562968\n",
      "Total training time: 80.24 seconds.\n",
      "-- Epoch 494\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.930212, T: 74847422, Avg. loss: 0.562895\n",
      "Total training time: 80.41 seconds.\n",
      "-- Epoch 495\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.924602, T: 74998935, Avg. loss: 0.562837\n",
      "Total training time: 80.58 seconds.\n",
      "-- Epoch 496\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.935473, T: 75150448, Avg. loss: 0.562872\n",
      "Total training time: 80.74 seconds.\n",
      "-- Epoch 497\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.926413, T: 75301961, Avg. loss: 0.562816\n",
      "Total training time: 80.91 seconds.\n",
      "-- Epoch 498\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.939020, T: 75453474, Avg. loss: 0.562827\n",
      "Total training time: 81.08 seconds.\n",
      "-- Epoch 499\n",
      "Norm: 1.03, NNZs: 200, Bias: 0.939562, T: 75604987, Avg. loss: 0.562742\n",
      "Total training time: 81.26 seconds.\n",
      "-- Epoch 500\n",
      "Norm: 1.02, NNZs: 200, Bias: 0.934828, T: 75756500, Avg. loss: 0.562893\n",
      "Total training time: 81.43 seconds.\n",
      "0.6662892306174224\n",
      "81.97390341758728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "t1=time.time()\n",
    "sgd = SGDClassifier(loss=\"log\", penalty=\"l2\",max_iter=500,verbose=True)\n",
    "sgd.fit(X_train,y_train)\n",
    "pred = sgd.predict_proba(X_valid)\n",
    "print(log_loss(y_valid,pred[:,1]))\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6662892306174224"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = sgd.predict_proba(X_valid)\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find predicted value on test set\n",
    "pred = sgd.predict_proba(test)\n",
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test_ids\n",
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=pred[:,1]\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')\n",
    "submit.to_csv('SGD_1000iter_impute_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6007574892063543\n",
      "7.061818838119507\n"
     ]
    }
   ],
   "source": [
    "#trying Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "t1=time.time()\n",
    "rfc = RandomForestClassifier(max_depth=4, random_state=0,verbose=True)\n",
    "rfc.fit(X_train, y_train)\n",
    "pred = rfc.predict_proba(X_valid)\n",
    "print(log_loss(y_valid,pred[:,1]))\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "pred = rfc.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test_ids\n",
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=pred[:,1]\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')\n",
    "submit.to_csv('RFC_depth4_impute-full_pca.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-424dd4d5ae2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclean_col_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'['\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "clean_col_name=[]\n",
    "for col in X_train.columns:\n",
    "    col = col.replace('<','X')\n",
    "    col = col.replace('>','Y')\n",
    "    col = col.replace('[','A')\n",
    "    col = col.replace(']','B')\n",
    "    clean_col_name.append(col)\n",
    "X_train.columns = clean_col_name\n",
    "clean_col_name=[]\n",
    "for col in X_valid.columns:\n",
    "    col = col.replace('<','X')\n",
    "    col = col.replace('>','Y')\n",
    "    col = col.replace('[','A')\n",
    "    col = col.replace(']','B')\n",
    "    clean_col_name.append(col)\n",
    "X_valid.columns = clean_col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mca'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-b26b7a584789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mca'"
     ]
    }
   ],
   "source": [
    "import mca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time for xgboost\n",
    "import xgboost as xgb\n",
    "import time\n",
    "t1 = time.time()\n",
    "model1 = xgb.XGBClassifier()\n",
    "#model2 = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5)\n",
    "xgb1 = model1.fit(X_train, y_train)\n",
    "#xgb2 = model2.fit(X_train.iloc[0:10,], y_train[0:10])\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193.9544222354889"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=xgb1.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5899355365775966"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=test.columns\n",
    "clean_col_name=[]\n",
    "for col in test.columns:\n",
    "    col = col.replace('<','X')\n",
    "    col = col.replace('>','Y')\n",
    "    col = col.replace('[','A')\n",
    "    col = col.replace(']','B')\n",
    "    clean_col_name.append(col)\n",
    "test.columns = clean_col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test_ids\n",
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=xgb1.predict_proba(test)[:,1]\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')\n",
    "submit.to_csv('XGB-entiretrainscl-impute-pca.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x7f7011857cc0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/site-packages/xgboost-0.7-py3.6.egg/xgboost/core.py\", line 366, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "788"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying MLPerceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20),verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.54744829\n",
      "Iteration 2, loss = 0.47460354\n",
      "Iteration 3, loss = 0.43774356\n",
      "Iteration 4, loss = 0.41372207\n",
      "Iteration 5, loss = 0.39637359\n",
      "Iteration 6, loss = 0.38394711\n",
      "Iteration 7, loss = 0.37292829\n",
      "Iteration 8, loss = 0.36526479\n",
      "Iteration 9, loss = 0.35813077\n",
      "Iteration 10, loss = 0.35204308\n",
      "Iteration 11, loss = 0.34779892\n",
      "Iteration 12, loss = 0.34290100\n",
      "Iteration 13, loss = 0.33910072\n",
      "Iteration 14, loss = 0.33684330\n",
      "Iteration 15, loss = 0.33237648\n",
      "Iteration 16, loss = 0.32981847\n",
      "Iteration 17, loss = 0.32737112\n",
      "Iteration 18, loss = 0.32472487\n",
      "Iteration 19, loss = 0.32250019\n",
      "Iteration 20, loss = 0.32039229\n",
      "Iteration 21, loss = 0.31800582\n",
      "Iteration 22, loss = 0.31688926\n",
      "Iteration 23, loss = 0.31564367\n",
      "Iteration 24, loss = 0.31349893\n",
      "Iteration 25, loss = 0.31237866\n",
      "Iteration 26, loss = 0.31130883\n",
      "Iteration 27, loss = 0.31075767\n",
      "Iteration 28, loss = 0.30924159\n",
      "Iteration 29, loss = 0.30739513\n",
      "Iteration 30, loss = 0.30619866\n",
      "Iteration 31, loss = 0.30603649\n",
      "Iteration 32, loss = 0.30467044\n",
      "Iteration 33, loss = 0.30378217\n",
      "Iteration 34, loss = 0.30346710\n",
      "Iteration 35, loss = 0.30217100\n",
      "Iteration 36, loss = 0.30186538\n",
      "Iteration 37, loss = 0.30126864\n",
      "Iteration 38, loss = 0.30023812\n",
      "Iteration 39, loss = 0.29927403\n",
      "Iteration 40, loss = 0.29974919\n",
      "Iteration 41, loss = 0.29882922\n",
      "Iteration 42, loss = 0.29789376\n",
      "Iteration 43, loss = 0.29660226\n",
      "Iteration 44, loss = 0.29630246\n",
      "Iteration 45, loss = 0.29656940\n",
      "Iteration 46, loss = 0.29497882\n",
      "Iteration 47, loss = 0.29492517\n",
      "Iteration 48, loss = 0.29370398\n",
      "Iteration 49, loss = 0.29350635\n",
      "Iteration 50, loss = 0.29316444\n",
      "Iteration 51, loss = 0.29273744\n",
      "Iteration 52, loss = 0.29207686\n",
      "Iteration 53, loss = 0.29141814\n",
      "Iteration 54, loss = 0.29200813\n",
      "Iteration 55, loss = 0.29116496\n",
      "Iteration 56, loss = 0.29051530\n",
      "Iteration 57, loss = 0.29043398\n",
      "Iteration 58, loss = 0.28964635\n",
      "Iteration 59, loss = 0.28926707\n",
      "Iteration 60, loss = 0.28869493\n",
      "Iteration 61, loss = 0.28809389\n",
      "Iteration 62, loss = 0.28832051\n",
      "Iteration 63, loss = 0.28779957\n",
      "Iteration 64, loss = 0.28751959\n",
      "Iteration 65, loss = 0.28713778\n",
      "Iteration 66, loss = 0.28679632\n",
      "Iteration 67, loss = 0.28582488\n",
      "Iteration 68, loss = 0.28579098\n",
      "Iteration 69, loss = 0.28596743\n",
      "Iteration 70, loss = 0.28503445\n",
      "Iteration 71, loss = 0.28511740\n",
      "Iteration 72, loss = 0.28432269\n",
      "Iteration 73, loss = 0.28491629\n",
      "Iteration 74, loss = 0.28407111\n",
      "Iteration 75, loss = 0.28370964\n",
      "Iteration 76, loss = 0.28364594\n",
      "Iteration 77, loss = 0.28390243\n",
      "Iteration 78, loss = 0.28344321\n",
      "Iteration 79, loss = 0.28306252\n",
      "Iteration 80, loss = 0.28294576\n",
      "Iteration 81, loss = 0.28243946\n",
      "Iteration 82, loss = 0.28238867\n",
      "Iteration 83, loss = 0.28219571\n",
      "Iteration 84, loss = 0.28235886\n",
      "Iteration 85, loss = 0.28151520\n",
      "Iteration 86, loss = 0.28129410\n",
      "Iteration 87, loss = 0.28099239\n",
      "Iteration 88, loss = 0.28153237\n",
      "Iteration 89, loss = 0.28085303\n",
      "Iteration 90, loss = 0.28138630\n",
      "Iteration 91, loss = 0.28018611\n",
      "Iteration 92, loss = 0.28061032\n",
      "Iteration 93, loss = 0.28026106\n",
      "Iteration 94, loss = 0.28007795\n",
      "Iteration 95, loss = 0.27994851\n",
      "Iteration 96, loss = 0.27982733\n",
      "Iteration 97, loss = 0.27943165\n",
      "Iteration 98, loss = 0.27909224\n",
      "Iteration 99, loss = 0.27872312\n",
      "Iteration 100, loss = 0.27841583\n",
      "Iteration 101, loss = 0.27903624\n",
      "Iteration 102, loss = 0.27798009\n",
      "Iteration 103, loss = 0.27842419\n",
      "Iteration 104, loss = 0.27808257\n",
      "Iteration 105, loss = 0.27712272\n",
      "Iteration 106, loss = 0.27812576\n",
      "Iteration 107, loss = 0.27727870\n",
      "Iteration 108, loss = 0.27814499\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "105.89590549468994\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "mlp.fit(X_train,y_train)\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5955693314970774"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "pred = mlp.predict_proba(X_valid)\n",
    "log_loss(y_valid,pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit the prediction score\n",
    "submit = pd.DataFrame()\n",
    "submit['INNOVATION_CHALLENGE_KEY'] = test['innovation_challenge_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=mlp.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1888195"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RENEWAL_PROBABLIITY\n",
    "submit['RENEWAL_PROBABLIITY']=pred[:,1]\n",
    "submit=submit.sort_values('INNOVATION_CHALLENGE_KEY')\n",
    "submit.to_csv('MLP_20_20_20_scl-impute.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
